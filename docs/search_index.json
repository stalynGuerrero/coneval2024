[["index.html", "Manual de procesamiento del indicador de pobreza multidimensional mediante el uso de estimación de áreas pequeñas para México 2024 Capítulo 1 Introducción", " Manual de procesamiento del indicador de pobreza multidimensional mediante el uso de estimación de áreas pequeñas para México 2024 Andrés Gutiérrez1, Stalyn Guerrero2 2024-07-29 Capítulo 1 Introducción Este manual se desarrolla como una herramienta para detallar el paso a paso seguido por la Comisión Económica para América Latina y el Caribe (CEPAL) en la obtención de estimaciones del indicador multidimensional de pobreza en México. El proceso utiliza técnicas de estimación de áreas pequeñas y métodos de Monte Carlo. Aunque el procedimiento se describe con los códigos de 2020, el proceso para 2015 es similar, con pequeñas variaciones. Sin embargo, la esencia y los principios fundamentales son los mismos en ambos periodos. Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["respositodio-de-códigos.html", "Respositodio de códigos", " Respositodio de códigos En el siguiente enlace encontrará las rutinas de R desarrolladas para la estimación del IPM Descargar "],["librerias-de-r-y-otros-insumos.html", "Capítulo 2 Librerias de R y otros insumos", " Capítulo 2 Librerias de R y otros insumos En el siguiente apartado, se describe el conjunto de librerías utilizadas para el desarrollo de este proyecto, así como una breve descripción de las bases de datos empleadas. "],["librerías-utilizadas.html", "2.1 Librerías Utilizadas", " 2.1 Librerías Utilizadas 2.1.1 Manipulación y Transformación de Datos tidyverse: Conjunto de paquetes (incluyendo dplyr, ggplot2, tibble, readr, purrr, tidyr, y stringr) que facilitan la manipulación y visualización de datos de manera coherente y eficiente. data.table: Proporciona herramientas rápidas y eficientes para la manipulación de grandes conjuntos de datos tabulares. dplyr: Parte del tidyverse, facilita la manipulación de datos mediante verbos intuitivos como select, filter, mutate, summarize, y arrange. magrittr: Introduce el operador %&gt;%, permitiendo una escritura de código más clara y encadenada. purrr: Extiende las capacidades de programación funcional para trabajar con listas y vectores. furrr: Permite realizar operaciones paralelas utilizando purrr y future. stringr: Simplifica la manipulación de cadenas de caracteres mediante funciones intuitivas. labelled: Facilita la manipulación de datos etiquetados, comúnmente utilizados en encuestas y datos sociológicos. 2.1.2 Lectura y Escritura de Datos openxlsx: Permite la creación, lectura y manipulación de archivos Excel sin depender de software adicional. haven: Permite leer y escribir datos en formatos usados por otros programas estadísticos como SPSS, Stata y SAS. readstata13: Especializado en la lectura de archivos Stata versión 13, asegurando compatibilidad con datos antiguos. 2.1.3 Análisis de Datos de Encuestas survey: Proporciona herramientas para el análisis de datos de encuestas complejas, incluyendo ponderaciones y diseños de muestras. srvyr: Ofrece una interfaz más amigable basada en dplyr para trabajar con el paquete survey. TeachingSampling: Incluye métodos y herramientas para realizar muestreo en investigaciones educativas. samplesize4surveys: Facilita el cálculo del tamaño de muestra necesario para encuestas, asegurando resultados estadísticamente significativos. convey: Extiende survey para analizar medidas de desigualdad y pobreza en datos de encuestas. 2.1.4 Modelado y Análisis Estadístico rstan: Interfaz de R para Stan, que realiza modelado bayesiano avanzado, permitiendo la creación de modelos complejos. lme4: Proporciona herramientas para ajustar y analizar modelos lineales y no lineales de efectos mixtos. car: Incluye diversas herramientas para la regresión aplicada y diagnósticos de modelos. randomForest: Implementa algoritmos de bosque aleatorio para clasificación y regresión. caret: Ofrece una amplia gama de herramientas para la creación y validación de modelos de aprendizaje automático. nortest: Contiene pruebas para evaluar la normalidad de los datos, esencial en muchos análisis estadísticos. 2.1.5 Visualización de Datos ggplot2: Parte del tidyverse, es una potente herramienta para la visualización de datos basada en la gramática de gráficos. DataExplorer: Simplifica la exploración inicial y la generación de reportes de datos. thematic: Facilita la personalización de temas gráficos en ggplot2, permitiendo una estética consistente. patchwork: Permite combinar múltiples gráficos de ggplot2 en una única visualización coherente. tmap: Especializado en la creación de mapas temáticos, útil para visualizar datos geoespaciales. sf: Proporciona una estructura eficiente para manipular datos espaciales, facilitando la integración con ggplot2 y tmap. 2.1.6 Informes y Reproducibilidad printr: Mejora el formato de la impresión de resultados en R Markdown, haciendo los informes más legibles. knitr: Herramienta clave para la creación de informes dinámicos y reproducibles, integrando código y texto. 2.1.7 Integración con Otros Lenguajes reticulate: Facilita la interoperabilidad entre R y Python, permitiendo ejecutar código de Python dentro de un entorno de R. "],["bases-de-datos-utilizadas.html", "2.2 Bases de Datos Utilizadas", " 2.2 Bases de Datos Utilizadas Para este proyecto, se emplearon diversas bases de datos que incluyen: Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH): Proporciona información detallada sobre los ingresos y gastos de los hogares en México. Esta encuesta es crucial para entender los patrones de consumo y el bienestar económico de la población. Formulario Ampliado del Censo 2020: Ofrece datos sociodemográficos detallados a nivel de hogar y persona. Esta fuente es esencial para capturar una amplia gama de variables necesarias para el análisis multidimensional de la pobreza. Encuesta Intercensal de 2015: Complementa la información del censo con datos adicionales recopilados entre periodos censales. Proporciona una actualización intermedia de las condiciones demográficas y socioeconómicas. Imágenes Satelitales: Proveen datos geoespaciales esenciales para capturar la diversidad de las condiciones de vida en México. Estas imágenes ayudan a incorporar información espacial detallada en los modelos de estimación. Estas librerías y bases de datos son fundamentales para la implementación de la metodología de estimación del Índice de Pobreza Multidimensional (IPM) y otras carencias en los distintos municipios de México. "],["estructura-del-proyecto.html", "2.3 Estructura del Proyecto", " 2.3 Estructura del Proyecto Para el desarrollo de los scripts, se estructuró un proyecto en R que cuenta con la siguiente organización: rcodes: En esta carpeta se encuentran los diferentes códigos desarrollados para cada año. Dentro de ella, hay subcarpetas específicas para 2020 y 2015, correspondientes a los años en los que se realizaron los cálculos de los indicadores de carencias y del Índice de Pobreza Multidimensional (IPM). Dentro de estas subcarpetas, los scripts están enumerados en orden de ejecución, como se muestra en la imagen a continuación: Cada subcarpeta contiene una serie de scripts organizados secuencialmente, asegurando que el proceso de cálculo y análisis se realice de manera ordenada y reproducible. Esta estructura facilita la identificación y ejecución de cada paso necesario para obtener los resultados del análisis de pobreza multidimensional. input: Esta carpeta contiene dos subcarpetas, una para 2015 y otra para 2020. En cada una de estas encontraremos la base de datos de la ENIGH, la encuesta ampliada del censo para el 2020 o la encuesta intercensal para el 2015, además de las variables predictoras a nivel de municipios. También incluye un archivo en formato CSV que contiene las líneas de pobreza para el año que se esté procesando. output: Esta carpeta sigue la misma estructura, con subcarpetas para 2015 y 2020. En cada una de estas subcarpetas encontraremos la carpeta de iteraciones, donde están disponibles los resultados de las iteraciones realizadas por cada estado para cada uno de los municipios. También dispone de una subcarpeta de modelos que contiene los modelos estimados para cada una de las carencias, así como algunos gráficos de validación. Además, encontraremos la carpeta de intermedias necesarias para el procesamiento de información o para el almacenamiento de algunos resultados. shapefile: Aquí están disponibles los shapefiles para la realización de mapas y descarga de información satelital, que son utilizados como covariables en la implementación del modelo. Esta organización permite mantener un flujo de trabajo claro y estructurado, donde cada script y conjunto de datos desempeñan un papel específico en el proceso general, desde la preparación de datos hasta la generación de resultados finales. "],["lectura-y-consolidación-de-bases-de-datos.html", "Capítulo 3 Lectura y consolidación de bases de datos", " Capítulo 3 Lectura y consolidación de bases de datos En el siguiente apartado se describe el proceso de integración y ordenamiento de las bases de datos proporcionadas por la Universidad Nacional Autónoma de México (UNAM) y el Consejo Nacional de Evaluación de la Política de Desarrollo Social (CONEVAL). Este proceso tiene como propósito asegurar el adecuado desarrollo de los scripts necesarios para el análisis. "],["consolidando-la-base-de-datos-de-la-encuesta-ampliada-del-censo.html", "3.1 Consolidando la base de datos de la encuesta ampliada del censo", " 3.1 Consolidando la base de datos de la encuesta ampliada del censo Limpieza del espacio de trabajo Esta línea elimina todos los objetos del espacio de trabajo actual. Es útil para asegurar que no haya objetos residuales que puedan interferir con el nuevo análisis. rm(list = ls()) Carga de bibliotecas Estas líneas cargan diversas bibliotecas en R: library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(DataExplorer) library(haven) library(purrr) library(furrr) library(labelled) Definición de variables para validación Se define un vector validar_var que contiene una lista de nombres de variables. Estos nombres podrían ser utilizados más adelante para validar o verificar datos en el análisis. validar_var &lt;- c(&quot;ic_rezedu&quot;, &quot;ic_asalud&quot;, &quot;ic_segsoc&quot;, &quot;ic_cv&quot;, &quot;ic_sbv&quot;, &quot;ic_ali_nc&quot;, &quot;ictpc&quot;) Obtener la lista de archivos .dta Estas líneas generan listas de archivos con extensión .dta en los directorios especificados: file_muestra_censo_2020_estado: Contiene la lista de archivos .dta en el directorio SegSocial/SegSoc/. file_muestra_censo_2020_estado_complemento: Contiene la lista de archivos .dta en el directorio Complemento_SegSoc/. muestra_cuestionario_ampliado_censo_2020_estado: Contiene la lista de archivos .dta en el directorio IndicadoresCenso/. (file_muestra_censo_2020_estado &lt;- list.files( &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/&quot;, full.names = TRUE, pattern = &quot;dta$&quot; )) ## [1] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_01.dta&quot; ## [2] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_02.dta&quot; ## [3] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_03.dta&quot; ## [4] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_04.dta&quot; ## [5] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_05.dta&quot; ## [6] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_06.dta&quot; ## [7] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_07.dta&quot; ## [8] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_08.dta&quot; ## [9] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_09.dta&quot; ## [10] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_10.dta&quot; ## [11] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_11.dta&quot; ## [12] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_12.dta&quot; ## [13] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_13.dta&quot; ## [14] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_14.dta&quot; ## [15] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_15.dta&quot; ## [16] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_16.dta&quot; ## [17] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_17.dta&quot; ## [18] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_18.dta&quot; ## [19] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_19.dta&quot; ## [20] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_20.dta&quot; ## [21] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_21.dta&quot; ## [22] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_22.dta&quot; ## [23] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_23.dta&quot; ## [24] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_24.dta&quot; ## [25] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_25.dta&quot; ## [26] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_26.dta&quot; ## [27] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_27.dta&quot; ## [28] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_28.dta&quot; ## [29] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_29.dta&quot; ## [30] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_30.dta&quot; ## [31] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_31.dta&quot; ## [32] &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/base_personas_32.dta&quot; (file_muestra_censo_2020_estado_complemento &lt;- list.files( &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/&quot;, full.names = TRUE, pattern = &quot;dta$&quot; )) ## [1] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc01.dta&quot; ## [2] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc02.dta&quot; ## [3] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc03.dta&quot; ## [4] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc04.dta&quot; ## [5] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc05.dta&quot; ## [6] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc06.dta&quot; ## [7] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc07.dta&quot; ## [8] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc08.dta&quot; ## [9] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc09.dta&quot; ## [10] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc10.dta&quot; ## [11] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc11.dta&quot; ## [12] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc12.dta&quot; ## [13] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc13.dta&quot; ## [14] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc14.dta&quot; ## [15] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc15.dta&quot; ## [16] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc16.dta&quot; ## [17] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc17.dta&quot; ## [18] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc18.dta&quot; ## [19] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc19.dta&quot; ## [20] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc20.dta&quot; ## [21] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc21.dta&quot; ## [22] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc22.dta&quot; ## [23] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc23.dta&quot; ## [24] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc24.dta&quot; ## [25] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc25.dta&quot; ## [26] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc26.dta&quot; ## [27] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc27.dta&quot; ## [28] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc28.dta&quot; ## [29] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc29.dta&quot; ## [30] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc30.dta&quot; ## [31] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc31.dta&quot; ## [32] &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/Complemento_segsoc32.dta&quot; (muestra_cuestionario_ampliado_censo_2020_estado &lt;- list.files( &quot;../input/2020/muestra_ampliada/IndicadoresCenso/&quot;, full.names = TRUE, pattern = &quot;dta$&quot; )) ## [1] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_01.dta&quot; ## [2] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_02.dta&quot; ## [3] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_03.dta&quot; ## [4] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_04.dta&quot; ## [5] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_05.dta&quot; ## [6] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_06.dta&quot; ## [7] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_07.dta&quot; ## [8] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_08.dta&quot; ## [9] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_09.dta&quot; ## [10] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_10.dta&quot; ## [11] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_11.dta&quot; ## [12] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_12.dta&quot; ## [13] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_13.dta&quot; ## [14] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_14.dta&quot; ## [15] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_15.dta&quot; ## [16] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_16.dta&quot; ## [17] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_17.dta&quot; ## [18] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_18.dta&quot; ## [19] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_19.dta&quot; ## [20] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_20.dta&quot; ## [21] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_21.dta&quot; ## [22] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_22.dta&quot; ## [23] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_23.dta&quot; ## [24] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_24.dta&quot; ## [25] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_25.dta&quot; ## [26] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_26.dta&quot; ## [27] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_27.dta&quot; ## [28] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_28.dta&quot; ## [29] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_29.dta&quot; ## [30] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_30.dta&quot; ## [31] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_31.dta&quot; ## [32] &quot;../input/2020/muestra_ampliada/IndicadoresCenso/CarSoc_32.dta&quot; Conslidar bases de la muestra ampliada El código inicia creando un dataframe vacío y cargando las bibliotecas necesarias para el análisis de datos. Luego, itera sobre los archivos .dta para leer y combinar datos de diferentes fuentes, realizando uniones internas y ajustes según sea necesario. Cada conjunto de datos combinado se guarda en archivos .rds individuales durante el proceso. Después, los datos combinados de cada archivo se acumulan en un dataframe principal. Finalmente, el dataframe completo se guarda en un archivo .rds. Este procedimiento facilita la integración y el análisis de grandes volúmenes de datos provenientes de diversas fuentes. # Crear un dataframe vacío para almacenar los datos df &lt;- data.frame() # Iterar sobre cada archivo y leerlo for (ii in 1:32) { muestra_censo_2020_estado_ii &lt;- read_dta(file_muestra_censo_2020_estado[ii]) muestra_censo_2020_estado_complemento_ii &lt;- read_dta(file_muestra_censo_2020_estado_complemento[ii]) %&gt;% mutate(id_per = id_persona) muestra_cuestionario_ampliado_censo_2020_estado_ii &lt;- read_dta(muestra_cuestionario_ampliado_censo_2020_estado[ii]) muestra_censo &lt;- inner_join(muestra_censo_2020_estado_ii, muestra_censo_2020_estado_complemento_ii) %&gt;% select(-tamloc) %&gt;% inner_join(muestra_cuestionario_ampliado_censo_2020_estado_ii) saveRDS(muestra_censo, paste0(&quot;output/2020/muestra_censo/depto_&quot;, ii, &quot;.rds&quot;)) df &lt;- bind_rows(df, muestra_censo) cat(file_muestra_censo_2020_estado[ii], &quot;\\n&quot;) } # Guardar el dataframe en un archivo .rds saveRDS(df, file = &quot;output/2020/muestra_cuestionario_ampliado.rds&quot;) "],["cambiando-de-formato-.dta-a-.rds-la-base-de-la-enigh..html", "3.2 Cambiando de formato .dta a .rds la base de la ENIGH.", " 3.2 Cambiando de formato .dta a .rds la base de la ENIGH. El código se enfoca en la integración y preparación de datos de la Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). Primero, lee archivos de datos relevantes para los hogares y la pobreza desde fuentes especificadas. Luego, combina estos datos mediante uniones internas, basándose en identificadores comunes para asegurar una correcta correspondencia entre registros. Se ajustan algunas variables, como la creación de una nueva columna para los datos de ictpc, y se elimina una columna innecesaria. Finalmente, el dataframe combinado se guarda en un archivo .rds para su posterior análisis. Este proceso facilita la consolidación y preparación de los datos de la ENIGH para análisis más detallados. enigh_hogares &lt;- read_dta(&quot;../input/2020/enigh/base_hogares20.dta&quot;) enigh_pobreza &lt;- read_dta(&quot;../input/2020/enigh/pobreza_20.dta&quot;) %&gt;% select(-ent) n_distinct(enigh_pobreza$folioviv) n_distinct(enigh_hogares$folioviv) enigh &lt;- inner_join( enigh_pobreza, enigh_hogares, by = join_by( folioviv, foliohog, est_dis, upm, factor, rururb, ubica_geo ), suffix = c(&quot;_pers&quot;, &quot;_hog&quot;), ) enigh$ic_ali_nc enigh$ictpc_pers enigh$ictpc &lt;- enigh$ictpc_pers enigh$ic_segsoc #saveRDS(enigh, file = &quot;output/2020/enigh.rds&quot;) "],["proceso-para-la-descarga-de-imágenes-satelitales.html", "Capítulo 4 Proceso para la descarga de imágenes satelitales", " Capítulo 4 Proceso para la descarga de imágenes satelitales En esta sección, se describe el procedimiento para la descarga de imágenes satelitales utilizando R y su integración con Google Earth Engine (GEE). La capacidad de acceder a datos satelitales es esencial para diversos análisis geoespaciales, que van desde el monitoreo ambiental hasta la planificación urbana y la investigación científica. A través de la biblioteca rgee, que actúa como un puente entre R y GEE, se pueden aprovechar las vastas colecciones de datos satelitales y herramientas analíticas de GEE directamente desde el entorno de R. Esta integración permite una manipulación y análisis eficientes de grandes volúmenes de datos geoespaciales. En el siguiente proceso, se detallarán los pasos para configurar el entorno de trabajo, establecer la conexión con GEE, y realizar la descarga de las imágenes satelitales requeridas. "],["configurar-el-entorno-de-r.html", "4.1 Configurar el entorno de R", " 4.1 Configurar el entorno de R El código configura el entorno de trabajo en R para el análisis de datos geográficos y la conexión con Google Earth Engine. Primero, limpia el espacio de trabajo y establece un límite de memoria. Luego, carga varias bibliotecas necesarias para la manipulación de datos, el muestreo, la conexión con Google Earth Engine, y el manejo de datos geográficos. # rm(list = ls()) memory.limit(500000) ## [1] Inf library(tidyverse) library(sampling) library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(concaveman) library(geojsonio) library(magrittr) library(furrr) library(readr) El siguiente código configura el entorno de Python necesario para trabajar con Google Earth Engine a través de reticulate, asegurando que el entorno rgee_py esté correctamente instalado y configurado. Finalmente, inicializa la conexión con Google Earth Engine, habilitando el acceso a los recursos de Google Drive. Este proceso prepara el entorno para realizar análisis avanzados que combinan capacidades de R y Python. rgee_environment_dir &lt;-reticulate::conda_list() rgee_environment_dir &lt;- rgee_environment_dir[rgee_environment_dir$name == &quot;rgee_py&quot;,2] reticulate::use_python(rgee_environment_dir, required = T ) reticulate::py_config() library(reticulate) # Conexión con Python rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) rgee::ee_Initialize(drive = T) "],["preparación-de-la-shapefile.html", "4.2 Preparación de la Shapefile", " 4.2 Preparación de la Shapefile Se leen todos los archivos shapefile en el directorio especificado, seleccionando las columnas relevantes y utilizando procesamiento paralelo para mejorar la velocidad. # Leer y seleccionar shapefiles MEX &lt;- list.files(&quot;../shapefile/2020/&quot;, pattern = &quot;shp$&quot;, full.names = TRUE) %&gt;% furrr::future_map(~read_sf(.x) %&gt;% select(CVEGEO, NOM_ENT, NOMGEO, geometry), .progress = TRUE) 4.2.1 Definición del Sistema de Referencia Espacial Se define el sistema de referencia espacial WGS84 para la transformación de coordenadas. mi_crs &lt;- &quot;+proj=longlat +datum=WGS84&quot; 4.2.2 Transformación y Combinación de Shapefiles Se transforman las coordenadas de los shapefiles al sistema de referencia espacial WGS84 y se asegura que las geometrías sean de tipo MULTIPOLYGON. Luego, se combinan todos los shapefiles transformados en un único dataframe espacial. # Transformar y combinar shapefiles MEX_trans &lt;- map(MEX, ~st_transform(.x, crs = mi_crs)) MEX_trans &lt;- map(MEX_trans, ~st_cast(.x, &quot;MULTIPOLYGON&quot;)) MEX_trans &lt;- bind_rows(MEX_trans) 4.2.3 Corrección de la Codificación de los Nombres Geográficos Se convierte la codificación de los nombres geográficos a UTF-8 para asegurar la correcta visualización de caracteres especiales. # Corregir la codificación de los nombres geográficos MEX_trans$NOMGEO &lt;- iconv(MEX_trans$NOMGEO, to = &quot;UTF-8&quot;, from = &quot;ISO-8859-1&quot;) 4.2.4 Guardado del Shapefile Transformado Se escribe el shapefile transformado y unificado en un nuevo archivo en el directorio especificado. # Guardar el shapefile transformado st_write(MEX_trans, &quot;shapefile/2020/MEX_2020.shp&quot;, append = TRUE) 4.2.5 Limpieza del Espacio de Trabajo Se limpia el espacio de trabajo eliminando todos los objetos para liberar memoria. # Limpiar el espacio de trabajo rm(list = ls()) 4.2.6 Lectura de Shapefile Transformado Este script carga un shapefile transformado que contiene información geográfica de México, añade una columna con los códigos de entidad, y extrae datos de luminosidad nocturna del conjunto de datos NOAA/VIIRS para el año 2020, transformando las imágenes seleccionadas en una banda que representa la luminosidad promedio anual. Los datos extraídos se combinan en un solo dataframe, se filtran para identificar y manejar valores faltantes, y finalmente se guardan en un archivo RDS para su uso posterior. # Leer el shapefile transformado MEX &lt;- read_sf(&quot;shapefile/2020/MEX_2020.shp&quot;) # Añadir una nueva columna con la entidad (los primeros dos caracteres de CVEGEO) MEX %&lt;&gt;% mutate(ent = substr(CVEGEO, 1, 2)) # Selección de datos de luminosidad nocturna de la colección de imágenes NOAA/VIIRS # Enlace: https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_ANNUAL_V21#bands # Cargar la colección de imágenes VIIRS-DNB y filtrar por el año 2020 luces = ee$ImageCollection(&quot;NOAA/VIIRS/DNB/ANNUAL_V21&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2020-01-01&quot;, &quot;2021-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;average&quot;)) %&gt;% ee$ImageCollection$toBands() # Imprimir la estructura de la colección de imágenes para ver detalles de las bandas ee_print(luces) # Extraer valores de luminosidad nocturna para cada entidad en MEX MEX_luces &lt;- map(unique(MEX$ent), ~tryCatch(ee_extract( x = luces, y = MEX[c(&quot;ent&quot;, &quot;CVEGEO&quot;)] %&gt;% filter(ent == .x), ee$Reducer$mean(), sf = FALSE ), error = function(e) data.frame(ent = .x)) ) # Combinar los resultados en un solo dataframe MEX_luces %&lt;&gt;% bind_rows() # Filtrar y mostrar las filas con valores faltantes en la columna de luminosidad promedio MEX_luces %&gt;% filter(is.na(X20200101_average)) %&gt;% select(CVEGEO) # Guardar los datos de luminosidad en un archivo RDS saveRDS(MEX_luces, &quot;output/2020/Satelital/MEX_luces.rds&quot;) 4.2.7 Cobertura urbana y uso del suelo El código realiza el procesamiento de datos relacionados con el uso del suelo y la cobertura urbana y de cultivos en México para el año 2019. Primero, carga la colección de imágenes de la serie Proba-V Landcover, que proporciona datos sobre fracciones de cobertura urbana y de cultivos. Luego, se filtra esta colección para el año 2019 y se seleccionan las bandas relevantes para analizar estas fracciones. Posteriormente, el código extrae estas fracciones para cada entidad geográfica en México, calculando la media de las bandas seleccionadas y manejando errores que puedan surgir durante el proceso. Los resultados se combinan en un solo dataframe y se revisan los datos para identificar cualquier valor faltante en las fracciones de cultivos y cobertura urbana. Finalmente, el dataframe resultante se guarda en un archivo RDS para su uso futuro. tiposuelo = ee$ImageCollection(&quot;COPERNICUS/Landcover/100m/Proba-V-C3/Global&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2019-01-01&quot;, &quot;2019-12-31&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;urban-coverfraction&quot;, &quot;crops-coverfraction&quot;)) %&gt;% ee$ImageCollection$toBands() ee_print(tiposuelo) MEX_urbano_cultivo &lt;- map(unique(MEX$CVEGEO), ~tryCatch(ee_extract( x = tiposuelo, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(CVEGEO == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(CVEGEO = .x)) ) MEX_urbano_cultivo %&lt;&gt;% bind_rows() MEX_urbano_cultivo %&gt;% filter(is.na(X2019_crops.coverfraction)) %&gt;% select(CVEGEO) MEX_urbano_cultivo %&gt;% filter(is.na(X2019_urban.coverfraction)) %&gt;% select(CVEGEO) saveRDS(MEX_urbano_cultivo, &quot;output/2020/Satelital/MEX_urbano_cultivo.rds&quot;) 4.2.8 Distancia a hospitales El código realiza el análisis de la distancia a los hospitales utilizando datos satelitales para el año 2019. Primero, se carga la imagen que representa la accesibilidad a la atención médica, proporcionada por el dataset de Oxford. Luego, se extraen datos de accesibilidad para cada entidad geográfica en México, calculando la media de los valores de accesibilidad para cada área. Se maneja cualquier error potencial durante este proceso, garantizando que, en caso de problemas, se registre la entidad correspondiente sin datos. Posteriormente, los resultados se combinan en un solo dataframe y se revisa si hay valores faltantes en las medidas de accesibilidad. Finalmente, el dataframe con los datos de accesibilidad se guarda en un archivo RDS para su análisis posterior. dist_salud = ee$Image(&#39;Oxford/MAP/accessibility_to_healthcare_2019&#39;) ee_print(dist_salud) MEX_dist_salud &lt;- map(unique(MEX$CVEGEO), ~tryCatch(ee_extract( x = dist_salud, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(CVEGEO == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(CVEGEO = .x)) ) MEX_dist_salud %&lt;&gt;% bind_rows() MEX_dist_salud %&gt;% filter(is.na(accessibility )) %&gt;% select(CVEGEO) MEX_dist_salud %&gt;% filter(is.na(accessibility_walking_only)) %&gt;% select(CVEGEO) saveRDS(MEX_dist_salud, &quot;output/2020/Satelital/MEX_dist_salud.rds&quot;) 4.2.9 Global Human Modification (GHM) El código realiza el análisis de la modificación humana global (gHM) utilizando datos de satélites para el año 2016. Primero, se carga la colección de imágenes correspondiente al índice de Modificación Humana Global, que proporciona información sobre la alteración del entorno por actividades humanas. Luego, se extraen estos datos para cada entidad geográfica en México, calculando la media de los valores para cada área. Se maneja cualquier error que pueda surgir durante la extracción de datos, asegurando que, en caso de problemas, se registre la entidad sin datos. Después de combinar los resultados en un solo dataframe, se revisa si hay valores faltantes en el índice de modificación humana. Finalmente, se guarda el dataframe con los datos de modificación humana en un archivo RDS para su análisis futuro. CSP_gHM = ee$ImageCollection(&#39;CSP/HM/GlobalHumanModification&#39;) ee_print(CSP_gHM) MEX_GHM &lt;- map(unique(MEX$CVEGEO), ~tryCatch(ee_extract( x = CSP_gHM, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(CVEGEO == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(CVEGEO = .x)) ) MEX_GHM %&lt;&gt;% bind_rows() MEX_GHM %&gt;% filter(is.na(X2016_gHM )) %&gt;% select(CVEGEO) saveRDS(MEX_GHM, &quot;output/2020/Satelital/MEX_GHM.rds&quot;) "],["conslidar-y-guardar-las-variables-satelitales.html", "4.3 Conslidar y guardar las variables satelitales", " 4.3 Conslidar y guardar las variables satelitales El código se encarga de consolidar y guardar los datos de predictores a nivel estatal obtenidos a partir de diferentes archivos satelitales. Primero, lee todos los archivos .rds generados previamente desde un directorio específico y los combina en un solo dataframe mediante una unión completa (full_join). Luego, normaliza las variables numéricas escalándolas para asegurar que estén en una escala comparable. A continuación, renombra las columnas del dataframe para hacerlas más descriptivas y comprensibles, como “modifica_humana” para el índice de modificación humana y “luces_nocturnas” para la luminosidad nocturna. Se verifica si hay valores faltantes en el dataframe y se visualizan las filas con datos faltantes. Finalmente, guarda el dataframe consolidado y limpio en un archivo .rds para su uso posterior. statelevel_predictors_df &lt;- list.files(&quot;../output/2020/Satelital/&quot;, full.names = TRUE) %&gt;% map( ~ readRDS(file = .x)) %&gt;% reduce(., full_join) %&gt;% mutate_if(is.numeric, function(x) as.numeric(scale(x))) %&gt;% rename( cve_mun = CVEGEO, &quot;modifica_humana&quot; = X2016_gHM, &quot;acceso_hosp&quot; = accessibility, &quot;acceso_hosp_caminando&quot; = accessibility_walking_only, cubrimiento_urbano = X2019_urban.coverfraction, cubrimiento_cultivo = X2019_crops.coverfraction, luces_nocturnas = X20200101_average ) statelevel_predictors_df[apply(statelevel_predictors_df,1,function(x)any(is.na(x))),] %&gt;% view() Tabla 4.1: Variables satelitales descargadas ent cve_mun acceso_hosp acceso_hosp_caminando modifica_humana luces_nocturnas cubrimiento_cultivo cubrimiento_urbano 01 01001 -0.6813 -0.5599 0.4796 0.9925 0.7079 0.6553 01 01002 -0.5563 -0.3965 0.1182 -0.1495 0.4772 -0.1273 01 01003 -0.2991 -0.3885 -0.4109 -0.2306 -0.2745 -0.2943 01 01004 -0.6035 -0.3140 0.4977 -0.1221 0.7296 -0.0922 01 01005 -0.5637 -0.4701 0.2252 0.1767 0.3310 0.1532 01 01006 -0.8050 -0.7857 0.7677 0.1072 0.9624 -0.0263 01 01007 -0.5964 -0.6407 0.1519 -0.0469 0.8441 -0.1150 01 01008 0.4619 0.2202 -1.0825 -0.2670 -0.4528 -0.3755 01 01009 -0.6636 -0.6723 0.3384 -0.0860 0.1691 -0.0672 01 01010 -0.5932 -0.5854 -0.2819 -0.1655 1.1342 -0.2584 saveRDS(statelevel_predictors_df, &quot;input/2020/predictores/statelevel_predictors_satelite.rds&quot;) "],["integración-de-bases-de-datos-predictoras..html", "Capítulo 5 Integración de bases de datos predictoras.", " Capítulo 5 Integración de bases de datos predictoras. Este código se centra en la selección y procesamiento de variables de covariables a nivel municipal para el año 2020. A continuación, se describe cada bloque del código en términos generales. Limpieza del Entorno de R Para asegurar un entorno limpio y evitar conflictos con datos previos, se eliminan todas las variables existentes en el entorno de R usando rm(list = ls()). Esto permite que el análisis comience sin datos residuales que puedan interferir. rm(list = ls()) Carga de librerías Se cargan varias librerías esenciales para la manipulación y análisis de datos. tidyverse se utiliza para la manipulación de datos y visualización; data.table para trabajar con grandes conjuntos de datos en formato de tabla; openxlsx para la lectura y escritura de archivos Excel; magrittr proporciona el operador de tubería (%&gt;%) para una sintaxis más clara; DataExplorer ayuda en la exploración y visualización de datos; haven permite la importación de archivos de datos en formatos como Stata; purrr se utiliza para aplicar funciones a listas y vectores; y labelled facilita el manejo de variables etiquetadas. Finalmente, cat(\"\\f\") limpia la consola para un entorno de trabajo más ordenado. library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(DataExplorer) library(haven) library(purrr) library(labelled) cat(&quot;\\f&quot;) Lectura de Bases de Datos de Contexto La sección de lectura de bases de datos comienza cargando los datos satelitales desde un archivo RDS. Posteriormente, se extraen los códigos municipales únicos de esta base de datos, lo que permite identificar todos los municipios representados en los datos satelitales. Luego, se carga la base de datos “Contexto_mun20” en formato Stata, que contiene información de diversas variables a nivel municipal. Al convertir estas variables en factores y revisar el número de categorías distintas en cada una, se puede entender mejor la estructura de los datos. Este paso es crucial para preparar los datos para el análisis posterior. satelital &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_satelite.rds&quot;) # Obtener los códigos municipales únicos de la base de datos satelital cod_mun &lt;- satelital %&gt;% distinct(cve_mun) # Cargar la base de datos &quot;Contexto_mun20&quot; en formato Stata y renombrar la columna &quot;CVEGEO&quot; a &quot;cve_mun&quot; Contexto_mun &lt;- read_dta(&quot;../input/2020/predictores/contexto_2020.dta&quot;) Contexto_mun %&gt;% as_factor() %&gt;% apply(., 2, n_distinct) %&gt;% sort() ## elec_mun19 elec_mun20 smg1 ## 2 2 2 ## ql_porc_cpa_urb ql_porc_cpa_rur dec_geologicas ## 5 6 9 ## dec_hidrometeo vabpc_15_19 desem_15_20 ## 20 32 32 ## transf_gobpc_15_20 itlpis_15_20 derhab_pea_15_20 ## 32 32 32 ## remespc_15_20 acc_muybajo acc_medio ## 32 275 887 ## prop_sucban prop_hospitales edad65mas_urb ## 1236 1248 1272 ## edad65mas_rur altitud1000 porc_10sal_urb ## 1429 1447 1478 ## pob_ind_urb porc_10sal_rur TasaDesempurb ## 1591 1614 1617 ## pob_urb porc_patnoagrnocal_urb porc_rur ## 1619 1626 1632 ## porc_urb porc_cpagr_urb porc_hogremesas_urb ## 1632 1647 1652 ## porc_jub_urb po_urb porc_expurb ## 1652 1656 1656 ## porc_norep_ing_urb porc_hogayotr_urb porc_ing_ilpi_urb ## 1656 1657 1658 ## porc_ic_asalud_urb porc_hogprogob_urb prom_esc_rel_urb ## 1658 1659 1659 ## acc_alto edad65mas porc_patnoagrnocal_rur ## 1826 1883 2005 ## acc_bajo TasaDesemprur pob_ind_rur ## 2024 2084 2168 ## pob_rur porc_jub_rur porc_hogremesas_rur ## 2264 2310 2319 ## acc_muyalto porc_hogayotr_rur porc_cpagr_rur ## 2331 2343 2345 ## porc_exprur porc_norep_ing_rur t_incdelic1 ## 2356 2368 2382 ## porc_ing_ilpi_rur pob_tot prop_clues ## 2386 2390 2401 ## po_rur porc_hogprogob_rur porc_ic_asalud_rur ## 2405 2408 2417 ## prom_esc_rel_rur porc_segsoc15 gini15m ## 2439 2449 2451 ## porc_ali15 plp15 ictpc15 ## 2452 2452 2453 ## cve_mun ## 2466 Procesamiento y Escalado de Variables En esta sección, el código procesa la base de datos Contexto_mun para asegurar que las variables categóricas sean correctamente identificadas y que las variables numéricas sean escaladas. Primero, se convierte toda la base de datos a factores usando as_factor(). Luego, se seleccionan variables específicas (elec_mun19, elec_mun20, smg1, ql_porc_cpa_urb, ql_porc_cpa_rur, dec_geologicas, dec_hidrometeo) y se aseguran que se manejen como factores. Además, cualquier columna numérica se escala usando scale(), lo que transforma estas variables para que tengan media cero y desviación estándar uno. Contexto_mun %&lt;&gt;% as_factor() %&gt;% mutate_at( .vars = c( &quot;elec_mun19&quot;, &quot;elec_mun20&quot;, &quot;smg1&quot;, &quot;ql_porc_cpa_urb&quot;, &quot;ql_porc_cpa_rur&quot;, &quot;dec_geologicas&quot;, &quot;dec_hidrometeo&quot; ), as.factor ) %&gt;% mutate_if(is.numeric, function(x) as.numeric(scale(x))) Identificación de Códigos Municipales Faltantes El siguiente paso implica una anti_join entre cod_mun y Contexto_mun. Esto permite identificar los códigos municipales presentes en la base de datos satelital que no están presentes en Contexto_mun. Esta operación es útil para detectar inconsistencias o faltantes en los datos municipales que podrían impactar en los análisis posteriores. anti_join(cod_mun, Contexto_mun) %&gt;% tba(cap = &quot;Tabla de municipios sin información&quot;) Tabla 5.1: Tabla de municipios sin información cve_mun 04012 07125 29048 Verificación de Columnas Completas Para garantizar que sólo se utilicen columnas sin valores faltantes, se aplica una función que verifica si alguna columna en Contexto_mun tiene valores NA. El resultado es almacenado en paso, y la frecuencia de columnas completas es mostrada con table(paso). Este paso asegura que las columnas usadas en el análisis posterior están completas y no contienen datos faltantes. paso &lt;- apply(Contexto_mun, 2, function(x) !any(is.na(x))) table(paso) FALSE TRUE 2 65 Combinación de Bases de Datos Se realiza una inner_join entre las bases de datos satelital y Contexto_mun, utilizando únicamente las columnas sin valores faltantes identificadas previamente. Esta combinación asegura que se unan únicamente los datos completos de ambas bases, resultando en una base de datos consolidada statelevel_predictors. Las dimensiones de esta nueva base de datos se obtienen usando dim(statelevel_predictors), proporcionando información sobre el tamaño de los datos resultantes. statelevel_predictors &lt;- inner_join(satelital, Contexto_mun[, paso]) dim(statelevel_predictors) ## [1] 2466 72 Guardado de la Base de Datos Final Finalmente, la base de datos consolidada statelevel_predictors se guarda en un archivo RDS para su uso posterior, utilizando saveRDS(). Este archivo es almacenado en la ruta especificada, permitiendo que los datos procesados estén disponibles para futuros análisis sin necesidad de repetir los pasos de limpieza y combinación. ```{r. eval = FALSE} saveRDS(statelevel_predictors,file = “../input/2020/predictores/statelevel_predictors_df.rds”) ``` "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
