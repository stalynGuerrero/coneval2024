[["index.html", "Manual de procesamiento del indicador de pobreza multidimensional mediante el uso de estimación de áreas pequeñas para México 2020 Introducción", " Manual de procesamiento del indicador de pobreza multidimensional mediante el uso de estimación de áreas pequeñas para México 2020 Andrés Gutiérrez1, Stalyn Guerrero2 2024-07-30 Introducción Este manual se desarrolla como una herramienta para detallar el paso a paso seguido por la Comisión Económica para América Latina y el Caribe (CEPAL) en la obtención de estimaciones del indicador multidimensional de pobreza en México. El proceso utiliza técnicas de estimación de áreas pequeñas y métodos de Monte Carlo. Aunque el procedimiento se describe con los códigos de 2020, el proceso para 2015 es similar, con pequeñas variaciones. Sin embargo, la esencia y los principios fundamentales son los mismos en ambos periodos. Respositodio de códigos En el siguiente enlace encontrará las rutinas de R desarrolladas para la estimación del IPM Descargar Estructura del Proyecto Para el desarrollo de los scripts, se estructuró un proyecto en R que cuenta con la siguiente organización: rcodes: En esta carpeta se encuentran los diferentes códigos desarrollados para cada año. Dentro de ella, hay subcarpetas específicas para 2020 y 2015, correspondientes a los años en los que se realizaron los cálculos de los indicadores de carencias y del Índice de Pobreza Multidimensional (IPM). Dentro de estas subcarpetas, los scripts están enumerados en orden de ejecución, como se muestra en la imagen a continuación: Cada subcarpeta contiene una serie de scripts organizados secuencialmente, asegurando que el proceso de cálculo y análisis se realice de manera ordenada y reproducible. Esta estructura facilita la identificación y ejecución de cada paso necesario para obtener los resultados del análisis de pobreza multidimensional. input: Esta carpeta contiene dos subcarpetas, una para 2015 y otra para 2020. En cada una de estas encontraremos la base de datos de la ENIGH, la encuesta ampliada del censo para el 2020 o la encuesta intercensal para el 2015, además de las variables predictoras a nivel de municipios. También incluye un archivo en formato CSV que contiene las líneas de pobreza para el año que se esté procesando. output: Esta carpeta sigue la misma estructura, con subcarpetas para 2015 y 2020. En cada una de estas subcarpetas encontraremos la carpeta de iteraciones, donde están disponibles los resultados de las iteraciones realizadas por cada estado para cada uno de los municipios. También dispone de una subcarpeta de modelos que contiene los modelos estimados para cada una de las carencias, así como algunos gráficos de validación. Además, encontraremos la carpeta de intermedias necesarias para el procesamiento de información o para el almacenamiento de algunos resultados. shapefile: Aquí están disponibles los shapefiles para la realización de mapas y descarga de información satelital, que son utilizados como covariables en la implementación del modelo. Esta organización permite mantener un flujo de trabajo claro y estructurado, donde cada script y conjunto de datos desempeñan un papel específico en el proceso general, desde la preparación de datos hasta la generación de resultados finales. Librerias de R y otros insumos En el siguiente apartado, se describe el conjunto de librerías utilizadas para el desarrollo de este proyecto, así como una breve descripción de las bases de datos empleadas. Librerías Utilizadas Manipulación y Transformación de Datos tidyverse: Conjunto de paquetes (incluyendo dplyr, ggplot2, tibble, readr, purrr, tidyr, y stringr) que facilitan la manipulación y visualización de datos de manera coherente y eficiente. data.table: Proporciona herramientas rápidas y eficientes para la manipulación de grandes conjuntos de datos tabulares. dplyr: Parte del tidyverse, facilita la manipulación de datos mediante verbos intuitivos como select, filter, mutate, summarize, y arrange. magrittr: Introduce el operador %&gt;%, permitiendo una escritura de código más clara y encadenada. purrr: Extiende las capacidades de programación funcional para trabajar con listas y vectores. furrr: Permite realizar operaciones paralelas utilizando purrr y future. stringr: Simplifica la manipulación de cadenas de caracteres mediante funciones intuitivas. labelled: Facilita la manipulación de datos etiquetados, comúnmente utilizados en encuestas y datos sociológicos. Lectura y Escritura de Datos openxlsx: Permite la creación, lectura y manipulación de archivos Excel sin depender de software adicional. haven: Permite leer y escribir datos en formatos usados por otros programas estadísticos como SPSS, Stata y SAS. readstata13: Especializado en la lectura de archivos Stata versión 13, asegurando compatibilidad con datos antiguos. Análisis de Datos de Encuestas survey: Proporciona herramientas para el análisis de datos de encuestas complejas, incluyendo ponderaciones y diseños de muestras. srvyr: Ofrece una interfaz más amigable basada en dplyr para trabajar con el paquete survey. TeachingSampling: Incluye métodos y herramientas para realizar muestreo en investigaciones educativas. samplesize4surveys: Facilita el cálculo del tamaño de muestra necesario para encuestas, asegurando resultados estadísticamente significativos. convey: Extiende survey para analizar medidas de desigualdad y pobreza en datos de encuestas. Modelado y Análisis Estadístico rstan: Interfaz de R para Stan, que realiza modelado bayesiano avanzado, permitiendo la creación de modelos complejos. lme4: Proporciona herramientas para ajustar y analizar modelos lineales y no lineales de efectos mixtos. car: Incluye diversas herramientas para la regresión aplicada y diagnósticos de modelos. randomForest: Implementa algoritmos de bosque aleatorio para clasificación y regresión. caret: Ofrece una amplia gama de herramientas para la creación y validación de modelos de aprendizaje automático. nortest: Contiene pruebas para evaluar la normalidad de los datos, esencial en muchos análisis estadísticos. Visualización de Datos ggplot2: Parte del tidyverse, es una potente herramienta para la visualización de datos basada en la gramática de gráficos. DataExplorer: Simplifica la exploración inicial y la generación de reportes de datos. thematic: Facilita la personalización de temas gráficos en ggplot2, permitiendo una estética consistente. patchwork: Permite combinar múltiples gráficos de ggplot2 en una única visualización coherente. tmap: Especializado en la creación de mapas temáticos, útil para visualizar datos geoespaciales. sf: Proporciona una estructura eficiente para manipular datos espaciales, facilitando la integración con ggplot2 y tmap. Informes y Reproducibilidad printr: Mejora el formato de la impresión de resultados en R Markdown, haciendo los informes más legibles. knitr: Herramienta clave para la creación de informes dinámicos y reproducibles, integrando código y texto. Integración con Otros Lenguajes reticulate: Facilita la interoperabilidad entre R y Python, permitiendo ejecutar código de Python dentro de un entorno de R. Bases de Datos Utilizadas Para este proyecto, se emplearon diversas bases de datos que incluyen: Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH): Proporciona información detallada sobre los ingresos y gastos de los hogares en México. Esta encuesta es crucial para entender los patrones de consumo y el bienestar económico de la población. Formulario Ampliado del Censo 2020: Ofrece datos sociodemográficos detallados a nivel de hogar y persona. Esta fuente es esencial para capturar una amplia gama de variables necesarias para el análisis multidimensional de la pobreza. Encuesta Intercensal de 2015: Complementa la información del censo con datos adicionales recopilados entre periodos censales. Proporciona una actualización intermedia de las condiciones demográficas y socioeconómicas. Imágenes Satelitales: Proveen datos geoespaciales esenciales para capturar la diversidad de las condiciones de vida en México. Estas imágenes ayudan a incorporar información espacial detallada en los modelos de estimación. Estas librerías y bases de datos son fundamentales para la implementación de la metodología de estimación del Índice de Pobreza Multidimensional (IPM) y otras carencias en los distintos municipios de México. Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["indicadores-estimados.html", "Indicadores estimados", " Indicadores estimados En esta sección se presentan los diferentes indicadores utilizados para medir y analizar la situación de pobreza y vulnerabilidad en la población. A continuación, se describen los indicadores estimados: Población en situación de pobreza multidimensional (I): 13_Estimacion_ent_02.R, 14_estimacion_municipios_ipm_pobreza.R y 15_estimacion_municipios_ipm_pobreza_error.R Población en situación de pobreza moderada: 13_Estimacion_ent_02.R, 14_estimacion_municipios_ipm_pobreza.R y 15_estimacion_municipios_ipm_pobreza_error.R Población en situación de pobreza extrema: 13_Estimacion_ent_02.R, 14_estimacion_municipios_ipm_pobreza.R y 15_estimacion_municipios_ipm_pobreza_error.R Población vulnerable por carencias sociales (II): 13_Estimacion_ent_02.R, 14_estimacion_municipios_ipm_pobreza.R y 15_estimacion_municipios_ipm_pobreza_error.R Población vulnerable por ingresos (III): 13_Estimacion_ent_02.R, 14_estimacion_municipios_ipm_pobreza.R y 15_estimacion_municipios_ipm_pobreza_error.R Población no pobre multidimensional y no vulnerable (IV): 13_Estimacion_ent_02.R, 14_estimacion_municipios_ipm_pobreza.R y 15_estimacion_municipios_ipm_pobreza_error.R Población con al menos una carencia social (tol_ic_1): 17_estimacion_municipios_carencias.R y 18_estimacion_municipios_carencias_error.R Población con al menos tres carencias sociales (tol_ic_2): 17_estimacion_municipios_carencias.R y 18_estimacion_municipios_carencias_error.R Carencia por acceso a la seguridad social (ic_segsoc): 17_estimacion_municipios_carencias.R y 18_estimacion_municipios_carencias_error.R Carencia por acceso a la alimentación nutritiva y de calidad (ic_ali_nc): 17_estimacion_municipios_carencias.R y 18_estimacion_municipios_carencias_error.R Población con ingreso inferior a la línea de pobreza extrema por ingresos (pobrea_li): 17_estimacion_municipios_carencias.R y 18_estimacion_municipios_carencias_error.R Población con ingreso inferior a la línea de pobreza por ingresos (pobrea_lp): 17_estimacion_municipios_carencias.R y 18_estimacion_municipios_carencias_error.R Rezago educativo: 20_Estimacion_directa_censo.R Carencia por acceso a los servicios de salud: 20_Estimacion_directa_censo.R Carencia por calidad y espacios de la vivienda: 20_Estimacion_directa_censo.R Carencia por acceso a los servicios básicos en la vivienda: 20_Estimacion_directa_censo.R "],["union_bases.html", "00_union_bases.R", " 00_union_bases.R Para la ejecución del presente archivo, debe abrir el archivo 00_union_bases.R disponible en la ruta Rcodes/2020/00_union_bases.R. El código comienza limpiando el entorno de R y cargando varias bibliotecas esenciales para la manipulación y análisis de datos. Posteriormente, define un conjunto de variables a validar y obtiene listas de archivos de datos en formato .dta correspondientes a diferentes conjuntos de datos del censo 2020. A continuación, el código itera sobre estos archivos para leer, combinar y almacenar los datos de cada estado en archivos .rds individuales. Estos datos se combinan en un único dataframe que se guarda para su uso posterior. En la sección final, el código se centra en los datos de la ENIGH 2020. Lee los archivos de hogares y pobreza, y los combina utilizando un inner_join. Se seleccionan y renombran variables clave para asegurar la consistencia en el análisis. Finalmente, el dataframe combinado se guarda en un archivo .rds para facilitar el acceso y análisis futuros. Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de R eliminando todos los objetos y se ejecuta el recolector de basura para liberar memoria. rm(list = ls()) gc() Se cargan varias bibliotecas esenciales para la manipulación y análisis de datos, incluyendo tidyverse, data.table, haven y otras. library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(DataExplorer) library(haven) library(purrr) library(furrr) library(labelled) cat(&quot;\\f&quot;) Configuración de la Memoria Se define un límite para la memoria RAM a utilizar, en este caso 250 GB. memory.limit(250000000) Definición de Variables y Obtención de Archivos Para llevar a cabo la validación y procesamiento de datos del censo 2020, se define un conjunto de variables que serán validadas. Estas variables incluyen indicadores clave como rezago educativo (ic_rezedu), acceso a servicios de salud (ic_asalud), seguridad social (ic_segsoc), calidad de la vivienda (ic_cv), servicios básicos en la vivienda (ic_sbv), acceso a alimentación nutritiva y de calidad (ic_ali_nc), y el índice de pobreza multidimensional (ictpc). validar_var &lt;- c( &quot;ic_rezedu&quot;, &quot;ic_asalud&quot;, &quot;ic_segsoc&quot;, &quot;ic_cv&quot;, &quot;ic_sbv&quot;, &quot;ic_ali_nc&quot;, &quot;ictpc&quot; ) Posteriormente, se obtienen listas de archivos con extensión .dta, que corresponden a diferentes conjuntos de datos del censo 2020. Estos archivos se organizan en tres categorías principales: Archivos de muestra del censo 2020 por estado: Se buscan archivos en la ruta ../input/2020/muestra_ampliada/SegSocial/SegSoc/ que contengan información de seguridad social. file_muestra_censo_2020_estado &lt;- list.files( &quot;../input/2020/muestra_ampliada/SegSocial/SegSoc/&quot;, full.names = TRUE, pattern = &quot;dta$&quot; ) Archivos complementarios de muestra del censo 2020 por estado: Se buscan archivos adicionales en la ruta ../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/ que contienen datos complementarios de seguridad social. file_muestra_censo_2020_estado_complemento &lt;- list.files( &quot;../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/&quot;, full.names = TRUE, pattern = &quot;dta$&quot; ) Archivos de cuestionario ampliado del censo 2020 por estado: Se buscan archivos en la ruta ../input/2020/muestra_ampliada/IndicadoresCenso/ que incluyen indicadores generales del censo. muestra_cuestionario_ampliado_censo_2020_estado &lt;- list.files( &quot;../input/2020/muestra_ampliada/IndicadoresCenso/&quot;, full.names = TRUE, pattern = &quot;dta$&quot; ) Estas listas de archivos permiten acceder y gestionar de manera eficiente los datos necesarios para el análisis y validación de los indicadores de pobreza y carencias sociales del censo 2020. Lectura y Combinación de Datos del Censo df &lt;- data.frame() for (ii in 1:32) { muestra_censo_2020_estado_ii &lt;- read_dta(file_muestra_censo_2020_estado[ii]) muestra_censo_2020_estado_complemento_ii &lt;- read_dta(file_muestra_censo_2020_estado_complemento[ii]) %&gt;% mutate(id_per = id_persona) muestra_cuestionario_ampliado_censo_2020_estado_ii &lt;- read_dta(muestra_cuestionario_ampliado_censo_2020_estado[ii]) muestra_censo &lt;- inner_join(muestra_censo_2020_estado_ii, muestra_censo_2020_estado_complemento_ii) %&gt;% select(-tamloc) %&gt;% inner_join(muestra_cuestionario_ampliado_censo_2020_estado_ii) saveRDS(muestra_censo, paste0(&quot;../output/2020/muestra_censo/depto_&quot;, ii, &quot;.rds&quot;)) df &lt;- bind_rows(df, muestra_censo) cat(file_muestra_censo_2020_estado[ii], &quot;\\n&quot;) } Se iteran los archivos de datos del censo, se combinan y se almacenan en archivos .rds individuales y en un dataframe único. saveRDS(df, file = &quot;../output/2020/muestra_cuestionario_ampliado.rds&quot;) Se guarda el dataframe combinado en un archivo .rds. Lectura y Combinación de Datos de la ENIGH enigh_hogares &lt;- read_dta(&quot;../input/2020/enigh/base_hogares20.dta&quot;) enigh_pobreza &lt;- read_dta(&quot;../input/2020/enigh/pobreza_20.dta&quot;) %&gt;% select(-ent) Se leen los datos de hogares y pobreza de la ENIGH 2020. n_distinct(enigh_pobreza$folioviv) n_distinct(enigh_hogares$folioviv) Se verifica la unicidad de los identificadores de viviendas. enigh &lt;- inner_join( enigh_pobreza, enigh_hogares, by = join_by( folioviv, foliohog, est_dis, upm, factor, rururb, ubica_geo ), suffix = c(&quot;_pers&quot;, &quot;_hog&quot;) ) Se combinan los datos de pobreza y hogares utilizando un inner_join. Selección y Renombramiento de Variables enigh$ic_ali_nc enigh$ictpc_pers enigh$ictpc &lt;- enigh$ictpc_pers enigh$ic_segsoc Se seleccionan y renombran variables clave para asegurar la consistencia en el análisis. Guardado del DataFrame Combinado saveRDS(enigh, file = &quot;../output/2020/enigh.rds&quot;) Se guarda el dataframe combinado en un archivo .rds para facilitar el acceso y análisis futuros. "],["descarga_satelitales.html", "01_Descarga_satelitales.R", " 01_Descarga_satelitales.R Para la ejecución del presente archivo, debe abrir el archivo 01_Descarga_satelitales.R disponible en la ruta Rcodes/2020/01_Descarga_satelitales.R. Este código en R se enfoca en el procesamiento y análisis de datos geoespaciales utilizando una combinación de bibliotecas y herramientas de Google Earth Engine. La primera parte del script configura el entorno de trabajo, carga las librerías necesarias, y establece la conexión con Python mediante la biblioteca reticulate, esencial para trabajar con rgee, que es la interfaz de R para Google Earth Engine. En la segunda parte, se realiza la limpieza y transformación de datos espaciales. Se cargan y transforman archivos shapefiles de México a un sistema de referencia de coordenadas adecuado y se convierten a un formato MULTIPOLYGON. Luego, se extraen y procesan datos de imágenes satelitales para diversas variables como luminosidad, urbanización, y distancia a servicios de salud utilizando colecciones de imágenes de Google Earth Engine. Estos datos se agrupan por región y se guardan en archivos RDS para su análisis posterior. Finalmente, se combinan todos los datos procesados en un único dataframe y se escalan para prepararlos para su uso en modelos o análisis adicionales. Lectura de libreías rm(list = ls()) memory.limit(500000) library(tidyverse) library(sampling) library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(concaveman) library(geojsonio) library(magrittr) library(furrr) library(readr) configuración inicial de Python rgee_environment_dir &lt;-reticulate::conda_list() rgee_environment_dir &lt;- rgee_environment_dir[rgee_environment_dir$name == &quot;rgee_py&quot;,2] reticulate::use_python(rgee_environment_dir, required = T ) reticulate::py_config() library(reticulate) # Conexión con Python rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) rgee::ee_Initialize(drive = T) Arreglar la shape # plan(multisession, workers = 2) # # MEX &lt;- list.files(&quot;shapefile/2020/&quot;, pattern = &quot;shp$&quot;, # full.names = TRUE) %&gt;% # furrr::future_map(~read_sf(.x) %&gt;% # select(CVEGEO, NOM_ENT, NOMGEO,geometry), # .progress = TRUE) # # mi_crs &lt;- &quot;+proj=longlat +datum=WGS84&quot; # # # Transforma el polígono al nuevo CRS # MEX_trans &lt;- map(MEX, ~st_transform(.x, crs = mi_crs)) # MEX_trans &lt;- map(MEX_trans, ~st_cast(.x, &quot;MULTIPOLYGON&quot;)) # # MEX_trans &lt;- bind_rows(MEX_trans) # # # # stringi::stri_enc_detect(MEX_trans$NOMGEO) # # MEX_trans$NOMGEO &lt;- iconv(MEX_trans$NOMGEO, # to = &quot;UTF-8&quot;, from = &quot;ISO-8859-1&quot;) # # # st_write(MEX_trans, # &quot;shapefile/2020/MEX_2020.shp&quot;, # append = TRUE) # # rm(list = ls()) MEX &lt;- read_sf(&quot;../shapefile/2020/MEX_2020.shp&quot;) MEX %&lt;&gt;% mutate(ent = substr(CVEGEO,1,2)) Descargar las imegenes satelitales #https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_ANNUAL_V21#bands luces = ee$ImageCollection(&quot;NOAA/VIIRS/DNB/ANNUAL_V21&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2020-01-01&quot;, &quot;2021-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;average&quot;)) %&gt;% ee$ImageCollection$toBands() ee_print(luces) MEX_luces &lt;- map(unique(MEX$ent), ~tryCatch(ee_extract( x = luces, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(ent == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(ent = .x)) ) MEX_luces %&lt;&gt;% bind_rows() MEX_luces %&gt;% filter(is.na(X20200101_average)) %&gt;% select(CVEGEO) saveRDS(MEX_luces, &quot;../output/2020/Satelital/MEX_luces.rds&quot;) ################# ### Urbanismo ### ################# tiposuelo = ee$ImageCollection(&quot;COPERNICUS/Landcover/100m/Proba-V-C3/Global&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2019-01-01&quot;, &quot;2019-12-31&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;urban-coverfraction&quot;, &quot;crops-coverfraction&quot;)) %&gt;% ee$ImageCollection$toBands() ee_print(tiposuelo) MEX_urbano_cultivo &lt;- map(unique(MEX$CVEGEO), ~tryCatch(ee_extract( x = tiposuelo, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(CVEGEO == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(CVEGEO = .x)) ) MEX_urbano_cultivo %&lt;&gt;% bind_rows() MEX_urbano_cultivo %&gt;% filter(is.na(X2019_crops.coverfraction)) %&gt;% select(CVEGEO) MEX_urbano_cultivo %&gt;% filter(is.na(X2019_urban.coverfraction)) %&gt;% select(CVEGEO) saveRDS(MEX_urbano_cultivo, &quot;../output/2020/Satelital/MEX_urbano_cultivo.rds&quot;) ################# ### Distancia a hospitales ### ################# dist_salud = ee$Image(&#39;Oxford/MAP/accessibility_to_healthcare_2019&#39;) ee_print(dist_salud) MEX_dist_salud &lt;- map(unique(MEX$CVEGEO), ~tryCatch(ee_extract( x = dist_salud, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(CVEGEO == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(CVEGEO = .x)) ) MEX_dist_salud %&lt;&gt;% bind_rows() MEX_dist_salud %&gt;% filter(is.na(accessibility )) %&gt;% select(CVEGEO) MEX_dist_salud %&gt;% filter(is.na(accessibility_walking_only)) %&gt;% select(CVEGEO) saveRDS(MEX_dist_salud, &quot;../output/2020/Satelital/MEX_dist_salud.rds&quot;) ################# # CSP gHM: Global Human Modification ################# CSP_gHM = ee$ImageCollection(&#39;CSP/HM/GlobalHumanModification&#39;) ee_print(CSP_gHM) MEX_GHM &lt;- map(unique(MEX$CVEGEO), ~tryCatch(ee_extract( x = CSP_gHM, y = MEX[c(&quot;ent&quot;,&quot;CVEGEO&quot;)] %&gt;% filter(CVEGEO == .x), ee$Reducer$mean(), sf = FALSE ) , error = function(e)data.frame(CVEGEO = .x)) ) MEX_GHM %&lt;&gt;% bind_rows() MEX_GHM %&gt;% filter(is.na(X2016_gHM )) %&gt;% select(CVEGEO) saveRDS(MEX_GHM, &quot;../output/2020/Satelital/MEX_GHM.rds&quot;) Unir y guardar los resultados de las descargas realizadas statelevel_predictors_df &lt;- list.files(&quot;../output/2020/Satelital/&quot;, full.names = TRUE) %&gt;% map( ~ readRDS(file = .x)) %&gt;% reduce(., full_join) %&gt;% mutate_if(is.numeric, function(x) as.numeric(scale(x))) %&gt;% rename( cve_mun = CVEGEO, &quot;modifica_humana&quot; = X2016_gHM, &quot;acceso_hosp&quot; = accessibility, &quot;acceso_hosp_caminando&quot; = accessibility_walking_only, cubrimiento_urbano = X2019_urban.coverfraction, cubrimiento_cultivo = X2019_crops.coverfraction, luces_nocturnas = X20200101_average ) statelevel_predictors_df[apply(statelevel_predictors_df,1,function(x)any(is.na(x))),] %&gt;% view() saveRDS(statelevel_predictors_df, &quot;input/2020/predictores/statelevel_predictors_satelite.rds&quot;) "],["union_predictores.html", "02_Union_predictores.R", " 02_Union_predictores.R Para la ejecución del presente archivo, debe abrir el archivo 02_Union_predictores.R disponible en la ruta Rcodes/2020/02_Union_predictores.R. Este script en R está orientado a la integración y limpieza de bases de datos a nivel municipal para el año 2020, combinando datos satelitales y de contexto. Primero, el entorno de trabajo se limpia y se cargan las bibliotecas necesarias para la manipulación y análisis de datos, incluyendo tidyverse, data.table, y openxlsx. En la primera parte, se carga una base de datos satelital en formato RDS, se renombra una columna para alinear los nombres y se eliminan columnas no necesarias. Luego, se obtienen los códigos municipales únicos de esta base de datos. A continuación, se carga una base de datos adicional en formato Stata (.dta), se convierten algunas variables a factores y se escalan las variables numéricas. Se realiza un análisis para identificar códigos municipales presentes en la base de datos satelital pero no en la de contexto, y se comprueba si hay columnas sin valores faltantes en la base de datos de contexto. Finalmente, se realiza un inner_join para combinar ambas bases de datos utilizando solo las columnas sin valores faltantes y se guarda el resultado en un archivo RDS para futuros análisis. ### Cleaning R environment ### rm(list = ls()) ################# ### Libraries ### ################# library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(DataExplorer) library(haven) library(purrr) library(labelled) cat(&quot;\\f&quot;) ################################################################################ # Lectura de bases de contexto 2020 ################################################################################ ## Covariables # Contexto_munXX : la base de datos contexto_20XX.dta contiene información de # variables a nivel municipal. # Cargar la base de datos satelital y renombrar la columna &quot;CVEGEO&quot; a &quot;cve_mun&quot; satelital &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_satelite.rds&quot;) %&gt;% rename(cve_mun = CVEGEO) %&gt;% mutate(ent = NULL) # Obtener los códigos municipales únicos de la base de datos satelital cod_mun &lt;- satelital %&gt;% distinct(cve_mun) # Cargar la base de datos &quot;Contexto_mun20&quot; en formato Stata y renombrar la columna &quot;CVEGEO&quot; a &quot;cve_mun&quot; Contexto_mun &lt;- read_dta(&quot;../input/2020/predictores/contexto_2020.dta&quot;) Contexto_mun %&gt;% as_factor() %&gt;% apply(., 2, n_distinct) %&gt;% sort() Contexto_mun$elec_mun19 Contexto_mun$elec_mun20 Contexto_mun$smg1 Contexto_mun$ql_porc_cpa_urb Contexto_mun$ql_porc_cpa_rur Contexto_mun$dec_geologicas Contexto_mun$dec_hidrometeo Contexto_mun %&lt;&gt;% as_factor() %&gt;% mutate_at( .vars = c( &quot;elec_mun19&quot;, &quot;elec_mun20&quot;, &quot;smg1&quot;, &quot;ql_porc_cpa_urb&quot;, &quot;ql_porc_cpa_rur&quot;, &quot;dec_geologicas&quot;, &quot;dec_hidrometeo&quot; ), as.factor ) %&gt;% mutate_if(is.numeric, function(x) as.numeric(scale(x))) # Realizar una anti_join para identificar códigos municipales presentes en &quot;cod_mun&quot; pero no en &quot;Contexto_mun&quot; anti_join(cod_mun, Contexto_mun) # Realizar un análisis para determinar si alguna columna de &quot;Contexto_mun&quot; no tiene valores faltantes paso &lt;- apply(Contexto_mun, 2, function(x) !any(is.na(x))) # Mostrar la frecuencia de columnas sin valores faltantes table(paso) # Realizar una inner_join entre las bases de datos &quot;satelital&quot; y &quot;Contexto_mun&quot; utilizando solo las columnas sin valores faltantes statelevel_predictors &lt;- inner_join(satelital, Contexto_mun[, paso]) # Obtener las dimensiones de la base de datos resultante dim(statelevel_predictors) ## Guardar saveRDS(statelevel_predictors,file = &quot;../input/2020/predictores/statelevel_predictors_df.rds&quot;) "],["estandarizar_muestra_cuestionario_ampliado.html", "03_Estandarizar_muestra_cuestionario_ampliado.R", " 03_Estandarizar_muestra_cuestionario_ampliado.R Para la ejecución del presente archivo, debe abrir el archivo 02_Union_predictores.R disponible en la ruta Rcodes/2020/02_Union_predictores.R. Este script está diseñado para llevar a cabo un análisis detallado de datos mediante un proceso estructurado en varias fases. En la etapa inicial, el código limpia el entorno de R eliminando todos los objetos existentes para comenzar con un espacio de trabajo limpio. A continuación, carga las bibliotecas necesarias, como tidyverse y data.table, que son fundamentales para la manipulación y análisis de datos, así como para la importación y exportación de archivos. Luego, el código lee las bases de datos correspondientes al censo de personas 2020 y a la Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). Estas bases se utilizan para identificar indicadores clave relacionados con carencia en salud, calidad de vivienda, servicios básicos y rezago educativo. Seguidamente, el código valida y armoniza las variables entre ambas bases de datos para asegurar la consistencia en códigos y etiquetas. Esto incluye la comparación de variables como códigos de entidad y municipio, áreas urbanas y rurales, sexo, edad, nivel educativo, discapacidad y lengua indígena. Posteriormente, estandariza las variables del censo para alinearlas con las de la encuesta y filtra los datos para eliminar cualquier inconsistencia. Finalmente, resume el conjunto de datos estandarizado por grupo y guarda los datos procesados en dos versiones: una con los datos estandarizados y otra con los datos resumidos. ### Cleaning R environment ### rm(list = ls()) ################# ### Libraries ### ################# library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(DataExplorer) library(haven) library(purrr) library(labelled) library(sampling) cat(&quot;\\f&quot;) ################################################################################ # Lectura de bases censo persona 2020 ################################################################################ muestra_cuestionario_ampliado &lt;- readRDS( &quot;../output/2020/muestra_cuestionario_ampliado.rds&quot; ) enigh &lt;- readRDS(&quot;../output/2020/enigh.rds&quot;) muestra_cuestionario_ampliado %&gt;% dim() ################################################################################ # Identificando indicadores ################################################################################ # Indicador de carencia por acceso a los servicios de salud (CENSO) head(muestra_cuestionario_ampliado$ic_asalud) # Carencia por la calidad y espacios de la vivienda. (CENSO) attributes(muestra_cuestionario_ampliado$ic_cv)$label # Carencia por servicios básicos en la vivienda. (CENSO) attributes(muestra_cuestionario_ampliado$ic_sbv)$label # Carencia por rezago educativo. attributes( muestra_cuestionario_ampliado$ic_rezedu)$label ################################################################################ ## Validaciones para la armonización de variables ################################################################################ ## codigos de ent y municipio ## Encuesta n_distinct(enigh$ent) # cod_dam n_distinct(enigh$cve_mun) # cod_mun ## censo n_distinct(muestra_cuestionario_ampliado$ent) n_distinct(muestra_cuestionario_ampliado$cve_mun) ################################################################################ ## area ## Encuesta attributes(enigh$rururb) ## censo table(muestra_cuestionario_ampliado$rururb, useNA = &quot;a&quot;) # 0 Urbano # 1 Rural ################################################################################ ## Sexo ## Encuesta attributes(enigh$sexo) # 2 Mujer # 1 Hombre ## censo table(muestra_cuestionario_ampliado$sexo, useNA = &quot;a&quot;) # 0 Mujer # 1 Hombre ################################################################################ ## edad ## Encuesta attributes(enigh$edad) ## censo head(muestra_cuestionario_ampliado$edad_cat) ################################################################################ ## nivel_edu # ## Encuesta attributes(enigh$niv_ed) # 0 [Con primaria incompleta o menos] # 1 [Primaria completa o secundaria incompleta] # 2 [Secundaria completa o media superior incompleta] # 3 [Media superior completa o mayor nivel educativo] # NA Niños de 0 a 3 años ## censo attributes(muestra_cuestionario_ampliado$niv_ed) # 0 [Con primaria incompleta o menos] # 1 [Primaria completa o secundaria incompleta] # 2 [Secundaria completa o media superior incompleta] # 3 [Media superior completa o mayor nivel educativo] # NA Niños de 0 a 3 años ################################################################################ ## Discapacidad # ## se puede eliminar o solicitar que sea enviada en el mismo formato que la encuesta ## Encuesta attributes(enigh$discap) ## censo attributes(muestra_cuestionario_ampliado$discap) ################################################################################ ## Habla dialecto o lengua indígena ## Encuesta attributes(enigh$hli) ## censo attributes(muestra_cuestionario_ampliado$hli) Estandarizando el censo censo_sta &lt;- muestra_cuestionario_ampliado %&gt;% transmute( ent, cve_mun, upm, estrato, area = as.character(rururb), sexo = if_else(sexo == 1, &quot;1&quot;, &quot;2&quot;), edad = as.character(edad_cat), nivel_edu = haven::as_factor(niv_ed, levels = &quot;values&quot;), discapacidad = as.character(discap), discapacidad = if_else(is.na(discapacidad), &quot;0&quot;, discapacidad), hlengua = as.character(hli), hlengua = if_else(is.na(hlengua), &quot;0&quot;, hlengua), ic_asalud= haven::as_factor(ic_asalud, levels = &quot;values&quot;), ic_cv = haven::as_factor(ic_cv, levels = &quot;values&quot;), ic_sbv = haven::as_factor(ic_sbv, levels = &quot;values&quot;), ic_rezedu = haven::as_factor(ic_rezedu, levels = &quot;values&quot;), factor ) %&gt;% filter( !cve_mun %in% c(&quot;04012&quot;, &quot;07125&quot;, &quot;29048&quot;)) censo_sta2 &lt;- censo_sta %&gt;% filter( !is.na(nivel_edu), !is.na(edad), !is.na(ic_sbv), !is.na(ic_cv), !is.na(ic_rezedu), !is.na(ic_asalud), ) sum(censo_sta2$factor) / sum(censo_sta$factor) #Se guarda el conjunto de datos estandarizado: saveRDS(censo_sta2,&quot;../output/2020/encuesta_ampliada.rds&quot;) #Se resumen los datos por grupo: censo_sta2 %&lt;&gt;% group_by( ent, cve_mun, area, hlengua, sexo, edad, nivel_edu, discapacidad, ic_asalud, ic_cv, ic_sbv, ic_rezedu ) %&gt;% summarise(n = sum(factor), .groups = &quot;drop&quot;) #Se guarda el conjunto de datos resumido: saveRDS(censo_sta2, &quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) "],["armonizar_encuesta.html", "04_Armonizar_encuesta.R", " 04_Armonizar_encuesta.R Para la ejecución del presente archivo, debe abrir el archivo 04_Armonizar_encuesta.R disponible en la ruta Rcodes/2020/04_Armonizar_encuesta.R. Este script está diseñado para llevar a cabo un análisis exhaustivo de datos a través de un proceso metódico en varias fases. En la primera fase, el código elimina todos los objetos del entorno de R para asegurar que el análisis se inicie con un entorno limpio y sin datos residuales. A continuación, se cargan las bibliotecas necesarias para el análisis, que proporcionan herramientas esenciales para la manipulación de datos y la importación y exportación de archivos. Después, se leen las bases de datos del censo de personas 2020 y de la Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). Estas bases se utilizan para identificar y verificar indicadores relacionados con carencias sociales. Posteriormente, se definen y validan las variables clave de la encuesta, como códigos de entidad, municipio, área, sexo, edad, nivel educativo, discapacidad y lengua indígena. Luego, se estandarizan las variables de la encuesta para alinear los datos con los del censo, se filtran los datos para eliminar inconsistencias y se guarda el conjunto de datos estandarizado. Además, se actualiza la tabla censal utilizando la técnica de calibración IPFP (Iterative Proportional Fitting Procedure) para asegurar que las distribuciones marginales de las variables estandarizadas coincidan con las del censo. Finalmente, se generan histogramas y gráficos de dispersión para visualizar los resultados de la calibración y se guarda el conjunto de datos ampliado. Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de R eliminando todos los objetos y se ejecuta el recolector de basura para liberar memoria. ### Cleaning R environment ### rm(list = ls()) gc() Se cargan varias bibliotecas esenciales para la manipulación y análisis de datos, incluyendo tidyverse, data.table, openxlsx, magrittr, DataExplorer, haven, purrr, labelled y sampling. library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(DataExplorer) library(haven) library(purrr) library(labelled) library(sampling) cat(&quot;\\f&quot;) Lectura de Datos Se leen las bases de datos muestra_cuestionario_ampliado.rds y enigh.rds, y se verifica la unicidad de los identificadores de entidad y municipio. encuesta_ampliada &lt;- readRDS(&quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) # Indicadores_MECXX: La base de datos Pobreza_15.dta contiene la información de # variables de carencias sociales a nivel individual (Utiliza como insumo la # información del MEC) enigh &lt;- readRDS(&quot;output//2020/enigh.rds&quot;) n_distinct(enigh$ent) # cod_dam n_distinct(enigh$cve_mun) # cod_mun Definición de Variables para la Encuesta Se definen y transforman las variables de área urbana o rural. # codigo municipio # area urabo o rural enigh %&gt;% group_by(rururb) %&gt;% summarise(n = sum(factor)) enigh %&lt;&gt;% mutate( area = haven::as_factor(rururb, levels = &quot;values&quot;), area = as.character(area) ) enigh %&gt;% distinct(rururb,area) Transformación de Variables Demográficas y Socioeconómicas Se transforman las variables de sexo, edad, nivel educativo, discapacidad y lengua indígena para su uso en el análisis. # Sexo enigh %&gt;% group_by(sexo) %&gt;% summarise(n = sum(factor)) # 2 Mujer # 1 Hombre # Edad enigh %&gt;% group_by(edad) %&gt;% summarise(n = sum(factor)) encuesta_ampliada %&gt;% distinct(edad) # 1 Menor de 14 años # 2 15 a 29 años # 3 30 a 44 años # 4 45 a 64 años # 5 65 años o más enigh %&lt;&gt;% mutate(g_edad = case_when(edad &lt;= 14 ~ &quot;1&quot;, # 0 a 14 edad &lt;= 29 ~ &quot;2&quot;, # 15 a 29 edad &lt;= 44 ~ &quot;3&quot;, # 30 a 44 edad &lt;= 64 ~ &quot;4&quot;, # 45 a 64 edad &gt; 64 ~ &quot;5&quot;, # 65 o mas TRUE ~ NA_character_)) enigh %&gt;% group_by(g_edad) %&gt;% summarise(n = sum(factor)) # nivel educativo enigh %&gt;% group_by(niv_ed) %&gt;% summarise(n = sum(factor), .groups = &quot;drop&quot;) %&gt;% mutate(prop = n / sum(n)) enigh %&gt;% mutate(nivel_edu = haven::as_factor(niv_ed, levels = &quot;values&quot;)) %&gt;% group_by(nivel_edu, g_edad) %&gt;% summarise(n = sum(factor)) %&gt;% data.frame() enigh %&lt;&gt;% mutate( nivel_edu = haven::as_factor(niv_ed, levels = &quot;values&quot;), nivel_edu = as.character(nivel_edu) ) enigh %&gt;% distinct(niv_ed,nivel_edu) # Discapacidad enigh %&gt;% group_by(discap) %&gt;% summarise(n = sum(factor)) enigh %&lt;&gt;% mutate( discapacidad = haven::as_factor(discap, levels = &quot;values&quot;), discapacidad = as.character(discapacidad) ) enigh %&gt;% distinct(discap,discapacidad) # Habla dialecto o lengua indígena attributes(enigh$hli) enigh %&gt;% group_by(hli) %&gt;% summarise(n = sum(factor)) enigh %&lt;&gt;% mutate( hlengua = haven::as_factor(hli, levels = &quot;values&quot;), hlengua = as.character(hlengua) ) enigh %&gt;% distinct(hlengua,hli) Análisis de Variables de Carencia Se analizan los indicadores de carencia y se visualiza la distribución logarítmica del ingreso per cápita. hist(log(enigh$ictpc)) # Indicador de carencia por acceso a servicios de salud attributes(enigh$ic_segsoc) # Indicador de carencia por acceso a la alimentación nutritiva y de calidad attributes(enigh$ic_ali_nc) enigh %&gt;% group_by(ic_segsoc) %&gt;% summarise(n = sum(factor)) Preparación y Guardado del Conjunto de Datos Se prepara el conjunto de datos final encuesta_sta con las variables de estudio y de diseño, y se guarda en un archivo .rds. ################################################################################ encuesta_sta &lt;- enigh %&gt;% transmute( ent = str_pad( string = ent, width = 2, pad = &quot;0&quot; ), cve_mun, area, sexo, edad = ifelse(is.na(g_edad), &quot;99&quot;, g_edad), nivel_edu = ifelse(is.na(nivel_edu), &quot;99&quot;, nivel_edu), discapacidad = ifelse(is.na(discapacidad), &quot;0&quot;, discapacidad), hlengua = ifelse(is.na(hlengua), &quot;0&quot;, hlengua), ## Variable de estudio ictpc = as.numeric(ictpc), ic_segsoc = as.numeric(ic_segsoc), ic_ali_nc = as.numeric(ic_ali_nc), ic_rezedu = as.numeric(ic_segsoc), ic_asalud = as.numeric(ic_segsoc), ic_sbv = as.numeric(ic_sbv_hog), ic_cv = as.numeric(ic_cv_hog), ## Variables diseño estrato = est_dis, upm = upm, fep = factor ) map(c( &quot;ent&quot;, &quot;cve_mun&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;nivel_edu&quot;, &quot;hlengua&quot;, &quot;discapacidad&quot;, &quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot; ), function(x) { encuesta_sta %&gt;% group_by_at(x ) %&gt;% summarise(Nd = sum(fep)) %&gt;% mutate(N = sum(Nd), prop = Nd / N) }) # Se guarda el conjunto de datos saveRDS(encuesta_sta, file = &quot;../input/2020/enigh/encuesta_sta.rds&quot;) Actualización de la Tabla Censal Se actualiza la tabla censal mediante calibración de pesos, y se guarda el conjunto de datos calibrado. # Actualización de tabla censal- IPFP ------------------------------------- names_cov &lt;- c(&quot;ent&quot;, &quot;cve_mun&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;) names_cov &lt;- names_cov[names_cov %in% names(encuesta_sta)] num_cat_censo &lt;- apply(encuesta_ampliada[names_cov], MARGIN = 2, function(x) length(unique(x))) num_cat_sample &lt;- apply(encuesta_sta[names_cov], MARGIN = 2, function(x) length(unique(x))) names_cov &lt;- names_cov[num_cat_censo == num_cat_sample] # MatrizCalibrada creada únicamente para los niveles completos # IMPORTANTE: Excluir las covariables que tengan niveles incompletos auxSuma &lt;- function(dat, col, ni) { dat %&gt;% ungroup() %&gt;% select(all_of(col)) %&gt;% fastDummies::dummy_cols(remove_selected_columns = TRUE) %&gt;% mutate_all(~.*ni) %&gt;% colSums() } N.g &lt;- map(names_cov, ~ auxSuma(encuesta_sta, col = .x, ni = encuesta_sta$fep)) %&gt;% unlist() N_censo.g &lt;- map(names_cov, ~ auxSuma(encuesta_ampliada, col = .x, ni = encuesta_ampliada$n)) %&gt;% unlist() names_xk &lt;- intersect(names(N.g), names(N_censo.g)) N.g &lt;- N.g[names_xk] N_censo.g &lt;- N_censo.g[names_xk] Xk &lt;- encuesta_ampliada %&gt;% ungroup() %&gt;% select(all_of(names_cov)) %&gt;% fastDummies::dummy_cols(remove_selected_columns = TRUE) % &gt;% select(all_of(names_xk)) gk &lt;- calib(Xs = Xk, d = encuesta_ampliada$n, total = N.g, method = &quot;linear&quot;) # linear primera opcion checkcalibration(Xs = Xk, d = encuesta_ampliada$n, total = N.g, g = gk) hist(gk) summary(gk) ggplot(data.frame(x = gk), aes(x = x)) + geom_histogram(binwidth = diff(range(gk)) / 20, color = &quot;black&quot;, alpha = 0.7) + labs(title = &quot;&quot;, x = &quot;&quot;, y = &quot;&quot;) + theme_minimal() + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) n1 &lt;- encuesta_ampliada$n * gk ggplot(data = data.frame(x = encuesta_ampliada$n, y = n1), aes(x = x, y = y)) + geom_point() + # Agregar puntos geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + labs(title = &quot;&quot;, x = &quot;Conteo censales antes&quot;, y = &quot;Conteos censales después&quot;) + theme_minimal(20) encuesta_ampliada$n &lt;- encuesta_ampliada$n * gk # Se guarda el conjunto de datos ampliado: saveRDS(encuesta_ampliada, &quot;../output/2020/encuesta_ampliada.rds&quot;) "],["validacion_encuestas.html", "05_Validacion_encuestas.R", " 05_Validacion_encuestas.R Para ejecutar este archivo, es necesario abrir el archivo 05_Validacion_encuestas.R ubicado en la ruta Rcodes/2020/05_Validacion_encuestas.R. Este script está diseñado para llevar a cabo un análisis integral de datos con varias fases que aseguran una evaluación rigurosa de la información disponible. En la primera etapa del código, se eliminan todos los objetos del entorno de R para garantizar que el análisis se realice en un entorno limpio y sin interferencias. Luego, se cargan las librerías necesarias para llevar a cabo el análisis y se establece el tema de visualización predeterminado usando bayesplot::theme_default(). También se carga un archivo de script adicional que incluye funciones específicas para la creación de gráficos. Posteriormente, se leen las bases de datos encuesta_sta y muestra_ampliada, que contienen los datos de la encuesta y del censo, respectivamente. El análisis procede realizando comparaciones entre los datos censales y los datos de la encuesta en diferentes variables categóricas, tales como edad, sexo, nivel educativo y estado. Para cada una de estas variables, se generan gráficos que permiten visualizar las diferencias. Utilizando la librería patchwork, se combinan estos gráficos en un solo diseño para facilitar la comparación. Además, se llevan a cabo análisis adicionales en variables como lengua indígena y discapacidad, y se exploran las interacciones entre variables como sexo y edad, nivel educativo y sexo, y estado y sexo. Finalmente, los gráficos que ilustran estas interacciones se integran en un diseño consolidado para una visualización comprensiva de los efectos. Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de R y se cargan las bibliotecas necesarias para la manipulación de datos y la creación de gráficos. #### Cleaning R environment ### rm(list = ls()) ################# #### Libraries ### ################# library(tidyverse) library(reshape2) library(stringr) library(ggalt) library(gridExtra) library(scales) library(formatR) library(patchwork) theme_set(bayesplot::theme_default()) source(file = &quot;../source/Plot_validacion.R&quot;, encoding = &quot;UTF-8&quot;) Lectura de Datos Se leen las bases de datos necesarias para el análisis. encuesta_sta &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) muestra_ampliada &lt;- readRDS(&quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) Comparación de Variables Se comparan las distribuciones de diferentes variables entre los datos censales y los de la encuesta utilizando la función Plot_Compare. Comparación de Edad, Sexo y Nivel Educativo #### AGE ### age_plot &lt;- Plot_Compare(dat_censo = muestra_ampliada, dat_encuesta = encuesta_sta, by = &quot;edad&quot;) #### Sex ### sex_plot &lt;- Plot_Compare(dat_censo = muestra_ampliada, dat_encuesta = encuesta_sta, by = &quot;sexo&quot;) #### Level of schooling (LoS) ### escolar_plot &lt;- Plot_Compare(dat_censo = muestra_ampliada, dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;) #### States ### depto_plot &lt;- Plot_Compare(dat_censo = muestra_ampliada, dat_encuesta = encuesta_sta, by = &quot;ent&quot;) Se crean gráficos comparativos para edad, sexo, nivel educativo y estados. Estos gráficos se combinan utilizando patchwork. #--- Patchwork en acción ---# (age_plot | sex_plot | escolar_plot) / (depto_plot) Comparación de Lengua Indígena y Discapacidad hlengua &lt;- Plot_Compare(dat_censo = muestra_ampliada, dat_encuesta = encuesta_sta, by = &quot;hlengua&quot;) plot_discapacidad &lt;- Plot_Compare(dat_censo = muestra_ampliada, dat_encuesta = encuesta_sta, by = &quot;discapacidad&quot;) (plot_discapacidad | hlengua ) Efectos de Interacción Se analizan los efectos de interacción entre diferentes variables utilizando la función plot_interaction. Interacción Edad x Sexo #### AGE x SEX ### encuesta_sta$pobreza &lt;- encuesta_sta$ictpc #--- Percentage of people in poverty by AGE x SEX ---# p_sex_age &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;edad&quot;) Interacción Nivel Educativo x Sexo #### Level of schooling (LoS) x SEX ### p_sex_escolar &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;nivel_edu&quot;) Interacción Estado x Sexo #### State x SEX ### p_sex_depto &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;ent&quot;) Se combinan los gráficos de interacción utilizando patchwork. #--- Patchwork in action ---# (p_sex_age + p_sex_escolar) / p_sex_depto Interacción Nivel Educativo x Edad #### Level of schooling (LoS) x AGE ### p_escolar_edad &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;, by2 = &quot;edad&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;nivel_edu&quot;) Interacción Estado x Edad #### State x AGE ### p_depto_edad &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;edad&quot;, by2 = &quot;ent&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;Edad&quot;) p_escolar_edad / p_depto_edad Interacción Nivel Educativo x Estado #### Level of schooling (LoS) x State ### p_depto_escolar &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;, by2 = &quot;ent&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;nivel_edu&quot;) p_depto_escolar Repetición del Análisis para Diferentes Indicadores de Pobreza El análisis de interacción se repite para diferentes indicadores de pobreza (ic_segsoc e ic_ali_nc), utilizando el mismo procedimiento descrito anteriormente. #### AGE x SEX ### encuesta_sta$pobreza &lt;- encuesta_sta$ic_segsoc #--- Percentage of people in poverty by AGE x SEX ---# p_sex_age &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;edad&quot;) #### Level of schooling (LoS) x SEX ### p_sex_escolar &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;nivel_edu&quot;) #### State x SEX ### p_sex_depto &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;ent&quot;) #--- Patchwork in action ---# (p_sex_age + p_sex_escolar) / p_sex_depto #### Level of schooling (LoS) x AGE ### p_escolar_edad &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;, by2 = &quot;edad&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;nivel_edu&quot;) #### State x AGE ### p_depto_edad &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;edad&quot;, by2 = &quot;ent&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;Edad&quot;) p_escolar_edad / p_depto_edad #### Level of schooling (LoS) x State ### p_depto_escolar &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;, by2 = &quot;ent&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;nivel_edu&quot;) p_depto_escolar #### AGE x SEX ### encuesta_sta$pobreza &lt;- encuesta_sta$ic_ali_nc #--- Percentage of people in poverty by AGE x SEX ---# p_sex_age &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;edad&quot;) #### Level of schooling (LoS) x SEX ### p_sex_escolar &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;nivel_edu&quot;) #### State x SEX ### p_sex_depto &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;sexo&quot;, by2 = &quot;ent&quot;) #--- Patchwork in action ---# (p_sex_age + p_sex_escolar) / p_sex_depto #### Level of schooling (LoS) x AGE ### p_escolar_edad &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;, by2 = &quot;edad&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;nivel_edu&quot;) #### State x AGE ### p_depto_edad &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;edad&quot;, by2 = &quot;ent&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;Edad&quot;) p_escolar_edad / p_depto_edad #### Level of schooling (LoS) x State ### p_depto_escolar &lt;- plot_interaction(dat_encuesta = encuesta_sta, by = &quot;nivel_edu&quot;, by2 = &quot;ent&quot;) + theme(legend.position = &quot;bottom&quot;) + labs(colour = &quot;nivel_edu&quot;) p_depto_escolar Este análisis detallado permite verificar la consistencia de los datos de la encuesta en comparación con los datos censales y entender mejor las interacciones entre diferentes variables demográficas y socioeconómicas. "],["modelo_unidad_ictpc.html", "06_Modelo_unidad_ictpc.R", " 06_Modelo_unidad_ictpc.R Para la ejecución del presente archivo, debe abrir el archivo 06_Modelo_unidad_ictpc.R disponible en la ruta Rcodes/2020/06_Modelo_unidad_ictpc.R. Este script en R se enfoca en la creación de un modelo multinivel utilizando datos de encuestas y censos para predecir ingresos. El proceso comienza con la limpieza del entorno de trabajo mediante rm(list = ls()) y la carga de librerías esenciales para el análisis, tales como patchwork, nortest, lme4, tidyverse, magrittr, caret, car, y una fuente externa de modelos (source(\"source/modelos_freq.R\")). Posteriormente, se definen las variables de agregación (byAgrega), se incrementa el límite de memoria disponible, y se cargan las bases de datos necesarias (encuesta_sta, censo_sta, statelevel_predictors_df). Se seleccionan las variables covariantes excluyendo aquellas que comienzan con hog_ o cve_mun. En la selección de variables relevantes, se mencionan algunos pasos comentados que realizan la selección de características utilizando el método rfe de caret. Las variables seleccionadas se listan explícitamente en variables_seleccionadas y se combinan con otras covariantes para formar cov_names. Luego, se construye la fórmula del modelo (formula_model), que incluye efectos aleatorios para cve_mun, hlengua y discapacidad, así como efectos fijos para las demás variables. Esto asegura que el modelo considere tanto la variabilidad dentro de cada grupo como las características específicas de cada variable. Finalmente, se realiza una transformación la variable de ingreso (ingreso) a formato numérico y se ajusta el modelo utilizando la función modelo_ingreso, que está definida en el archivo fuente cargado previamente. Los datos de entrada incluyen encuesta_sta, statelevel_predictors_df, censo_sta y la fórmula del modelo. Los resultados del modelo se guardan en un archivo RDS (fit_mrp_ictpc.rds). Se generan y guardan gráficos de densidad y histogramas de las distribuciones posteriores utilizando ggsave, proporcionando una visualización clara de los resultados obtenidos. Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de R y se cargan las bibliotecas necesarias para el análisis. rm(list =ls()) # Loading required libraries ---------------------------------------------- library(patchwork) library(nortest) library(lme4) library(tidyverse) library(magrittr) library(caret) library(car) source(&quot;../source/modelos_freq.R&quot;) Variables de Agregación Se define un vector de variables que se utilizarán para la agregación de datos. byAgrega &lt;- c(&quot;ent&quot;, &quot;cve_mun&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;, &quot;nivel_edu&quot; ) Carga de Datos Se cargan los datos de la encuesta, el censo y los predictores a nivel estatal. # Loading data ------------------------------------------------------------ memory.limit(10000000) encuesta_sta &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) censo_sta &lt;- readRDS(&quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) statelevel_predictors_df &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_df.rds&quot;) cov_names &lt;- names(statelevel_predictors_df) cov_names &lt;- cov_names[!grepl(x = cov_names,pattern = &quot;^hog_|cve_mun&quot;)] Selección de Variables Se seleccionan las variables predictoras que se utilizarán en el modelo. # Selección de variables # encuesta_sta2 &lt;- encuesta_sta %&gt;% group_by_at((byAgrega)) %&gt;% # summarise(ingreso = mean(ictpc), .groups = &quot;drop&quot;, # n = n()) %&gt;% # inner_join(statelevel_predictors_df) # # # resultado_rfe &lt;- # rfe( # encuesta_sta2[, cov_names], # encuesta_sta2$ingreso, # sizes = c(1:10), # rfeControl = rfeControl(functions = lmFuncs, method = &quot;LOOCV&quot;) # ) variables_seleccionadas &lt;- c( &quot;prom_esc_rel_urb&quot;, &quot;smg1&quot;, &quot;gini15m&quot; , &quot;ictpc15&quot; , &quot;altitud1000&quot;, &quot;prom_esc_rel_rur&quot; , &quot;porc_patnoagrnocal_urb&quot;, &quot;acc_medio&quot; , &quot;derhab_pea_15_20&quot; , &quot;porc_norep_ing_urb&quot; ) cov_names &lt;- c( &quot;modifica_humana&quot;, &quot;acceso_hosp&quot;, &quot;acceso_hosp_caminando&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;luces_nocturnas&quot; , variables_seleccionadas ) cov_registros &lt;- setdiff( cov_names, c( &quot;elec_mun20&quot;, &quot;elec_mun19&quot;, &quot;transf_gobpc_15_20&quot;, &quot;derhab_pea_15_20&quot;, &quot;vabpc_15_19&quot; , &quot;itlpis_15_20&quot; , &quot;remespc_15_20&quot;, &quot;desem_15_20&quot;, &quot;smg1&quot;, &quot;ql_porc_cpa_urb&quot;, &quot;ql_porc_cpa_rur&quot; ) ) Fórmula del Modelo Se define la fórmula del modelo incluyendo los efectos aleatorios y las variables predictoras seleccionadas. cov_registros &lt;- paste0(cov_registros, collapse = &quot; + &quot;) formula_model &lt;- paste0(&quot;ingreso ~ (1 | cve_mun) + (1 | hlengua) + (1 | discapacidad) + ent + nivel_edu + edad + area + sexo +&quot;, &quot; + &quot;, cov_registros) Ajuste del Modelo Se ajusta el modelo de ingreso utilizando la función modelo_ingreso y se guarda el resultado. encuesta_sta$ingreso &lt;- as.numeric(encuesta_sta$ictpc) fit &lt;- modelo_ingreso( encuesta_sta = encuesta_sta , predictors = statelevel_predictors_df, censo_sta = censo_sta, formula_mod = formula_model, byAgrega = byAgrega ) #--- Exporting Bayesian Multilevel Model Results ---# saveRDS(fit, file = &quot;../output/2020/modelos/fit_mrp_ictpc.rds&quot;) ggsave(plot = fit$plot_densy, &quot;../output/2020/plots/01_densidad_ictpc.png&quot;,scale = 3) fit$plot_hist_post "],["modelo_unidad_py_ic_ali_nc.html", "07_Modelo_unidad_py_ic_ali_nc.R", " 07_Modelo_unidad_py_ic_ali_nc.R Para la ejecución del presente archivo, debe abrir el archivo 07_Modelo_unidad_py_ic_ali_nc.R disponible en la ruta Rcodes/2020/07_Modelo_unidad_py_ic_ali_nc.R. Este script en R se centra en la creación de un modelo multinivel para predecir la carencia en alimentos nutritivos y de calidad , utilizando datos de encuestas y censos. Comienza limpiando el entorno de trabajo (rm(list = ls())) y cargando las librerías necesarias para la integración con Python. Además, se importan los módulos pandas y las bibliotecas sklearn.feature_selection y sklearn.ensemble de Python. También se carga una fuente externa de modelos (source(\"source/modelos_freq.R\")). Luego, se definen las variables para la agregación (byAgrega) y se aumenta el límite de memoria disponible. Las bases de datos necesarias (encuesta_sta, censo_sta y statelevel_predictors_df) se cargan, y se seleccionan las variables covariantes relevantes excluyendo aquellas que comienzan con hog_ o cve_mun. El script incluye un paso comentado para la selección de características utilizando el método RFE de caret y Python, pero en esta versión, se listan explícitamente las variables seleccionadas (variables_seleccionadas) y se combinan con otras covariantes para formar cov_names. Luego, se construye la fórmula del modelo (formula_model), que incluye efectos aleatorios para cve_mun, hlengua y discapacidad, así como efectos fijos para las demás variables. Se ajusta el modelo utilizando la función modelo_dummy, que está definida en el archivo fuente cargado previamente. Los datos de entrada incluyen encuesta_sta (con una nueva variable yk), statelevel_predictors_df, censo_sta y la fórmula del modelo. Este enfoque permite una modelización robusta y ajustada a las especificaciones de los datos disponibles. Finalmente, los resultados del modelo se guardan en un archivo RDS (fit_mrp_ic_ali_nc.rds). Este paso asegura que los resultados del análisis estén disponibles para futuras referencias y análisis adicionales. Guardar los resultados en un archivo RDS facilita su recuperación y análisis posterior, permitiendo a los investigadores y analistas continuar trabajando con los resultados sin necesidad de recalcular el modelo cada vez. Además, se generan gráficos de densidad y histogramas de las distribuciones posteriores utilizando ggsave, lo que proporciona una visualización clara y comprensible de los resultados del modelo. Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de R y se cargan las bibliotecas necesarias para el análisis, incluyendo la biblioteca reticulate para interactuar con Python. rm(list =ls()) # Loading required libraries ---------------------------------------------- library(patchwork) library(nortest) library(lme4) library(tidyverse) library(magrittr) library(caret) library(car) library(randomForest) library(reticulate) pd &lt;- import(&quot;pandas&quot;) sklearn_fs &lt;- import(&quot;sklearn.feature_selection&quot;) sklearn_ensemble &lt;- import(&quot;sklearn.ensemble&quot;) source(&quot;../source/modelos_freq.R&quot;) Carga de Datos Se cargan los datos de la encuesta, el censo y los predictores a nivel estatal. # Loading data ------------------------------------------------------------ memory.limit(10000000) encuesta_sta &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) censo_sta &lt;- readRDS(&quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) statelevel_predictors_df &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_df.rds&quot;) cov_names &lt;- names(statelevel_predictors_df) cov_names &lt;- cov_names[!grepl(x = cov_names,pattern = &quot;^hog_|cve_mun&quot;)] Selección de Variables Se definen las variables de agregación y se seleccionan las variables predictoras más relevantes. ## Selección de variables byAgrega &lt;- c(&quot;ent&quot;, &quot;cve_mun&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;, &quot;nivel_edu&quot; ) # encuesta_sta2 &lt;- encuesta_sta %&gt;% mutate( # yk = as.factor(ifelse(ic_ali_nc == 1 ,1,0))) %&gt;% # inner_join(statelevel_predictors_df[, c(&quot;cve_mun&quot;,cov_names)]) # # # Convertir &#39;encuesta_sta2&#39; a un dataframe de Python # encuesta_sta2_py &lt;- pd$DataFrame(encuesta_sta2) # # # Obtener &#39;X&#39; y &#39;y&#39; del dataframe de Python # X &lt;- encuesta_sta2_py[cov_names] # y &lt;- encuesta_sta2_py[[&#39;yk&#39;]] # # # Crear el modelo de clasificación, por ejemplo, un Random Forest # modelo &lt;- sklearn_ensemble$RandomForestClassifier() # # # Crear el selector RFE con el modelo y el número de características a seleccionar # selector &lt;- sklearn_fs$RFE(modelo, n_features_to_select = as.integer(10)) # # # Ajustar los datos # selector$fit(X, y) # # Obtener las variables seleccionadas # variables_seleccionadas &lt;- X[selector$support_] %&gt;% names() variables_seleccionadas &lt;- c( &quot;porc_ing_ilpi_urb&quot;, &quot;pob_ind_rur&quot;, &quot;pob_ind_urb&quot;, &quot;porc_hogremesas_rur&quot;, &quot;porc_segsoc15&quot;, &quot;porc_ali15&quot;, &quot;plp15&quot;, &quot;pob_rur&quot;, &quot;altitud1000&quot; ) cov_names &lt;- c( &quot;modifica_humana&quot;, &quot;acceso_hosp&quot;, &quot;acceso_hosp_caminando&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;luces_nocturnas&quot; , variables_seleccionadas ) Definición de la Fórmula del Modelo Se define la fórmula del modelo incluyendo los efectos aleatorios y las variables predictoras seleccionadas. cov_registros &lt;- setdiff( cov_names, c( &quot;elec_mun20&quot;, &quot;elec_mun19&quot;, &quot;transf_gobpc_15_20&quot;, &quot;derhab_pea_15_20&quot;, &quot;vabpc_15_19&quot; , &quot;itlpis_15_20&quot; , &quot;remespc_15_20&quot;, &quot;desem_15_20&quot;, &quot;porc_urb&quot; , &quot;edad65mas_urb&quot;, &quot;pob_tot&quot; , &quot;acc_muyalto&quot; , &quot;smg1&quot;, &quot;ql_porc_cpa_rur&quot;, &quot;ql_porc_cpa_urb&quot; ) ) cov_registros &lt;- paste0(cov_registros, collapse = &quot; + &quot;) formula_model &lt;- paste0( &quot;cbind(si, no) ~ (1 | cve_mun) + (1 | hlengua) + (1 | discapacidad) + nivel_edu + edad + ent + area + sexo &quot; , &quot; + &quot;, cov_registros ) Ajuste del Modelo Se ajusta el modelo utilizando la función modelo_dummy y se guarda el resultado. fit &lt;- modelo_dummy( encuesta_sta = encuesta_sta %&gt;% mutate(yk = ifelse(ic_ali_nc == 1 , 1, 0)) , predictors = statelevel_predictors_df, censo_sta = censo_sta, formula_mod = formula_model, byAgrega = byAgrega ) #--- Exporting Bayesian Multilevel Model Results ---# saveRDS(fit, file = &quot;../output/2020/modelos/fit_mrp_ic_ali_nc.rds&quot;) "],["modelo_unidad_py_ic_segsoc.html", "08_Modelo_unidad_py_ic_segsoc", " 08_Modelo_unidad_py_ic_segsoc Para la ejecución del presente análisis, se debe abrir el archivo 08_Modelo_unidad_py_ic_segsoc.R disponible en la ruta Rcodes/2020/08_Modelo_unidad_py_ic_segsoc.R. Este script en R se enfoca en la creación de un modelo multinivel para analizar la carencia en por cobertura de seguridad social usando datos de encuestas y censos. El proceso inicia con la limpieza del entorno de trabajo y la carga de librerías esenciales para el análisis. Se define el límite de memoria con memory.limit(10000000) para asegurar suficiente espacio durante el procesamiento de datos. Luego, se cargan los datos necesarios (encuesta_sta, censo_sta, statelevel_predictors_df) y se excluyen ciertas variables de los datos de predictores. La selección de variables relevantes para el modelo se realiza a través de un procedimiento comentado que emplea la técnica de Recursive Feature Elimination (RFE) usando un modelo Random Forest implementado en Python. Las variables seleccionadas se listan explícitamente y se combinan con otras covariantes para formar cov_names. Posteriormente, se construye una fórmula del modelo (formula_model) que incluye efectos aleatorios para cve_mun, hlengua y discapacidad, y efectos fijos para las demás variables, considerando tanto la variabilidad dentro de cada grupo como las características específicas de cada variable. El ajuste del modelo se realiza mediante la función modelo_dummy, transformando la variable de interés (ic_segsoc) en formato binario y utilizando los datos de encuesta y censo junto con la fórmula del modelo y las variables de agregación. Los resultados del modelo se guardan en un archivo RDS (fit_mrp_ic_segsoc.rds) y se documentan para futuros análisis. Esto incluye la exportación de los resultados del modelo multinivel y la creación de visualizaciones pertinentes para evaluar el desempeño del modelo y la distribución de las predicciones. Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de R y se cargan las bibliotecas necesarias para el análisis, incluyendo reticulate para interactuar con Python. rm(list =ls()) # Loading required libraries ---------------------------------------------- library(patchwork) library(nortest) library(lme4) library(tidyverse) library(magrittr) library(caret) library(car) library(randomForest) library(reticulate) pd &lt;- import(&quot;pandas&quot;) sklearn_fs &lt;- import(&quot;sklearn.feature_selection&quot;) sklearn_ensemble &lt;- import(&quot;sklearn.ensemble&quot;) Carga de Datos Se cargan los datos de la encuesta, el censo y los predictores a nivel estatal. memory.limit(10000000) source(&quot;../source/modelos_freq.R&quot;) # Loading data ------------------------------------------------------------ memory.limit(10000000) encuesta_sta &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) censo_sta &lt;- readRDS(&quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) statelevel_predictors_df &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_df.rds&quot;) cov_names &lt;- names(statelevel_predictors_df) cov_names &lt;- cov_names[!grepl(x = cov_names, pattern = &quot;^hog_|cve_mun&quot;)] Selección de Variables Se definen las variables de agregación y se seleccionan las variables predictoras más relevantes. ## Selección de variables byAgrega &lt;- c(&quot;ent&quot;, &quot;cve_mun&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;, &quot;nivel_edu&quot; ) # encuesta_sta2 &lt;- encuesta_sta %&gt;% mutate( # yk = as.factor(ifelse(ic_asegsoc == 1 ,1,0))) %&gt;% # inner_join(statelevel_predictors_df[, c(&quot;cve_mun&quot;,cov_names)]) # # table(encuesta_sta2$yk, encuesta_sta2$ic_segsoc) # # # Convertir &#39;encuesta_sta2&#39; a un dataframe de Python # encuesta_sta2_py &lt;- pd$DataFrame(encuesta_sta2) # # # Obtener &#39;X&#39; y &#39;y&#39; del dataframe de Python # X &lt;- encuesta_sta2_py[cov_names] # y &lt;- encuesta_sta2_py[[&#39;yk&#39;]] # # # Crear el modelo de clasificación, por ejemplo, un Random Forest # modelo &lt;- sklearn_ensemble$RandomForestClassifier() # # # Crear el selector RFE con el modelo y el número de características a seleccionar # selector &lt;- sklearn_fs$RFE(modelo, n_features_to_select = as.integer(10)) # # # Ajustar los datos # selector$fit(X, y) # # # Obtener las variables seleccionadas # variables_seleccionadas &lt;- X[selector$support_] %&gt;% names() variables_seleccionadas &lt;- c( &quot;porc_rur&quot;, &quot;porc_urb&quot;, &quot;porc_ing_ilpi_rur&quot;, &quot;porc_ing_ilpi_urb&quot;, &quot;porc_jub_urb&quot;, &quot;porc_segsoc15&quot;, &quot;plp15&quot;, &quot;ictpc15&quot;, &quot;pob_urb&quot;, &quot;pob_tot&quot; ) cov_names &lt;- c( &quot;modifica_humana&quot;, &quot;acceso_hosp&quot;, &quot;acceso_hosp_caminando&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;luces_nocturnas&quot;, variables_seleccionadas ) cov_registros &lt;- setdiff( cov_names, c( &quot;elec_mun20&quot;, &quot;elec_mun19&quot;, &quot;transf_gobpc_15_20&quot;, &quot;derhab_pea_15_20&quot;, &quot;vabpc_15_19&quot;, &quot;itlpis_15_20&quot;, &quot;remespc_15_20&quot;, &quot;desem_15_20&quot;, &quot;porc_urb&quot;, &quot;edad65mas_urb&quot;, &quot;pob_tot&quot;, &quot;acc_muyalto&quot;, &quot;smg1&quot;, &quot;ql_porc_cpa_rur&quot;, &quot;ql_porc_cpa_urb&quot; ) ) cov_registros &lt;- paste0(cov_registros, collapse = &quot; + &quot;) Definición de la Fórmula del Modelo Se define la fórmula del modelo incluyendo los efectos aleatorios y las variables predictoras seleccionadas. formula_model &lt;- paste0( &quot;cbind(si, no) ~ (1 | cve_mun) + (1 | hlengua) + (1 | discapacidad) + nivel_edu + edad + ent + area + sexo &quot;, &quot; + &quot;, cov_registros ) Ajuste del Modelo Se ajusta el modelo utilizando la función modelo_dummy y se guarda el resultado. fit &lt;- modelo_dummy( encuesta_sta = encuesta_sta %&gt;% mutate(yk = ifelse(ic_segsoc == 1 ,1,0)), predictors = statelevel_predictors_df, censo_sta = censo_sta, formula_mod = formula_model, byAgrega = byAgrega ) #--- Exporting Bayesian Multilevel Model Results ---# saveRDS(fit, file = &quot;output/2020/modelos/fit_mrp_ic_segsoc.rds&quot;) "],["postbenchmarking_ictpc.html", "09_PostBenchmarking_ictpc.R", " 09_PostBenchmarking_ictpc.R Para la ejecución del presente archivo, debe abrir el archivo 09_PostBenchmarking_ictpc.R disponible en la ruta Rcodes/2020/09_PostBenchmarking_ictpc.R. Este script en R se centra en la validación y benchmarking de un modelo multinivel para predecir ingresos, utilizando datos de encuestas y censos. Comienza limpiando el entorno de trabajo (rm(list = ls())) y cargando las librerías necesarias. Además, se leen las funciones auxiliares desde archivos externos (Plot_validacion_bench.R y Benchmarking.R). Posteriormente, se carga el modelo preentrenado desde un archivo RDS (fit_mrp_ictpc.rds). El proceso de benchmarking se realiza mediante la función benchmarking, utilizando covariables específicas como ent, area, sexo, edad, discapacidad y hlengua. El resultado se guarda en list_bench. A continuación, se crea un diseño de encuesta (diseno_encuesta) y se realizan validaciones a nivel nacional comparando los resultados del modelo ajustado (yk_lmer) y del modelo ajustado con benchmarking (yk_bench). Estas validaciones se llevan a cabo mediante la comparación de medias ponderadas. Para validar el modelo a nivel de subgrupos, se definen diversos subgrupos como ent, area, sexo, edad, discapacidad, hlengua y nivel_edu. Se crean gráficos de validación para cada subgrupo utilizando la función plot_uni_validacion. Estos gráficos se combinan y se guardan en un archivo JPG (plot_uni.jpg). Además, se guarda la lista de gráficos y el dataframe postestratificado en archivos RDS (plot_uni.rds y poststrat_df_ictpc.rds respectivamente), lo que permite una fácil recuperación y análisis posterior. rm(list =ls()) cat(&quot;\\f&quot;) ############################################################### # Loading required libraries ---------------------------------------------- ############################################################### library(scales) library(patchwork) library(srvyr) library(survey) library(haven) library(sampling) library(tidyverse) ############################################################### # Lectura de funciones ############################################################### source(&quot;../source/Plot_validacion_bench.R&quot;, encoding = &quot;UTF-8&quot;) source(&quot;../source/Benchmarking.R&quot;, encoding = &quot;UTF-8&quot;) ############################################################### # Lectura del modelo ############################################################### fit &lt;- readRDS(&quot;../output/2020/modelos/fit_mrp_ictpc.rds&quot;) proceso de benchmarking list_bench &lt;- benchmarking(modelo = fit, names_cov = c(&quot;ent&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;), metodo = &quot;logit&quot;) # saveRDS(list_bench,&quot;output/2020/plots/ictpc/list_bench.rds&quot;) # list_bench &lt;- readRDS(&quot;output/2020/plots/ictpc/list_bench.rds&quot;) Validaciones y plot uni poststrat_df &lt;- fit$poststrat_df %&gt;% data.frame() %&gt;% mutate(yk_lmer = yk, gk_bench = list_bench$gk_bench, yk_bench = yk * gk_bench ) diseno_encuesta &lt;- fit$encuesta_mrp %&gt;% mutate(yk_dir = yk) %&gt;% as_survey_design(weights = fep) mean(predict(fit$fit_mrp,type = &quot;response&quot;)) mean(diseno_encuesta$variables$yk) ## validación nacional. cbind( diseno_encuesta %&gt;% summarise(Nacional_dir = survey_mean(yk_dir)) %&gt;% select(Nacional_dir), poststrat_df %&gt;% summarise( Nacional_lmer = sum(n * yk_lmer) / sum(n), Nacional_bench = sum(n * yk_bench) / sum(n*gk_bench), )) %&gt;% print() Validaciones por subgrupo completo subgrupo &lt;- c(&quot;ent&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;,&quot;nivel_edu&quot;) plot_subgrupo &lt;- map( .x = setNames(subgrupo, subgrupo), ~ plot_uni_validacion( sample_diseno = diseno_encuesta, poststrat = poststrat_df, by1 = .x ) ) plot_subgrupo$sexo$tabla %&gt;% arrange(desc(n_sample) ) %&gt;% mutate(ER = abs((directo - stan_lmer)/directo )*100) %&gt;% data.frame() %&gt;% select(sexo:directo_upp,stan_lmer, ER) plot_subgrupo$ent$gg_plot plot_subgrupo$sexo$gg_plot + plot_subgrupo$nivel_edu$gg_plot + plot_subgrupo$edad$gg_plot + plot_subgrupo$hlengua$gg_plot + plot_subgrupo$area$gg_plot + plot_subgrupo$discapacidad$gg_plot plot_uni &lt;- plot_subgrupo$ent$gg_plot / ( plot_subgrupo$sexo$gg_plot + plot_subgrupo$nivel_edu$gg_plot + plot_subgrupo$edad$gg_plot + plot_subgrupo$hlengua$gg_plot + plot_subgrupo$area$gg_plot + plot_subgrupo$discapacidad$gg_plot ) ggsave(plot = plot_uni, filename = &quot;../output/2020/modelos/plot_uni.jpg&quot;, scale = 3) saveRDS(object = plot_subgrupo, file = &quot;../output/2020/modelos/plot_uni.rds&quot;) saveRDS(object = poststrat_df, file = &quot;../input/2020/muestra_ampliada/poststrat_df_ictpc.rds&quot;) "],["postbenchmarking_ic_ali_nc.html", "10_PostBenchmarking_ic_ali_nc.R", " 10_PostBenchmarking_ic_ali_nc.R Para la ejecución del presente archivo, debe abrir el archivo 10_PostBenchmarking_ic_ali_nc.R disponible en la ruta Rcodes/2020/10_PostBenchmarking_ic_ali_nc.R. Este script en R se centra en la validación y benchmarking de un modelo multinivel para predecir la carencia en alimentos nutritivos y de calidad (ic_ali_nc) utilizando datos de encuestas y censos. Comienza limpiando el entorno de trabajo y cargando librerías necesarias. Además, se leen funciones auxiliares desde archivos externos para la validación y el benchmarking. Luego, se carga el modelo preentrenado desde un archivo RDS. El proceso de benchmarking se realiza con la función benchmarking, utilizando covariables específicas como ent, area, sexo, edad, discapacidad y hlengua, y aplicando el método “logit”. Los resultados se almacenan en list_bench. A continuación, se crea un diseño de encuesta y se realizan validaciones a nivel nacional comparando los resultados del modelo ajustado (yk_stan_lmer) y del modelo ajustado con benchmarking (yk_bench). Estas validaciones se llevan a cabo mediante la comparación de medias ponderadas. Para validar el modelo a nivel de subgrupos, se definen diversos subgrupos como ent, area, sexo, edad, discapacidad, hlengua y nivel_edu. Se crean gráficos de validación para cada subgrupo utilizando la función plot_uni_validacion. Estos gráficos se combinan y se guardan en un archivo JPG (plot_uni_ic_ali_nc.jpg). Además, se guarda la lista de gráficos y el dataframe postestratificado en archivos RDS (plot_uni_ic_ali_nc.rds y poststrat_df_ic_ali_nc.rds respectivamente), lo que permite una fácil recuperación y análisis posterior. rm(list =ls()) cat(&quot;\\f&quot;) ############################################################### # Loading required libraries ---------------------------------------------- ############################################################### library(scales) library(patchwork) library(srvyr) library(survey) library(haven) library(sampling) library(tidyverse) ############################################################### # Lectura de funciones ############################################################### source(&quot;../source/Plot_validacion_bench.R&quot;, encoding = &quot;UTF-8&quot;) source(&quot;../source/Benchmarking.R&quot;, encoding = &quot;UTF-8&quot;) ############################################################### # Lectura del modelo ############################################################### fit &lt;- readRDS(&quot;../output/2020/modelos/fit_mrp_ic_ali_nc.rds&quot;) proceso de benchmarking list_bench &lt;- benchmarking(modelo = fit, names_cov = c(&quot;ent&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;), metodo = &quot;logit&quot;) Validaciones y plot uni poststrat_df &lt;- fit$poststrat_df %&gt;% data.frame() %&gt;% mutate(yk_stan_lmer = yk, gk_bench = list_bench$gk_bench, yk_bench = yk * gk_bench ) diseno_encuesta &lt;- fit$encuesta_mrp %&gt;% mutate(yk_dir = yk) %&gt;% as_survey_design(weights = fep) mean(predict(fit$fit_mrp,type = &quot;response&quot;)) mean(diseno_encuesta$variables$yk) ## validación nacional. cbind( diseno_encuesta %&gt;% summarise(Nacional_dir = survey_mean(yk_dir)) %&gt;% select(Nacional_dir), poststrat_df %&gt;% summarise( Nacional_stan_lmer = sum(n * yk_stan_lmer) / sum(n), Nacional_bench = sum(n * yk_bench) / sum(n*gk_bench), )) %&gt;% print() Validaciones por subgrupo completo subgrupo &lt;- c(&quot;ent&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;,&quot;nivel_edu&quot;) plot_subgrupo &lt;- map( .x = setNames(subgrupo, subgrupo), ~ plot_uni_validacion( sample_diseno = diseno_encuesta, poststrat = poststrat_df, by1 = .x ) ) plot_subgrupo$sexo$tabla %&gt;% arrange(desc(n_sample) ) %&gt;% mutate(ER = abs((directo - stan_lmer)/directo )*100) %&gt;% data.frame() %&gt;% select(sexo:directo_upp,stan_lmer, ER) plot_subgrupo$ent$gg_plot plot_subgrupo$sexo$gg_plot + plot_subgrupo$nivel_edu$gg_plot + plot_subgrupo$edad$gg_plot + plot_subgrupo$hlengua$gg_plot + plot_subgrupo$area$gg_plot + plot_subgrupo$discapacidad$gg_plot plot_uni &lt;- plot_subgrupo$ent$gg_plot / ( plot_subgrupo$sexo$gg_plot + plot_subgrupo$nivel_edu$gg_plot + plot_subgrupo$edad$gg_plot + plot_subgrupo$hlengua$gg_plot + plot_subgrupo$area$gg_plot + plot_subgrupo$discapacidad$gg_plot ) ggsave(plot = plot_uni, filename = &quot;../output/2020/modelos/plot_uni_ic_ali_nc.jpg&quot;, scale = 3) saveRDS(object = plot_subgrupo, file = &quot;../output/2020/modelos/plot_uni_ic_ali_nc.rds&quot;) saveRDS(object = poststrat_df, file = &quot;../input/2020/muestra_ampliada/poststrat_df_ic_ali_nc.rds&quot;) "],["postbenchmarking_ic_segsoc.html", "11_PostBenchmarking_ic_segsoc.r", " 11_PostBenchmarking_ic_segsoc.r Para la ejecución del presente análisis, se debe abrir el archivo 11_PostBenchmarking_ic_segsoc.R disponible en la ruta Rcodes/2020/11_PostBenchmarking_ic_segsoc.R. Este script en R se centra en el proceso de benchmarking y validación de un modelo multinivel previamente ajustado para lala carencia en por cobertura de seguridad social. Inicia con la limpieza del entorno de trabajo y la carga de librerías esenciales como scales, patchwork, srvyr, survey, haven, sampling y tidyverse. Luego, se cargan funciones adicionales desde archivos externos para la validación y el benchmarking del modelo. El modelo ajustado se lee desde un archivo RDS (fit_mrp_ic_segsoc.rds) y se procede a la etapa de benchmarking utilizando la función benchmarking, aplicando el método logit sobre un conjunto de covariables específicas (ent, area, sexo, edad, discapacidad, hlengua). Los resultados del benchmarking se incorporan al dataframe de post-estratificación y se recalculan las predicciones ponderadas. El script también realiza validaciones a nivel nacional y por subgrupos (ent, area, sexo, edad, discapacidad, hlengua, nivel_edu). Para cada subgrupo, se generan gráficos de validación univariada utilizando la función plot_uni_validacion, que compara las estimaciones directas y ajustadas del modelo. Los gráficos resultantes se combinan y se guardan en un archivo JPG, proporcionando una visualización completa del desempeño del modelo. Finalmente, tanto los gráficos de subgrupos como el dataframe de post-estratificación se guardan en archivos RDS para futuros análisis y referencia. rm(list =ls()) cat(&quot;\\f&quot;) ############################################################### # Loading required libraries ---------------------------------- ############################################################### library(scales) library(patchwork) library(srvyr) library(survey) library(haven) library(sampling) library(tidyverse) ############################################################### # Lectura de funciones ############################################################### source(&quot;../source/Plot_validacion_bench.R&quot;, encoding = &quot;UTF-8&quot;) source(&quot;../source/Benchmarking.R&quot;, encoding = &quot;UTF-8&quot;) ############################################################### # Lectura del modelo ############################################################### fit &lt;- readRDS(&quot;../output/2020/modelos/fit_mrp_ic_segsoc.rds&quot;) proceso de benchmarking list_bench &lt;- benchmarking(modelo = fit, names_cov = c(&quot;ent&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;), metodo = &quot;logit&quot;) Validaciones y plot uni poststrat_df &lt;- fit$poststrat_df %&gt;% data.frame() %&gt;% mutate(yk_stan_lmer = yk, gk_bench = list_bench$gk_bench, yk_bench = yk * gk_bench ) diseno_encuesta &lt;- fit$encuesta_mrp %&gt;% mutate(yk_dir = yk) %&gt;% as_survey_design(weights = fep) mean(predict(fit$fit_mrp,type = &quot;response&quot;)) mean(diseno_encuesta$variables$yk) ## validación nacional. cbind( diseno_encuesta %&gt;% summarise(Nacional_dir = survey_mean(yk_dir)) %&gt;% select(Nacional_dir), poststrat_df %&gt;% summarise( Nacional_stan_lmer = sum(n * yk_stan_lmer) / sum(n), Nacional_bench = sum(n * yk_bench) / sum(n*gk_bench), )) %&gt;% print() Validaciones por subgrupo completo subgrupo &lt;- c(&quot;ent&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;,&quot;nivel_edu&quot;) plot_subgrupo &lt;- map( .x = setNames(subgrupo, subgrupo), ~ plot_uni_validacion( sample_diseno = diseno_encuesta, poststrat = poststrat_df, by1 = .x ) ) plot_subgrupo$sexo$tabla %&gt;% arrange(desc(n_sample) ) %&gt;% mutate(ER = abs((directo - stan_lmer)/directo )*100) %&gt;% data.frame() %&gt;% select(sexo:directo_upp,stan_lmer, ER) plot_subgrupo$ent$gg_plot plot_subgrupo$sexo$gg_plot + plot_subgrupo$nivel_edu$gg_plot + plot_subgrupo$edad$gg_plot + plot_subgrupo$hlengua$gg_plot + plot_subgrupo$area$gg_plot + plot_subgrupo$discapacidad$gg_plot plot_uni &lt;- plot_subgrupo$ent$gg_plot / ( plot_subgrupo$sexo$gg_plot + plot_subgrupo$nivel_edu$gg_plot + plot_subgrupo$edad$gg_plot + plot_subgrupo$hlengua$gg_plot + plot_subgrupo$area$gg_plot + plot_subgrupo$discapacidad$gg_plot ) ggsave(plot = plot_uni, filename = &quot;../output/2020/modelos/plot_uni_ic_segsoc.jpg&quot;, scale = 3) saveRDS(object = plot_subgrupo, file = &quot;../output/2020/modelos/plot_uni_ic_segsoc.rds&quot;) saveRDS(object = poststrat_df, file = &quot;../input/2020/muestra_ampliada/poststrat_df_ic_segsoc.rds&quot;) "],["imputacion_censal.html", "12_Imputacion_censal.R", " 12_Imputacion_censal.R Para la ejecución del presente análisis, se debe abrir el archivo 12_Imputacion_censal.R disponible en la ruta Rcodes/2020/12_Imputacion_censal.R. Este script en R se centra en el análisis y la predicción de datos utilizando modelos estadísticos ajustados a encuestas y censos. Primero, se limpia el entorno de trabajo y se cargan diversas librerías necesarias para el análisis. Luego, se leen modelos previamente ajustados (fit_ingreso, fit_alimento, fit_salud) y datos de encuestas y líneas de bienestar. Se realiza una combinación de datos entre las encuestas y predictores de nivel estatal, y se calculan predicciones basadas en estos modelos. Los resultados de estas predicciones se almacenan en un archivo RDS para su posterior análisis. El script también incluye un paso para ajustar y validar los datos de la encuesta ampliada mediante simulaciones de variables predictoras y el cálculo de medidas de desviación estándar residual. Posteriormente, se lleva a cabo un proceso iterativo para generar nuevas predicciones y comparar estas predicciones con las originales. Finalmente, el script prepara los datos para un análisis posterior al actualizar las variables con valores simulados basados en las predicciones y desviaciones calculadas. Este proceso permite validar y ajustar los modelos utilizados, asegurando la precisión de las predicciones en la encuesta ampliada. Limpieza del Entorno y Carga de Bibliotecas rm(list = ls()) library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(haven) library(labelled) library(sampling) library(lme4) library(survey) library(srvyr) cat(&quot;\\f&quot;) source(&quot;../source/benchmarking_indicador.R&quot;) Carga de Modelos y Datos Se leen los modelos ajustados previamente para ingresos, alimentación, y salud. fit_ingreso &lt;- readRDS(&quot;../output/2020/modelos/fit_mrp_ictpc.rds&quot;)$fit_mrp fit_alimento &lt;- readRDS(&quot;../output/2020/modelos/fit_mrp_ic_ali_nc.rds&quot;)$fit_mrp fit_salud &lt;- readRDS(&quot;../output/2020/modelos/fit_mrp_ic_segsoc.rds&quot;)$fit_mrp Se cargan los datos de la encuesta, la línea de bienestar, y los predictores a nivel estatal. También se lee una muestra ampliada del censo. encuesta_enigh &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) %&gt;% mutate(ingreso = ictpc) # lineas de bienestar LB &lt;- read.delim( &quot;input/2020/Lineas_Bienestar.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot; ) %&gt;% mutate(area = as.character(area)) statelevel_predictors &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_df.rds&quot;) ############################################################### # Lectura del muestras intercensal o muestra_ampliada del censo ############################################################### muestra_ampliada &lt;- readRDS(&quot;../output/2020/encuesta_ampliada.rds&quot;) col_names_muestra &lt;- names(muestra_ampliada) muestra_ampliada &lt;- inner_join(muestra_ampliada, statelevel_predictors) muestra_ampliada &lt;- muestra_ampliada %&gt;% inner_join(LB) Predicción y Evaluación Se realizan predicciones utilizando los modelos ajustados sobre la muestra ampliada. pred_ingreso &lt;- predict(fit_ingreso, newdata = muestra_ampliada, allow.new.levels = TRUE, type = &quot;response&quot;) pred_ic_ali_nc &lt;- predict(fit_alimento, newdata = muestra_ampliada, allow.new.levels = TRUE, type = &quot;response&quot;) pred_segsoc &lt;- predict(fit_salud, newdata = muestra_ampliada, allow.new.levels = TRUE, type = &quot;response&quot;) Se calcula la desviación estándar residual para ajustar el modelo de ingreso. paso &lt;- encuesta_enigh %&gt;% mutate(pred_ingreso = pred_ingreso) %&gt;% survey::svydesign( id = ~ upm , strata = ~ estrato , data = ., weights = ~ fep ) %&gt;% as_survey_design() sd_1 &lt;- paso %&gt;% group_by(cve_mun) %&gt;% summarise(media_obs = survey_mean(ingreso), media_pred = survey_mean(pred_ingreso)) %&gt;% summarise(media_sd = sqrt(mean(c( media_obs - media_pred ) ^ 2))) desv_estandar_residual &lt;- min(c(as.numeric(sd_1), sigma(fit_ingreso))) Se calcula el Índice de Pobreza Multidimensional (IPM) utilizando las predicciones de los modelos y los datos originales. encuesta_enigh &lt;- encuesta_enigh %&gt;% inner_join(LB) %&gt;% mutate( tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu, ipm = case_when( # Población en situación de pobreza. ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, # Población vulnerable por carencias sociales. ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, # Poblacion vulnerable por ingresos. ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, # Población no pobre multidimensional y no vulnerable. ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ) ) Se realiza una predicción final sobre la muestra ampliada y se guarda el resultado. saveRDS(list(pred_ingreso = pred_ingreso, pred_ic_ali_nc = pred_ic_ali_nc, pred_segsoc = pred_segsoc, desv_estandar_residual = desv_estandar_residual), file = &quot;../output/2020/modelos/predicciones.rds&quot;) Se ajustan los valores en la muestra ampliada utilizando las predicciones y se valida la precisión de las predicciones. muestra_ampliada_pred &lt;- muestra_ampliada %&gt;% select(all_of(col_names_muestra), li, lp) %&gt;% mutate( ic_segsoc = rbinom(n = n(), size = 1, prob = pred_segsoc), ic_ali_nc = rbinom(n = n(), size = 1, prob = pred_ic_ali_nc), ingreso = pred_ingreso + rnorm(n = n(), mean = 0, desv_estandar_residual), tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu ) # Validación de los predict mean(muestra_ampliada_pred$ic_ali_nc) - mean(pred_ic_ali_nc) mean(muestra_ampliada_pred$ic_segsoc) - mean(pred_segsoc) mean(muestra_ampliada_pred$ingreso) - mean(pred_ingreso) "],["estimacion_ent_02.html", "13_Estimacion_ent_02.R", " 13_Estimacion_ent_02.R Para la ejecución del presente archivo, debe abrir el archivo 13_Estimacion_ent_02.R disponible en la ruta Rcodes/2020/13_Estimacion_ent_02.R. Inicialización y Carga de Librerías En esta primera sección, se limpia el entorno de trabajo eliminando todas las variables y objetos existentes mediante rm(list = ls()). A continuación, se cargan diversas librerías necesarias para el análisis de datos y modelado, como patchwork para la visualización, lme4 para modelos lineales mixtos, tidyverse para manipulación de datos, y otras librerías específicas para análisis de encuestas y predicción. También se incluye un archivo externo de funciones llamado modelos_freq.R. rm(list = ls()) library(patchwork) library(nortest) library(lme4) library(tidyverse) library(magrittr) library(caret) library(car) library(survey) library(srvyr) source(&quot;../source/modelos_freq.R&quot;) Configuración del Nivel de Agregación y Carga de Datos Aquí se define un vector byAgrega que especifica los niveles de agregación para el análisis, como entidad, municipio, área, y variables demográficas. byAgrega &lt;- c(&quot;ent&quot;, &quot;cve_mun&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;edad&quot;, &quot;discapacidad&quot;, &quot;hlengua&quot;, &quot;nivel_edu&quot; ) memory.limit(10000000) Esta sección carga y filtra los datos necesarios desde archivos RDS y CSV. Se establece un límite de memoria y se cargan varios conjuntos de datos relevantes para el análisis, como la encuesta enigh, datos del censo, y predictores a nivel estatal. También se carga un archivo de líneas de bienestar y se actualizan los nombres de las variables para el análisis. encuesta_enigh &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) %&gt;% filter(ent == &quot;02&quot;, ictpc &lt;= 10000) censo_sta &lt;- readRDS(&quot;../input/2020/muestra_ampliada/muestra_cuestionario_ampliado.rds&quot;) %&gt;% filter(ent == &quot;02&quot;) statelevel_predictors_df &lt;- readRDS(&quot;../input/2020/predictores/statelevel_predictors_df.rds&quot;) %&gt;% filter(ent == &quot;02&quot;) muestra_ampliada &lt;- readRDS(&quot;output/2020/encuesta_ampliada.rds&quot;) %&gt;% filter(ent == &quot;02&quot;) LB &lt;- read.delim( &quot;../input/2020/Lineas_Bienestar.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot; ) %&gt;% mutate(area = as.character(area)) cov_names &lt;- names(statelevel_predictors_df) cov_names &lt;- cov_names[!grepl(x = cov_names, pattern = &quot;^hog_|cve_mun&quot;)] Preparación de Datos y Modelado Aquí se preparan los datos combinando diferentes conjuntos mediante inner_join para integrar información de predictores estatales y líneas de bienestar con la muestra ampliada. col_names_muestra &lt;- names(muestra_ampliada) muestra_ampliada &lt;- inner_join(muestra_ampliada, statelevel_predictors_df) muestra_ampliada &lt;- muestra_ampliada %&gt;% inner_join(LB) Modelado del Ingreso Se seleccionan y preparan las variables para modelar el ingreso. Se define la fórmula del modelo que incluye efectos aleatorios y fijos, y se ajusta un modelo usando la función modelo_ingreso. Los resultados del modelo se guardan en fit_ingreso. variables_seleccionadas &lt;- c( &quot;prom_esc_rel_urb&quot;, &quot;smg1&quot;, &quot;gini15m&quot;, &quot;ictpc15&quot;, &quot;altitud1000&quot;, &quot;prom_esc_rel_rur&quot;, &quot;porc_patnoagrnocal_urb&quot;, &quot;acc_medio&quot;, &quot;derhab_pea_15_20&quot;, &quot;porc_norep_ing_urb&quot; ) cov_names &lt;- c( &quot;modifica_humana&quot;, &quot;acceso_hosp&quot;, &quot;acceso_hosp_caminando&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;luces_nocturnas&quot;, variables_seleccionadas ) cov_registros &lt;- setdiff( cov_names, c( &quot;elec_mun20&quot;, &quot;elec_mun19&quot;, &quot;transf_gobpc_15_20&quot;, &quot;derhab_pea_15_20&quot;, &quot;vabpc_15_19&quot;, &quot;itlpis_15_20&quot;, &quot;remespc_15_20&quot;, &quot;desem_15_20&quot;, &quot;smg1&quot;, &quot;ql_porc_cpa_urb&quot;, &quot;ql_porc_cpa_rur&quot; ) ) cov_registros &lt;- paste0(cov_registros, collapse = &quot; + &quot;) formula_model &lt;- paste0(&quot;ingreso ~ (1 | cve_mun) + (1 | hlengua) + (1 | discapacidad) + nivel_edu + edad + area + sexo +&quot;, &quot; + &quot;, cov_registros) encuesta_enigh$ingreso &lt;- (as.numeric(encuesta_enigh$ictpc)) fit &lt;- modelo_ingreso( encuesta_sta = encuesta_enigh, predictors = statelevel_predictors_df, censo_sta = censo_sta, formula_mod = formula_model, byAgrega = byAgrega ) fit_ingreso &lt;- fit$fit_mrp Modelado de Alimentos de Calidad y Seguridad Social Esta sección es similar a la anterior, pero se enfoca en modelar la calidad de alimentos y seguridad social. Se ajustan modelos para alimentos de calidad y seguridad social, con fórmulas adecuadas y utilizando la función modelo_dummy. variables_seleccionadas &lt;- c( &quot;porc_ing_ilpi_urb&quot;, &quot;pob_ind_rur&quot;, &quot;pob_ind_urb&quot;, &quot;porc_hogremesas_rur&quot;, &quot;porc_segsoc15&quot;, &quot;porc_ali15&quot;, &quot;plp15&quot;, &quot;pob_rur&quot;, &quot;altitud1000&quot; ) cov_names &lt;- c( &quot;modifica_humana&quot;, &quot;acceso_hosp&quot;, &quot;acceso_hosp_caminando&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;luces_nocturnas&quot;, variables_seleccionadas ) cov_registros &lt;- setdiff( cov_names, c( &quot;elec_mun20&quot;, &quot;elec_mun19&quot;, &quot;transf_gobpc_15_20&quot;, &quot;derhab_pea_15_20&quot;, &quot;vabpc_15_19&quot;, &quot;itlpis_15_20&quot;, &quot;remespc_15_20&quot;, &quot;desem_15_20&quot;, &quot;porc_urb&quot;, &quot;edad65mas_urb&quot;, &quot;pob_tot&quot;, &quot;acc_muyalto&quot;, &quot;smg1&quot;, &quot;ql_porc_cpa_rur&quot;, &quot;ql_porc_cpa_urb&quot; ) ) cov_registros &lt;- paste0(cov_registros, collapse = &quot; + &quot;) formula_model &lt;- paste0( &quot;cbind(si, no) ~ (1 | cve_mun) + (1 | hlengua) + (1 | discapacidad) + nivel_edu + edad + area + sexo &quot;, &quot; + &quot;, cov_registros ) fit &lt;- modelo_dummy( encuesta_sta = encuesta_enigh %&gt;% mutate(yk = ifelse(ic_ali_nc == 1 , 1, 0)), predictors = statelevel_predictors_df, censo_sta = censo_sta, formula_mod = formula_model, byAgrega = byAgrega ) fit_alimento &lt;- fit$fit_mrp ################################################################################ # modelo para seguridad social # ################################################################################ variables_seleccionadas &lt;- c( &quot;porc_rur&quot;, &quot;porc_urb&quot;, &quot;porc_ing_ilpi_rur&quot;, &quot;porc_ing_ilpi_urb&quot;, &quot;porc_jub_urb&quot;, &quot;porc_segsoc15&quot;, &quot;plp15&quot;, &quot;ictpc15&quot;, &quot;pob_urb&quot;, &quot;pob_tot&quot; ) cov_names &lt;- c( &quot;modifica_humana&quot;, &quot;acceso_hosp&quot;, &quot;acceso_hosp_caminando&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;luces_nocturnas&quot; , variables_seleccionadas ) cov_registros &lt;- setdiff( cov_names, c( &quot;elec_mun20&quot;, &quot;elec_mun19&quot;, &quot;transf_gobpc_15_20&quot;, &quot;derhab_pea_15_20&quot;, &quot;vabpc_15_19&quot; , &quot;itlpis_15_20&quot; , &quot;remespc_15_20&quot;, &quot;desem_15_20&quot;, &quot;porc_urb&quot; , &quot;edad65mas_urb&quot;, &quot;pob_tot&quot; , &quot;acc_muyalto&quot; , &quot;smg1&quot;, &quot;ql_porc_cpa_rur&quot;, &quot;ql_porc_cpa_urb&quot; ) ) cov_registros &lt;- paste0(cov_registros, collapse = &quot; + &quot;) formula_model &lt;- paste0( &quot;cbind(si, no) ~ (1 | cve_mun) + (1 | hlengua) + (1 | discapacidad) + nivel_edu + edad + area + sexo &quot; , &quot; + &quot;, cov_registros ) fit &lt;- modelo_dummy( encuesta_sta = encuesta_enigh %&gt;% mutate(yk = ifelse(ic_segsoc == 1 ,1,0)) , predictors = statelevel_predictors_df, censo_sta = censo_sta , formula_mod = formula_model, byAgrega = byAgrega ) fit_segsoc &lt;- fit$fit_mrp Predicción y Validación En esta parte, se realiza la predicción del ingreso utilizando el modelo ajustado y se evalúa la precisión de las predicciones. Se calcula el error estándar residual y se ajustan los valores en función de las predicciones realizadas. encuesta_sta &lt;- inner_join(encuesta_enigh, statelevel_predictors_df) pred_ingreso &lt;- (predict(fit_ingreso, newdata = encuesta_sta)) sum(pred_ingreso &lt; 3560) rm(encuesta_sta) paso &lt;- encuesta_enigh %&gt;% mutate(pred_ingreso = pred_ingreso) %&gt;% survey::svydesign(id =~ upm , strata = ~ estrato , data = ., weights = ~ fep) %&gt;% as_survey_design() sd_1 &lt;- paso %&gt;% group_by(cve_mun) %&gt;% summarise(media_obs = survey_mean(ingreso), media_pred = survey_mean(pred_ingreso)) %&gt;% summarise(media_sd = sqrt(mean(c(media_obs - media_pred)^2))) desv_estandar_residual &lt;- min(c(as.numeric(sd_1), sigma(fit_ingreso))) Cálculo del Índice de Pobreza Multidimensional (IPM) Finalmente, se calcula el índice de pobreza multidimensional (IPM) basado en varias dimensiones, se ajusta la encuesta con estos datos, y se calcula la media del IPM. encuesta_enigh &lt;- encuesta_enigh %&gt;% inner_join(LB) %&gt;% mutate( tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu, ipm = case_when( # Población en situación de pobreza. ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, # Población vulnerable por carencias sociales. ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, # Poblacion vulnerable por ingresos. ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, # Población no pobre multidimensional y no vulnerable. ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ), # Población en situación de pobreza moderada. pobre_moderada = ifelse(c(ingreso &gt; li &amp; ingreso &lt; lp) &amp; tol_ic &gt; 2, 1, 0), pobre_extrema = ifelse(ipm == &quot;I&quot; &amp; pobre_moderada == 0, 1, 0) ) Predicción con los Modelos Ajustados Se generan predicciones para las variables de interés (ingreso, ic_ali_nc, ic_segsoc) usando los modelos ajustados (fit_ingreso, fit_alimento, fit_segsoc) y el conjunto de datos muestra_ampliada. Se permite el uso de nuevos niveles en las predicciones. pred_ingreso &lt;- predict(fit_ingreso, newdata = muestra_ampliada , allow.new.levels = TRUE, type = &quot;response&quot;) pred_ic_ali_nc &lt;- predict(fit_alimento, newdata = muestra_ampliada , allow.new.levels = TRUE, type = &quot;response&quot;) pred_segsoc &lt;- predict(fit_segsoc, newdata = muestra_ampliada , allow.new.levels = TRUE, type = &quot;response&quot;) Creación de Variables Dummy y Preparación de Datos Aquí, se crean variables dummy para indicadores de salud, seguridad social, y otras características. Luego, se preparan datos adicionales para muestra_ampliada_pred, generando variables simuladas (ic_segsoc, ic_ali_nc) y ajustando el ingreso con ruido aleatorio. Se calcula el total del índice de carencias (tol_ic). muestra_ampliada %&lt;&gt;% mutate( ic_asalud = ifelse(ic_asalud == 1, 1,0), ic_cv = ifelse(ic_cv == 1, 1,0), ic_sbv = ifelse(ic_sbv == 1, 1,0), ic_rezedu = ifelse(ic_rezedu == 1, 1,0)) muestra_ampliada_pred &lt;- muestra_ampliada %&gt;% select(all_of(col_names_muestra), li, lp) %&gt;% mutate( ic_segsoc = rbinom(n = n(), size = 1, prob = pred_segsoc), ic_ali_nc = rbinom(n = n(), size = 1, prob = pred_ic_ali_nc), ingreso = pred_ingreso + rnorm(n = n(), mean = 0, desv_estandar_residual), tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu) Validación de las Predicciones Se validan las predicciones comparando las medias de las variables simuladas con las predicciones generadas por los modelos. Se calcula la diferencia entre estas medias para verificar la precisión de las predicciones. mean(muestra_ampliada_pred$ic_ali_nc) - mean(pred_ic_ali_nc) mean(muestra_ampliada_pred$ic_segsoc) - mean(pred_segsoc) mean(muestra_ampliada_pred$ingreso) - mean(pred_ingreso) Cálculo del Índice de Pobreza Multidimensional (IPM) con Predicciones Se clasifica la población en diferentes categorías de pobreza multidimensional (ipm) y se identifican las personas en pobreza moderada y extrema. Se calcula la proporción de cada categoría en muestra_ampliada_pred. muestra_ampliada_pred &lt;- muestra_ampliada_pred %&gt;% mutate( ipm = case_when( ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ), pobre_moderada = ifelse(c(ingreso &gt; li &amp; ingreso &lt; lp) &amp; tol_ic &gt; 2, 1, 0), pobre_extrema = ifelse(ipm == &quot;I&quot; &amp; pobre_moderada == 0, 1, 0) ) prop.table(table(muestra_ampliada_pred$ipm)) Estimación y Comparación del IPM con Predicciones Se estima el IPM para las predicciones y se compara con la estimación real basada en encuesta_enigh. Se calculan las proporciones de cada categoría IPM para ambas estimaciones. muestra_ampliada_pred %&gt;% filter() %&gt;% group_by(ipm) %&gt;% summarise(num_ipm =sum(factor)) %&gt;% mutate(est_ipm = num_ipm/sum(num_ipm)) encuesta_enigh %&gt;% group_by(ipm) %&gt;% summarise(num_ipm =sum(fep)) %&gt;% mutate(est_ipm = num_ipm/sum(num_ipm)) Ajuste de las Predicciones y Calibración Se actualizan las variables de muestra_ampliada_pred con información adicional y se elimina cualquier dato faltante en encuesta_enigh. muestra_ampliada_pred %&lt;&gt;% mutate( tol_ic4 = ic_asalud + ic_cv + ic_sbv + ic_rezedu, pred_segsoc = pred_segsoc, pred_ingreso = pred_ingreso, desv_estandar_residual = desv_estandar_residual, pred_ic_ali_nc = pred_ic_ali_nc ) ii_ent = &quot;02&quot; encuesta_enigh %&lt;&gt;% na.omit() Cálculo de la Población y Pobreza por Municipio Se calcula la densidad de población por municipio y se determina el porcentaje de población en cada categoría de IPM. Se estima la pobreza moderada y extrema en función de las encuestas y la población total. total_mpio &lt;- muestra_ampliada_pred %&gt;% group_by(cve_mun) %&gt;% summarise(den_mpio = sum(factor),.groups = &quot;drop&quot;) %&gt;% mutate(tot_ent = sum(den_mpio)) tot_pob &lt;- encuesta_enigh %&gt;% group_by(ipm) %&gt;% summarise(num_ent = sum(fep),.groups = &quot;drop&quot;) %&gt;% transmute(ipm, prop_ipm = num_ent/sum(num_ent), tx_ipm = sum(total_mpio$den_mpio)*prop_ipm) %&gt;% filter(ipm != &quot;I&quot;) pobreza &lt;- encuesta_enigh %&gt;% summarise( pobre_ext = weighted.mean(pobre_extrema, fep), pob_mod = weighted.mean(pobre_moderada, fep), pobre_moderada = sum(total_mpio$den_mpio)*pob_mod, pobre_extrema = sum(total_mpio$den_mpio)*pobre_ext) Iteraciones de Calibración Este bloque realiza iteraciones para ajustar y calibrar los datos, actualizando las predicciones en cada iteración y comparando los resultados con la población real. Se calculan y guardan los resultados en archivos .rds para cada iteración. for( iter in 1:200){ cat(&quot;\\n iteracion = &quot;, iter,&quot;\\n\\n&quot;) muestra_ampliada_pred %&lt;&gt;% mutate( ic_segsoc = rbinom(n = n(), size = 1, prob = pred_segsoc), ic_ali_nc = rbinom(n = n(), size = 1, prob = pred_ic_ali_nc), ingreso = pred_ingreso + rnorm(n = n(),mean = 0, desv_estandar_residual), tol_ic = tol_ic4 + ic_segsoc + ic_ali_nc ) %&gt;% mutate( ipm = case_when( ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ), pobre_moderada = ifelse(c(ingreso &gt; li &amp; ingreso &lt; lp) &amp; tol_ic &gt; 2, 1, 0), pobre_extrema = ifelse(ipm == &quot;I&quot; &amp; pobre_moderada == 0, 1, 0) ) Xk &lt;- muestra_ampliada_pred %&gt;% select(&quot;ipm&quot;, &quot;cve_mun&quot;) %&gt;% fastDummies::dummy_columns(select_columns = c(&quot;ipm&quot;, &quot;cve_mun&quot;)) %&gt;% select(names(Tx_hat[-c(1:2)])) diseno_post &lt;- bind_cols(muestra_ampliada_pred,Xk) %&gt;% mutate(fep = factor) %&gt;% as_survey_design( ids = upm, weights = fep, nest = TRUE, # strata = estrato ) mod_calib &lt;- as.formula(paste0(&quot;~ -1+&quot;,paste0(names(Tx_hat), collapse = &quot; + &quot;))) diseno_calib &lt;- calibrate(diseno_post, formula = mod_calib, population = Tx_hat,calfun = &quot;raking&quot;) estima_calib_ipm &lt;- diseno_calib %&gt;% group_by(cve_mun,ipm) %&gt;% summarise(est_ipm = survey_mean(vartype =&quot;var&quot;)) estima_calib_pob &lt;- diseno_calib %&gt;% group_by(cve_mun) %&gt;% summarise(est_pob_ext = survey_mean(pobre_extrema , vartype =&quot;var&quot;), est_pob_mod = survey_mean(pobre_moderada , vartype =&quot;var&quot; )) estima_calib &lt;- pivot_wider( data = estima_calib_ipm, id_cols = &quot;cve_mun&quot;, names_from = &quot;ipm&quot;, values_from = c(&quot;est_ipm&quot;, &quot;est_ipm_var&quot;),values_fill = 0 ) %&gt;% full_join(estima_calib_pob) valida_ipm &lt;- estima_calib_ipm %&gt;% inner_join(total_mpio, by = &quot;cve_mun&quot;) %&gt;% group_by(ipm) %&gt;% summarise(prop_ampliada = sum(est_ipm*den_mpio)/unique(tot_ent)) %&gt;% full_join(tot_pob, by = &quot;ipm&quot;) valida_pob &lt;- estima_calib_pob %&gt;% select(cve_mun, est_pob_ext , est_pob_mod) %&gt;% inner_join(total_mpio, by = &quot;cve_mun&quot;) %&gt;% summarise(pob_ext_ampliada = sum(est_pob_ext * den_mpio) / unique(tot_ent), pob_mod_ampliada = sum(est_pob_mod * den_mpio) / unique(tot_ent), ipm_I = pob_ext_ampliada + pob_mod_ampliada) %&gt;% bind_cols(pobreza[,1:2]) saveRDS(list(valida = list(valida_pob = valida_pob, valida_ipm = valida_ipm), estima_calib = estima_calib), file = paste0( &quot;../output/2020/iteraciones/mpio_calib/&quot;, ii_ent,&quot;/iter&quot;,iter,&quot;.rds&quot;)) } fin &lt;- Sys.time() tiempo_total &lt;- difftime(fin, inicio, units = &quot;mins&quot;) print(tiempo_total) cat(&quot;####################################################################\\n&quot;) "],["estimacion_municipios_ipm_pobreza.html", "14_estimacion_municipios_ipm_pobreza.R", " 14_estimacion_municipios_ipm_pobreza.R Para la ejecución del presente análisis, se debe abrir el archivo 14_estimacion_municipios_ipm_pobreza.R disponible en la ruta Rcodes/2020/14_estimacion_municipios_ipm_pobreza.R. Este script en R realiza un análisis exhaustivo para calibrar y validar indicadores de pobreza a nivel municipal, utilizando modelos predictivos y datos de encuestas. Primero, el entorno de trabajo se limpia y se cargan las librerías necesarias para la manipulación de datos (tidyverse, data.table, magrittr, etc.) y para realizar cálculos estadísticos (lme4, survey, srvyr). Luego, se leen datos de predicciones y encuestas ampliadas, junto con información adicional sobre las líneas de bienestar. En la sección inicial de procesamiento, se unen y mutan los datos de la encuesta ENIGH para calcular indicadores de pobreza y vulnerabilidad multidimensional. Se asignan categorías de pobreza (ipm) y se crean nuevas variables que reflejan la pobreza moderada y extrema, basadas en umbrales de ingreso y carencias sociales. El script luego se centra en la preparación de datos para la calibración de modelos. La muestra ampliada se ajusta para incluir variables adicionales y predicciones de ingreso y carencias sociales. A continuación, se inicia un bucle que itera sobre un conjunto de códigos de entidad territorial. Para cada entidad, se filtran y ajustan los datos de la encuesta ampliada y se simulan nuevas variables (como ingresos y carencias sociales) utilizando distribuciones probabilísticas basadas en las predicciones anteriores. Dentro de cada iteración, se realiza una calibración de los datos utilizando el método de “raking”, que ajusta las estimaciones de los indicadores a las estimaciones poblacionales conocidas. Los resultados de la calibración se calculan para cada municipio y se comparan con los datos originales para validar la precisión de las estimaciones. Finalmente, los resultados de cada iteración se guardan en archivos RDS separados para su análisis posterior y se reporta el tiempo total de ejecución del proceso. Este enfoque permite realizar un ajuste fino de los modelos predictivos y asegurar que las estimaciones de pobreza sean lo más precisas posible, utilizando simulaciones y técnicas avanzadas de calibración. Limpieza del Entorno y Carga de Bibliotecas rm(list = ls()) library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(haven) library(labelled) library(sampling) library(lme4) library(survey) library(srvyr) source(&quot;../source/benchmarking_indicador.R&quot;) Lectura de Datos Se cargan los datos necesarios y las predicciones de los modelos previamente ajustados. predicciones &lt;- readRDS( &quot;../output/2020/modelos/predicciones.rds&quot;) muestra_ampliada &lt;- readRDS(&quot;../output/2020/encuesta_ampliada.rds&quot;) encuesta_enigh &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) %&gt;% mutate(ingreso = ictpc) LB &lt;- read.delim( &quot;../input/2020/Lineas_Bienestar.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot; ) %&gt;% mutate(area = as.character(area)) Cálculo del IPM en la Encuesta Aquí se calcula el IPM y las nuevas variables de pobreza moderada y extrema en la encuesta encuesta_enigh. ################################################################################ # IPM en la enigh ################################################################################ encuesta_enigh &lt;- encuesta_enigh %&gt;% inner_join(LB) %&gt;% mutate( tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu, ipm = case_when( # Población en situación de pobreza. ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, # Población vulnerable por carencias sociales. ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, # Poblacion vulnerable por ingresos. ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, # Población no pobre multidimensional y no vulnerable. ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ), # Población en situación de pobreza moderada. pobre_moderada = ifelse(c(ingreso &gt; li &amp; ingreso &lt; lp) &amp; tol_ic &gt; 2, 1, 0), pobre_extrema = ifelse(ipm == &quot;I&quot; &amp; pobre_moderada == 0, 1, 0) ) Preparación de la Muestra Intercensal Se ajusta la muestra ampliada y se añaden las predicciones y variables necesarias. ################################################################################ # preparando encuesta intercensal ################################################################################ muestra_ampliada &lt;- muestra_ampliada %&gt;% inner_join(LB) muestra_ampliada %&lt;&gt;% mutate( ic_asalud = ifelse(ic_asalud == 1, 1,0), ic_cv = ifelse(ic_cv == 1, 1,0), ic_sbv = ifelse(ic_sbv == 1, 1,0), ic_rezedu = ifelse(ic_rezedu == 1, 1,0), tol_ic4 = ic_asalud + ic_cv + ic_sbv + ic_rezedu, pred_segsoc = predicciones$pred_segsoc, pred_ingreso = predicciones$pred_ingreso, desv_estandar_residual = predicciones$desv_estandar_residual, pred_ic_ali_nc = predicciones$pred_ic_ali_nc ) rm(predicciones) Iteración por Entidades Federativas Para cada entidad federal (ii_ent), se realiza una iteración que ajusta la muestra post-calibración y calcula las estimaciones. ii_ent &lt;- &quot;02&quot; iter = 1 for(ii_ent in c(&quot;03&quot;, &quot;06&quot;, &quot;23&quot;, &quot;04&quot;, &quot;01&quot;, &quot;22&quot;, &quot;27&quot;, &quot;25&quot;, &quot;18&quot;, &quot;05&quot;, &quot;17&quot;, &quot;28&quot;, &quot;10&quot;, &quot;26&quot;, &quot;09&quot;, &quot;32&quot;, &quot;08&quot;, &quot;19&quot;, &quot;29&quot;, &quot;24&quot;, &quot;11&quot;, &quot;31&quot;, &quot;13&quot;, &quot;16&quot;, &quot;12&quot;, &quot;14&quot;, &quot;15&quot;, &quot;07&quot;, &quot;21&quot;, &quot;30&quot;, &quot;20&quot; ,&quot;02&quot; )){ cat(&quot;####################################################################\\n&quot;) inicio &lt;- Sys.time() print(inicio) # Filtrar muestra post muestra_post = muestra_ampliada %&gt;% filter(ent == ii_ent) # Cálculo de totales por municipio y población total_mpio &lt;- muestra_post %&gt;% group_by(cve_mun) %&gt;% summarise(den_mpio = sum(factor), .groups = &quot;drop&quot;) %&gt;% mutate(tot_ent = sum(den_mpio)) encuesta_sta = encuesta_enigh %&gt;% filter(ent == ii_ent) encuesta_sta %&lt;&gt;% na.omit() tot_pob &lt;- encuesta_sta %&gt;% group_by(ipm) %&gt;% summarise(num_ent = sum(fep), .groups = &quot;drop&quot;) %&gt;% transmute(ipm, prop_ipm = num_ent/sum(num_ent), tx_ipm = sum(total_mpio$den_mpio)*prop_ipm) %&gt;% filter(ipm != &quot;I&quot;) pobreza &lt;- encuesta_sta %&gt;% summarise( pobre_ext = weighted.mean(pobre_extrema, fep), pob_mod = weighted.mean(pobre_moderada, fep), pobre_moderada = sum(total_mpio$den_mpio)*pob_mod, pobre_extrema = sum(total_mpio$den_mpio)*pobre_ext) Tx_ipm &lt;- setNames(tot_pob$tx_ipm,paste0(&quot;ipm_&quot;,tot_pob$ipm)) Tx_pob &lt;- pobreza[,3:4] %&gt;% as.vector() %&gt;% unlist() tx_mun &lt;- setNames(total_mpio$den_mpio, paste0(&quot;cve_mun_&quot;,total_mpio$cve_mun)) Tx_hat &lt;- c(Tx_pob, Tx_ipm, tx_mun) for(iter in 1:200){ cat(&quot;\\n municipio = &quot;, ii_ent,&quot;\\n\\n&quot;) cat(&quot;\\n iteracion = &quot;, iter,&quot;\\n\\n&quot;) # Ajuste de la muestra post muestra_post %&lt;&gt;% mutate( ic_segsoc = rbinom(n = n(), size = 1, prob = pred_segsoc), ic_ali_nc = rbinom(n = n(), size = 1, prob = pred_ic_ali_nc), ingreso = pred_ingreso + rnorm(n = n(), mean = 0, desv_estandar_residual), tol_ic = tol_ic4 + ic_segsoc + ic_ali_nc ) %&gt;% mutate( ipm = case_when( # Población en situación de pobreza. ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, # Población vulnerable por carencias sociales. ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, # Poblacion vulnerable por ingresos. ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, # Población no pobre multidimensional y no vulnerable. ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ), # Población en situación de pobreza moderada. pobre_moderada = ifelse(c(ingreso &gt; li &amp; ingreso &lt; lp) &amp; tol_ic &gt; 2, 1, 0), pobre_extrema = ifelse(ipm == &quot;I&quot; &amp; pobre_moderada == 0, 1, 0)) Xk &lt;- muestra_post %&gt;% select(&quot;ipm&quot;, &quot;cve_mun&quot;) %&gt;% fastDummies::dummy_columns(select_columns = c(&quot;ipm&quot;, &quot;cve_mun&quot;)) %&gt;% select(names(Tx_hat[-c(1:2)])) diseno_post &lt;- bind_cols(muestra_post, Xk) %&gt;% mutate(fep = factor) %&gt;% as_survey_design( ids = upm, weights = fep, nest = TRUE ) mod_calib &lt;- as.formula(paste0(&quot;~ -1+&quot;,paste0(names(Tx_hat), collapse = &quot; + &quot;))) diseno_calib &lt;- calibrate(diseno_post, formula = mod_calib, population = Tx_hat, calfun = &quot;raking&quot;) estima_calib_ipm &lt;- diseno_calib %&gt;% group_by(cve_mun, ipm) %&gt;% summarise(est_ipm = survey_mean(vartype =&quot;var&quot;)) estima_calib_pob &lt;- diseno_calib %&gt;% group_by(cve_mun) %&gt;% summarise(est_pob_ext = survey_mean(pobre_extrema , vartype =&quot;var&quot;), est_pob_mod = survey_mean(pobre_moderada , vartype =&quot;var&quot; )) estima_calib &lt;- pivot_wider( data = estima_calib_ipm, id_cols = &quot;cve_mun&quot;, names_from = &quot;ipm&quot;, values_from = c(&quot;est_ipm&quot;, &quot;est_ipm_var&quot;), values_fill = 0 ) %&gt;% full_join(estima_calib_pob) valida_ipm &lt;- estima_calib_ipm %&gt;% inner_join(total_mpio, by = &quot;cve_mun&quot;) %&gt;% group_by(ipm) %&gt;% summarise(prop_ampliada = sum(est_ipm*den_mpio)/unique(tot_ent)) %&gt;% full_join(tot_pob, by = &quot;ipm&quot;) valida_pob &lt;- estima_calib_pob %&gt;% select(cve_mun, est_pob_ext, est_pob_mod) %&gt;% inner_join(total_mpio, by = &quot;cve_mun&quot;) %&gt;% summarise(pob_ext_ampliada = sum(est_pob_ext * den_mpio) / unique(tot_ent), pob_mod_ampliada = sum(est_pob_mod * den_mpio) / unique(tot_ent), ipm_I = pob_ext_ampliada + pob_mod_ampliada) %&gt;% bind_cols(pobreza[,1:2]) saveRDS(list(valida = list(valida_pob = valida_pob, valida_ipm = valida_ipm), estima_calib = estima_calib), file = paste0( &quot;../output/2020/iteraciones/mpio_calib/&quot;, ii_ent,&quot;/iter&quot;,iter,&quot;.rds&quot;)) gc() } fin &lt;- Sys.time() tiempo_total &lt;- difftime(fin, inicio, units = &quot;mins&quot;) print(tiempo_total) cat(&quot;####################################################################\\n&quot;) } "],["estimacion_municipios_ipm_pobreza_error.html", "15_estimacion_municipios_ipm_pobreza_error.R", " 15_estimacion_municipios_ipm_pobreza_error.R Para la ejecución del presente análisis, se debe abrir el archivo 15_estimacion_municipios_ipm_pobreza_error.R disponible en la ruta Rcodes/2020/15_estimacion_municipios_ipm_pobreza_error.R. El script comienza con la limpieza del entorno de trabajo y la carga de librerías necesarias, como tidyverse y survey. A continuación, se cargan y preparan los datos de muestra_ampliada, encuesta_enigh, y LB. Se calculan las variables relacionadas con la pobreza multidimensional (IPM) en la encuesta, incluyendo la clasificación en categorías como “I”, “II”, “III”, y “IV”, y se identifican los casos de pobreza moderada y extrema. Luego, se procesa la información de los resultados de estimación por municipio, que se leen desde archivos generados en iteraciones previas. Se calculan y combinan las estimaciones, varianzas y medias, y se consolidan en un solo conjunto de datos. Estos resultados se guardan en archivos RDS y Excel para su posterior análisis. Finalmente, se realiza una visualización comparativa de las estimaciones de pobreza multidimensional. Utilizando ggplot2, se crean gráficos que comparan las estimaciones obtenidas de la ENIGH con las de CEPAL para cada estado, y estos gráficos se exportan en formato PNG. Este paso permite la evaluación visual de la precisión y consistencia de las estimaciones realizadas. El código que compartiste está orientado a procesar y analizar datos de una encuesta para calcular y comparar las estimaciones de pobreza y el Índice de Pobreza Multidimensional (IPM). A continuación, te proporciono un desglose de las principales secciones y acciones del código: Preparación del Entorno Limpia el entorno y carga las bibliotecas necesarias para el análisis. rm(list = ls()) library(tidyverse) library(magrittr) library(survey) library(srvyr) Lectura de Datos Carga y prepara los datos necesarios: muestra_ampliada, encuesta_enigh, y LB. muestra_ampliada &lt;- readRDS(&quot;../output/2020/encuesta_ampliada.rds&quot;) encuesta_enigh &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) %&gt;% mutate(ingreso = ictpc) LB &lt;- read.delim( &quot;input/2020/Lineas_Bienestar.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot; ) %&gt;% mutate(area = as.character(area)) Cálculo del IPM en la Encuesta Calcula el IPM y las variables asociadas en encuesta_enigh. encuesta_enigh &lt;- encuesta_enigh %&gt;% inner_join(LB) %&gt;% mutate( tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu, ipm = case_when( ingreso &lt; lp &amp; tol_ic &gt;= 1 ~ &quot;I&quot;, ingreso &gt;= lp &amp; tol_ic &gt;= 1 ~ &quot;II&quot;, ingreso &lt;= lp &amp; tol_ic &lt; 1 ~ &quot;III&quot;, ingreso &gt;= lp &amp; tol_ic &lt; 1 ~ &quot;IV&quot; ), pobre_moderada = ifelse(c(ingreso &gt; li &amp; ingreso &lt; lp) &amp; tol_ic &gt; 2, 1, 0), pobre_extrema = ifelse(ipm == &quot;I&quot; &amp; pobre_moderada == 0, 1, 0) ) Resumen de Datos por Municipio Suma el número de personas por municipio en muestra_ampliada. N_mpio &lt;- muestra_ampliada %&gt;% group_by(ent, cve_mun) %&gt;% summarise(N_pers_mpio = sum(factor)) Lectura de Archivos de Iteraciones y Cálculo de Estadísticas Lee los archivos generados en iteraciones y filtra aquellos con resultados. list_estiacion &lt;- list.files(&quot;../output/2020/iteraciones/mpio_calib/&quot;, full.names = TRUE) list_estiacion &lt;- data.frame(list_estiacion, iter = list_estiacion %&gt;% map_dbl(~list.files(.x) %&gt;% length()) ) %&gt;% filter(iter &gt; 0) Proceso Iterativo para Cada estado Para cada estado, se calculan las estimaciones promedio, la varianza, y otros indicadores. Los resultados se guardan en una lista. resul_ent_ipm &lt;- list() for(ii_ent in list_estiacion$list_estiacion){ archivos &lt;- list.files(ii_ent, full.names = TRUE) datos &lt;- map_df(archivos, ~ readRDS(.x)$estima_calib) dat_estima &lt;- datos %&gt;% group_by(cve_mun) %&gt;% summarise_at(vars(!matches(&quot;var&quot;)), mean) %&gt;% data.frame() dat_n &lt;- datos %&gt;% group_by(cve_mun) %&gt;% tally() dat_B &lt;- datos %&gt;% group_by(cve_mun) %&gt;% summarise_at(vars(!matches(&quot;var&quot;)), var) %&gt;% data.frame() dat_Ubar &lt;- datos %&gt;% group_by(cve_mun) %&gt;% summarise_at(vars(matches(&quot;var&quot;)), mean) %&gt;% data.frame() names(dat_Ubar) &lt;- gsub(&quot;_var&quot;, &quot;&quot;, names(dat_Ubar)) dat_var &lt;- dat_B %&gt;% gather(key = &quot;Indicador&quot;, value = &quot;B&quot;, -cve_mun) %&gt;% inner_join( dat_Ubar %&gt;% gather(key = &quot;Indicador&quot;, value = &quot;Ubar&quot;, -cve_mun) ) %&gt;% inner_join(dat_n) var_est &lt;- dat_var %&gt;% transmute(cve_mun, Indicador, var = Ubar + (1 +1/n)*B, ee = sqrt(var)) %&gt;% pivot_wider( data = ., id_cols = &quot;cve_mun&quot;, names_from = &quot;Indicador&quot;, values_from = c(&quot;var&quot;, &quot;ee&quot;), values_fill = 0 ) dat_var &lt;- pivot_wider( data = dat_var, id_cols = &quot;cve_mun&quot;, names_from = &quot;Indicador&quot;, values_from = c(&quot;B&quot;, &quot;Ubar&quot;), values_fill = 0 ) resul_ent_ipm[[ii_ent]] &lt;- dat_n %&gt;% inner_join(dat_estima) %&gt;% inner_join(dat_var) %&gt;% inner_join(var_est) } Guardar Resultados y Crear Archivos Excel Combina los resultados de todas las estadoes, los guarda en un archivo RDS y en un archivo Excel. También calcula las estimaciones estatales y las guarda en un archivo Excel separado. temp &lt;- resul_ent_ipm %&gt;% bind_rows() temp %&lt;&gt;% inner_join(N_mpio) %&gt;% select(ent, cve_mun, N_pers_mpio, est_ipm_I, est_pob_mod, est_pob_ext, est_ipm_II:est_ipm_IV, ee_ipm_I = ee_est_ipm_I, ee_pob_mod = ee_est_pob_mod, ee_pob_ext = ee_est_pob_ext, ee_ipm_II = ee_est_ipm_II, ee_ipm_III = ee_est_ipm_III, ee_ipm_IV = ee_est_ipm_IV) temp %&gt;% saveRDS(&quot;../output/Entregas/2020/result_mpios.RDS&quot;) openxlsx::write.xlsx(temp, paste0( &quot;../output/Entregas/2020/estimacion_numicipal_&quot;, Sys.Date() , &quot;.xlsx&quot; )) temp2 &lt;- temp %&gt;% select(ent, est_ipm_I:est_ipm_IV, N_pers_mpio) %&gt;% group_by(ent) %&gt;% summarise(across(starts_with(&quot;est_&quot;), ~ sum(.x * N_pers_mpio) / sum(N_pers_mpio))) openxlsx::write.xlsx(temp2, paste0( &quot;../output/Entregas/2020/estimacion_estado_&quot;, Sys.Date() , &quot;.xlsx&quot; &quot;)) Generación de Gráficos Genera gráficos para cada tipo de IPM y los guarda en archivos PNG. diseno &lt;- encuesta_enigh %&gt;% na.omit() %&gt;% as_survey_design( ids = upm, weights = fep, nest = TRUE, strata = estrato ) estimad_dir &lt;- diseno %&gt;% group_by(ent, ipm) %&gt;% summarise(direct = survey_mean(vartype = &quot;ci&quot; )) estimad_dir_ipm &lt;- pivot_wider( data = estimad_dir, id_cols = &quot;ent&quot;, names_from = &quot;ipm&quot;, values_from = c(&quot;direct&quot;, &quot;direct_low&quot;, &quot;direct_upp&quot;) ) estimad_dir &lt;- diseno %&gt;% group_by(ent) %&gt;% summarise(direct_mod = survey_mean(pobre_moderada, vartype = &quot;ci&quot;), direct_ext = survey_mean(pobre_extrema, vartype = &quot;ci&quot;)) estimad_dir %&lt;&gt;% inner_join(estimad_dir_ipm) ind &lt;- c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;, &quot;IV&quot;, &quot;mod&quot;, &quot;ext&quot;) ii_ipm = 1 for (ii_ipm in 1:6){ ii_ipm &lt;- ind[ii_ipm] paso &lt;- paste0(&quot;(_|\\\\b)&quot;, ii_ipm, &quot;(_|\\\\b)&quot;) dat_plot &lt;- inner_join(estimad_dir, temp2) %&gt;% select(ent, matches(paso)) dat_lim &lt;- dat_plot %&gt;% select(ent, matches(&quot;upp|low&quot;)) names(dat_lim) &lt;- c(&quot;ent&quot;,&quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) dat_plot %&lt;&gt;% select(-matches(&quot;upp|low&quot;)) %&gt;% gather(key = &quot;Origen&quot;, value = &quot;Prop&quot;, -ent) %&gt;% mutate(Origen = ifelse(grepl(pattern = &quot;direct&quot;, x = Origen), &quot;ENIGH&quot;, &quot;Estimación CEPAL&quot;)) %&gt;% inner_join(dat_lim) gg_plot &lt;- ggplot(data = dat_plot, aes(x = ent, y = Prop, color = Origen)) + labs( x = &quot;&quot;, y = &quot;Estimación&quot;, color = &quot;&quot;, title = paste0(&quot;Estimación de la pobreza multidimensional 2020 - Tipo &quot;, ii_ipm ) )+ theme_bw(20) + geom_jitter(width = 0.3) + theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = 0.5)) + scale_y_continuous(labels = scales::percent_format(scale = 100)) gg_plot &lt;- gg_plot + geom_errorbar(data = dat_plot %&gt;% filter(Origen == &quot;ENIGH&quot;), aes(ymin = Lim_Inf, ymax = Lim_Sup, x = ent), width = 0.2, linewidth = 1) + scale_color_manual( breaks = c(&quot;ENIGH&quot;, &quot;Estimación CEPAL&quot;), values = c(&quot;red&quot;, &quot;blue3&quot;) ) + theme( legend.position = &quot;bottom&quot;, axis.title = element_text(size = 10), axis.text.y = element_text(size = 10), axis.text.x = element_text( angle = 90, size = 8, vjust = 0.3 ), legend.title = element_text(size = 15), legend.text = element_text(size = 15) ) ggsave(plot = gg_plot, width = 16, height = 9, filename = paste0(&quot;../output/Entregas/2020/plot_uni/ipm_&quot;, ii_ipm, &quot;.png&quot;)) } "],["mapas.html", "16_Mapas.R", " 16_Mapas.R Para la ejecución del presente análisis, se debe abrir el archivo 16_Mapas.R disponible en la ruta Rcodes/2020/16_Mapas.R. Este script en R está diseñado para procesar y visualizar datos geoespaciales relacionados con la pobreza multidimensional en México para el año 2020. Primero, se limpia el entorno de trabajo con rm(list = ls()) para eliminar objetos previos y gc() para liberar memoria, lo que garantiza que el script se ejecute en un entorno limpio. Luego, se cargan diversas librerías necesarias para la manipulación de datos (dplyr, data.table), para trabajar con datos espaciales (sf), y para la creación de gráficos (tmap). La memoria disponible se limita a 250 GB con memory.limit(250000000), lo que es crucial para manejar grandes conjuntos de datos y evitar errores por falta de memoria. A continuación, se cargan dos conjuntos de datos importantes: ipm_mpios, que contiene estimaciones de pobreza para los municipios, y ShapeDAM, que es un archivo shapefile con la geometría de los municipios. En ShapeDAM, se renombra y ajusta la columna cve_mun para extraer códigos de entidad y se eliminan columnas innecesarias. El script luego define los cortes de clasificación para las estimaciones de pobreza y genera mapas temáticos para diferentes tipos de pobreza multidimensional (IPM_I, IPM_II, IPM_III, IPM_IV) y para pobreza extrema y moderada. Utiliza la función tm_shape() de la librería tmap para crear los mapas y tm_polygons() para definir la apariencia de las áreas en el mapa, incluyendo los colores, la leyenda, y el título. Los mapas se guardan en archivos PNG con tmap_save(), especificando dimensiones y resolución para asegurar una alta calidad en la visualización. Finalmente, el script también visualiza y guarda los errores de estimación asociados con cada tipo de pobreza, utilizando el mismo procedimiento para crear mapas temáticos con los errores de estimación. La visualización de estos errores ayuda a evaluar la precisión de las estimaciones y a identificar áreas con alta incertidumbre. Este proceso proporciona una manera efectiva de comunicar visualmente la distribución de la pobreza y los errores asociados a través de mapas detallados y estéticamente ajustados. Configuración y carga de librerías ### Cleaning R environment ### rm(list = ls()) gc() ################# ### Libraries ### ################# library(dplyr) library(data.table) library(haven) library(magrittr) library(stringr) library(openxlsx) library(tmap) library(sf) select &lt;- dplyr::select ###------------ Definiendo el límite de la memoria RAM a emplear ------------### memory.limit(250000000) Carga de datos y creación de mapas de IPM ################################################################################ ###----------------------------- Loading datasets ---------------------------### ################################################################################ ipm_mpios &lt;- readRDS(&quot;../output/Entregas/2020/result_mpios.RDS&quot;) %&gt;% mutate(est_pob_ext = ifelse(est_pob_ext &lt; 0, 0, est_pob_ext)) ShapeDAM &lt;- read_sf(&quot;../shapefile/2020/MEX_2020.shp&quot;) ShapeDAM %&lt;&gt;% mutate(cve_mun = CVEGEO, ent = substr(cve_mun, 1, 2), CVEGEO = NULL) cortes &lt;- c(0, 15, 30, 50, 80, 100 )/100 P1_mpio_norte &lt;- tm_shape(ShapeDAM %&gt;% inner_join(ipm_mpios, by = &quot;cve_mun&quot;)) Mapa_I &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;est_ipm_I&quot;, title = &quot;Estimación de la pobreza \\nmultidimensional 2020 - Tipo I&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_I, filename = &quot;../output/Entregas/2020/mapas/IPM_I.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_II &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;est_ipm_II&quot;, title = &quot;Estimación de la pobreza \\nmultidimensional 2020 - Tipo II&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_II, filename = &quot;../output/Entregas/2020/mapas/IPM_II.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_III &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;est_ipm_III&quot;, title = &quot;Estimación de la pobreza \\nmultidimensional 2020 - Tipo III&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_III, filename = &quot;../output/Entregas/2020/mapas/IPM_III.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_IV &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;est_ipm_IV&quot;, title = &quot;Estimación de la pobreza \\nmultidimensional 2020 - Tipo IV&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_IV, filename = &quot;../output/Entregas/2020/mapas/IPM_IV.png&quot;, width = 4000, height = 3000, asp = 0 ) Crea#cióne pobreza extrema y moderada, y errores de estimació cortes &lt;- c(0, 15, 30, 50, 80, 100 )/100 summary(ipm_mpios$est_pob_ext) Mapa_ext &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;est_pob_ext&quot;, title = &quot;Estimación de la pobreza extrema&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_ext, filename = &quot;../output/Entregas/2020/mapas/pob_ext.png&quot;, width = 4000, height = 3000, asp = 0 ) # cortes &lt;- c(0.00, 0.01, 0.03, 0.05, 0.1, 0.2) summary(ipm_mpios$est_pob_mod) Mapa_mod &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;est_pob_mod&quot;, title = &quot;Estimación de la pobreza moderada&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_mod, filename = &quot;../output/Entregas/2020/mapas/pob_mod.png&quot;, width = 4000, height = 3000, asp = 0 ) ################################################################################ ## Errores ################################################################################ cortes &lt;- c(0, 1, 5, 10, 25, 50, 100 )/100 Mapa_I_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;ee_ipm_I&quot;, title = &quot;Error de estimación de la pobreza \\nmultidimensional 2020 - Tipo I&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_I_ee, filename = &quot;../output/Entregas/2020/mapas/IPM_I_ee.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_II_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;ee_ipm_II&quot;, title = &quot;Error de estimación de la pobreza \\nmultidimensional 2020 - Tipo II&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_II_ee, filename = &quot;../output/Entregas/2020/mapas/IPM_II_ee.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_III_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;ee_ipm_III&quot;, title = &quot;Error de estimación de la pobreza \\nmultidimensional 2020 - Tipo III&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_III_ee, filename = &quot;../output/Entregas/2020/mapas/IPM_III_ee.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_IV_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;ee_ipm_IV&quot;, title = &quot;Error de estimación de la pobreza \\nmultidimensional 2020 - Tipo IV&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_IV_ee, filename = &quot;../output/Entregas/2020/mapas/IPM_IV_ee.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_ext_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;ee_pob_ext&quot;, title = &quot;Error de estimación de la pobreza extrema&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_ext_ee, filename = &quot;../output/Entregas/2020/mapas/pob_ext_ee.png&quot;, width = 4000, height = 3000, asp = 0 ) Mapa_mod_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, &quot;ee_pob_mod&quot;, title = &quot;Error de estimación de la pobreza moderada&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_mod_ee, filename = &quot;../output/Entregas/2020/mapas/pob_mod_ee.png&quot;, width = 4000, height = 3000, asp = 0 ) "],["estimacion_municipios_carencias.html", "17_estimacion_municipios_carencias.R", " 17_estimacion_municipios_carencias.R Para la ejecución del presente análisis, se debe abrir el archivo 17_estimacion_municipios_carencias.R disponible en la ruta Rcodes/2020/17_estimacion_municipios_carencias.R. El código proporcionado realiza una serie de procesos para la estimación y calibración de indicadores de carencias a nivel municipal en base a datos de encuestas. Comienza limpiando el entorno de trabajo y cargando las librerías necesarias, como tidyverse, data.table, y survey, además de leer los datos de predicciones y encuestas. Se preparan las bases de datos con variables calculadas para las carencias, incluyendo indicadores de carencias sociales y líneas de pobreza. Posteriormente, el script filtra y prepara los datos de la encuesta para cada entidad (municipio) específica. Para cada municipio, se realiza un proceso iterativo de simulación donde se actualizan las variables del modelo (como ingreso y carencias sociales) con base en predicciones previas. Se calcula la población total y se ajusta la muestra para realizar estimaciones calibradas utilizando el método de “raking”, que ajusta los pesos de la muestra para que coincidan con las estimaciones de la población. Finalmente, se guarda cada iteración de los resultados calibrados en archivos RDS y se calcula el tiempo total de ejecución. Cada archivo contiene estimaciones para los indicadores de pobreza en cada municipio. El script utiliza técnicas de muestreo y calibración para asegurar que las estimaciones sean consistentes con las características de la población y se gestionan errores potenciales durante el proceso de calibración. Configuración y carga de librerías ### Cleaning R environment ### rm(list = ls()) library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(haven) library(labelled) library(sampling) library(lme4) library(survey) library(srvyr) source(&quot;../source/benchmarking_indicador.R&quot;) Lectura de datos ################################################################################ # Lectura de datos ################################################################################ predicciones &lt;- readRDS(&quot;../output/2020/modelos/predicciones.rds&quot;) encuesta_ampliada &lt;- readRDS(&quot;../output/2020/encuesta_ampliada.rds&quot;) encuesta_enigh &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) %&gt;% mutate(ingreso = ictpc) LB &lt;- read.delim( &quot;../input/2020/Lineas_Bienestar.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot; ) %&gt;% mutate(area = as.character(area)) IPM en la ENIGH ################################################################################ # IPM en la enigh ################################################################################ encuesta_enigh &lt;- encuesta_enigh %&gt;% inner_join(LB) %&gt;% mutate( tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu, pobrea_lp = ifelse(ingreso &lt; lp, 1, 0), pobrea_li = ifelse(ingreso &lt; li, 1, 0), tol_ic_1 = ifelse(tol_ic &gt; 0, 1, 0), tol_ic_2 = ifelse(tol_ic &gt; 2, 1, 0) ) Preparación de encuesta intercensal ################################################################################ # preparando encuesta intercensal ################################################################################ encuesta_ampliada &lt;- encuesta_ampliada %&gt;% inner_join(LB) %&gt;% mutate( ic_asalud = ifelse(ic_asalud == 1, 1, 0), ic_cv = ifelse(ic_cv == 1, 1, 0), ic_sbv = ifelse(ic_sbv == 1, 1, 0), ic_rezedu = ifelse(ic_rezedu == 1, 1, 0), tol_ic4 = ic_asalud + ic_cv + ic_sbv + ic_rezedu, pred_segsoc = predicciones$pred_segsoc, pred_ingreso = predicciones$pred_ingreso, desv_estandar_residual = predicciones$desv_estandar_residual, pred_ic_ali_nc = predicciones$pred_ic_ali_nc ) # dir.create(&quot;output/2020/iteraciones/mpio_calib_carencia&quot;) # map( # paste0( # &quot;output/2020/iteraciones/mpio_calib_carencia/&quot;, # unique(encuesta_enigh$ent) # ), # ~ dir.create(path = .x) # ) rm(predicciones) ii_ent &lt;- &quot;20&quot; list_yks &lt;- list( c(&quot;pobrea_lp&quot;), c(&quot;pobrea_li&quot;), c(&quot;tol_ic_1&quot;, &quot;tol_ic_2&quot;, &quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot;) ) Iteraciones y calibración for(ii_ent in c(&quot;03&quot;, &quot;06&quot;, &quot;23&quot;, &quot;04&quot;, &quot;01&quot;, &quot;22&quot;, &quot;27&quot;, &quot;25&quot;, &quot;18&quot;, &quot;05&quot;, &quot;17&quot;, &quot;28&quot;, &quot;10&quot;, &quot;26&quot;, &quot;09&quot;, &quot;32&quot;, &quot;08&quot;, &quot;19&quot;, &quot;29&quot;, &quot;24&quot;, &quot;11&quot;, &quot;31&quot;, &quot;13&quot;, &quot;16&quot;, &quot;12&quot;, &quot;14&quot;, &quot;15&quot;, &quot;07&quot;, &quot;21&quot;, &quot;30&quot;, &quot;20&quot;, &quot;02&quot;)) { cat(&quot;####################################################################\\n&quot;) inicio &lt;- Sys.time() print(inicio) yks &lt;- unlist(list_yks) muestra_post &lt;- encuesta_ampliada %&gt;% filter(ent == ii_ent) total_mpio &lt;- muestra_post %&gt;% group_by(cve_mun) %&gt;% summarise(den_mpio = sum(factor), .groups = &quot;drop&quot;) %&gt;% mutate(tot_ent = sum(den_mpio)) encuesta_sta &lt;- encuesta_enigh %&gt;% filter(ent == ii_ent) %&gt;% na.omit() tot_pob &lt;- encuesta_sta %&gt;% summarise_at(.vars = yks, .funs = list( ~ weighted.mean(., w = fep, na.rm = TRUE) * sum(total_mpio$den_mpio) )) top_caren &lt;- unlist(as.vector(tot_pob)) tx_mun &lt;- setNames(total_mpio$den_mpio, paste0(&quot;cve_mun_&quot;, total_mpio$cve_mun)) Tx_hat &lt;- c(top_caren, tx_mun) for(iter in 1:200) { cat(&quot;\\n municipio = &quot;, ii_ent, &quot;\\n\\n&quot;) cat(&quot;\\n iteracion = &quot;, iter, &quot;\\n\\n&quot;) ############################################ muestra_post &lt;- muestra_post %&gt;% mutate( ic_segsoc = rbinom(n = n(), size = 1, prob = pred_segsoc), ic_ali_nc = rbinom(n = n(), size = 1, prob = pred_ic_ali_nc), ingreso = pred_ingreso + rnorm(n = n(), mean = 0, sd = desv_estandar_residual), tol_ic = tol_ic4 + ic_segsoc + ic_ali_nc ) %&gt;% mutate( pobrea_lp = ifelse(ingreso &lt; lp, 1, 0), pobrea_li = ifelse(ingreso &lt; li, 1, 0), tol_ic_1 = ifelse(tol_ic &gt; 0, 1, 0), tol_ic_2 = ifelse(tol_ic &gt; 2, 1, 0) ) estima_calib_ipm &lt;- list() for(ii in 1:length(list_yks)) { yks &lt;- list_yks[[ii]] var_calib &lt;- c(yks, names(tx_mun)) Xk &lt;- muestra_post %&gt;% select(&quot;cve_mun&quot;) %&gt;% fastDummies::dummy_columns(select_columns = c(&quot;cve_mun&quot;), remove_selected_columns = TRUE) diseno_post &lt;- bind_cols(muestra_post, Xk) %&gt;% mutate(fep = factor) %&gt;% as_survey_design( ids = upm, weights = fep, nest = TRUE ) mod_calib &lt;- as.formula(paste0(&quot;~ -1 +&quot;, paste0(var_calib, collapse = &quot; + &quot;))) diseno_calib &lt;- tryCatch({ calibrate(diseno_post, formula = mod_calib, population = Tx_hat[var_calib], calfun = &quot;raking&quot;, maxit = 50) }, error = function(e) { message(&quot;Error en la calibración: &quot;, yks) return(NULL) }) if (is.null(diseno_calib)) { next } estima_calib_ipm[[ii]] &lt;- diseno_calib %&gt;% group_by(cve_mun) %&gt;% summarise_at(.vars = yks, .funs = list( ~ survey_mean(., vartype = &quot;var&quot;, na.rm = TRUE)) ) } estima_calib_ipm &lt;- keep(estima_calib_ipm, ~ !is.null(.)) %&gt;% reduce(inner_join) saveRDS(list(estima_calib = estima_calib_ipm), file = paste0(&quot;../output/2020/iteraciones/mpio_calib_carencia/&quot;, ii_ent, &quot;/iter&quot;, iter, &quot;.rds&quot;)) gc() } fin &lt;- Sys.time() tiempo_total &lt;- difftime(fin, inicio, units = &quot;mins&quot;) print(tiempo_total) cat(&quot;####################################################################\\n&quot;) } "],["estimacion_municipios_carencias_error.html", "18_estimacion_municipios_carencias_error.R", " 18_estimacion_municipios_carencias_error.R Para la ejecución del presente análisis, se debe abrir el archivo 18_estimacion_municipios_carencias_error.R disponible en la ruta Rcodes/2020/18_estimacion_municipios_carencias_error.R. Este script en R está diseñado para procesar y analizar datos de encuestas sobre pobreza en México para el año 2020. El código comienza limpiando el entorno de trabajo con rm(list = ls()) y cargando las bibliotecas necesarias (tidyverse, magrittr, survey, srvyr, tidyselect) para la manipulación de datos y análisis estadístico. La primera sección del script se enfoca en la lectura de datos. Se cargan dos conjuntos de datos usando readRDS(): encuesta_ampliada y encuesta_enigh, este último con una modificación en la columna ingreso. También se lee un archivo CSV con read.delim(), que contiene datos sobre las líneas de bienestar, y se convierte la columna area a formato de carácter. En la sección del Índice de Pobreza Multidimensional en la encuesta ENIGH, se realiza una combinación interna (inner_join()) entre encuesta_enigh y LB para añadir variables relacionadas con carencias sociales y pobreza. Se crean nuevas variables en encuesta_enigh, tales como tol_ic (suma de indicadores de carencia social), pobrea_lp (indicador de pobreza por ingresos) y pobrea_li (indicador de pobreza extrema por ingresos). Además, se genera un resumen del número de personas por municipio en N_mpio. A continuación, el script lee una lista de archivos de estimación de carencia desde un directorio específico, y procesa estos archivos para calcular diversas estadísticas como media, varianza y errores estándar de las estimaciones. Utiliza la función map_df() para combinar datos de múltiples archivos, y realiza cálculos de varianza y errores estándar para cada indicador de pobreza. Los resultados se almacenan en una lista y se combinan en un único marco de datos. Los resultados combinados (temp) se enriquecen con información de población de N_mpio y se guardan en archivos RDS y Excel utilizando saveRDS() y write.xlsx(). Se calcula también un resumen a nivel estatal, agregando los resultados por entidad. En la última sección del script, se realiza un análisis gráfico de las estimaciones de pobreza. Se convierte encuesta_enigh en un diseño de encuesta usando as_survey_design(), y se calculan las estimaciones de pobreza por entidad. Luego, se generan gráficos con ggplot2 para comparar las estimaciones de carencia entre los datos de la encuesta ENIGH y las estimaciones de CEPAL. Los gráficos se guardan como archivos PNG utilizando ggsave(). Limpieza del Entorno y Carga de Bibliotecas Se limpia el entorno de trabajo y se cargan las bibliotecas necesarias para la manipulación de datos y la visualización. rm(list = ls()) library(tidyverse) library(magrittr) library(survey) library(srvyr) library(tidyselect) Lectura de Datos Se cargan los datos de las encuestas y las líneas de bienestar desde archivos RDS y CSV. encuesta_ampliada &lt;- readRDS(&quot;../../output/2020/encuesta_ampliada.rds&quot;) encuesta_enigh &lt;- readRDS(&quot;../input/2020/enigh/encuesta_sta.rds&quot;) %&gt;% mutate(ingreso = ictpc) LB &lt;- read.delim( &quot;../input/2020/Lineas_Bienestar.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot; ) %&gt;% mutate(area = as.character(area)) Cálculo de Indicadores de Pobreza Se calculan varios indicadores de pobreza basados en el ingreso y las carencias sociales. encuesta_enigh &lt;- encuesta_enigh %&gt;% inner_join(LB) %&gt;% mutate( tol_ic = ic_segsoc + ic_ali_nc + ic_asalud + ic_cv + ic_sbv + ic_rezedu, pobrea_lp = ifelse(ingreso &lt; lp, 1, 0), pobrea_li = ifelse(ingreso &lt; li, 1, 0), tol_ic_1 = ifelse(tol_ic &gt; 0, 1, 0), tol_ic_2 = ifelse(tol_ic &gt; 2, 1, 0) ) Procesamiento de Resultados Iterativos Se listan y procesan los directorios que contienen resultados iterativos. list_estiacion &lt;- list.files(&quot;../../output/2020/iteraciones/mpio_calib_carencia//&quot;, full.names = TRUE) list_estiacion &lt;- data.frame(list_estiacion, iter = list_estiacion %&gt;% map_dbl(~list.files(.x) %&gt;% length())) %&gt;% filter(iter &gt; 0) resul_ent_ipm &lt;- list() Combinación de Resultados Iterativos Se combinan los resultados de las iteraciones en un único conjunto de datos. for (ii_ent in list_estiacion$list_estiacion) { archivos &lt;- list.files(ii_ent, full.names = TRUE) datos &lt;- map_df(archivos, ~ readRDS(.x)$estima_calib) dat_estima &lt;- datos %&gt;% group_by(cve_mun) %&gt;% summarise_at(vars(!matches(&quot;var&quot;)), mean) %&gt;% data.frame() dat_n &lt;- datos %&gt;% group_by(cve_mun) %&gt;% tally() dat_B &lt;- datos %&gt;% group_by(cve_mun) %&gt;% summarise_at(vars(!matches(&quot;var&quot;)), var) %&gt;% data.frame() dat_Ubar &lt;- datos %&gt;% group_by(cve_mun) %&gt;% summarise_at(vars(matches(&quot;var&quot;)), mean) %&gt;% data.frame() names(dat_Ubar) &lt;- gsub(&quot;_var&quot;, &quot;&quot;, names(dat_Ubar)) dat_var &lt;- dat_B %&gt;% gather(key = &quot;Indicador&quot;, value = &quot;B&quot;, -cve_mun) %&gt;% inner_join(dat_Ubar %&gt;% gather(key = &quot;Indicador&quot;, value = &quot;Ubar&quot;, -cve_mun)) %&gt;% inner_join(dat_n) var_est &lt;- dat_var %&gt;% transmute(cve_mun, Indicador, var = Ubar + (1 + 1 / n) * B, ee = sqrt(var)) %&gt;% pivot_wider( id_cols = &quot;cve_mun&quot;, names_from = &quot;Indicador&quot;, values_from = c(&quot;var&quot;, &quot;ee&quot;), values_fill = 0 ) dat_var &lt;- pivot_wider( data = dat_var, id_cols = &quot;cve_mun&quot;, names_from = &quot;Indicador&quot;, values_from = c(&quot;B&quot;, &quot;Ubar&quot;), values_fill = 0 ) resul_ent_ipm[[ii_ent]] &lt;- dat_n %&gt;% inner_join(dat_estima) %&gt;% inner_join(dat_var) %&gt;% inner_join(var_est) } Guardado de Resultados Se guardan los resultados combinados en archivos RDS y Excel. temp &lt;- resul_ent_ipm %&gt;% bind_rows() temp %&lt;&gt;% inner_join(N_mpio) %&gt;% select( ent, cve_mun, &quot;pobrea_lp&quot;, &quot;pobrea_li&quot;, &quot;tol_ic_1&quot;, &quot;tol_ic_2&quot;, &quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot;, N_pers_mpio, matches(&quot;ee&quot;) ) temp %&gt;% saveRDS(&quot;../output/Entregas/2020/result_mpios_carencia.RDS&quot;) openxlsx::write.xlsx( temp, paste0( &quot;../output/Entregas/2020/estimacion_numicipal_carencia_&quot;, Sys.Date(), &quot;.xlsx&quot; ) ) temp2 &lt;- temp %&gt;% select(ent, pobrea_lp:ic_ali_nc, N_pers_mpio) %&gt;% group_by(ent) %&gt;% summarise(across( c( &quot;pobrea_lp&quot;, &quot;pobrea_li&quot;, &quot;tol_ic_1&quot;, &quot;tol_ic_2&quot;, &quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot; ), ~ sum(.x * N_pers_mpio) / sum(N_pers_mpio) )) openxlsx::write.xlsx( temp2, paste0( &quot;../output/Entregas/2020/estimacion_estado_carencia_&quot;, Sys.Date(), &quot;.xlsx&quot; ) ) Visualización de Resultados Se generan y guardan gráficos que comparan los indicadores de pobreza entre diferentes fuentes de datos. diseno &lt;- encuesta_enigh %&gt;% na.omit() %&gt;% as_survey_design(ids = upm, weights = fep, nest = TRUE) estimad_dir &lt;- diseno %&gt;% group_by(ent) %&gt;% summarise_at( .vars = c( &quot;pobrea_lp&quot;, &quot;pobrea_li&quot;, &quot;tol_ic_1&quot;, &quot;tol_ic_2&quot;, &quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot; ), .funs = list(dir = ~ survey_mean(., vartype = &quot;ci&quot;, na.rm = TRUE)) ) ind &lt;- c(&quot;pobrea_lp&quot;, &quot;pobrea_li&quot;, &quot;tol_ic_1&quot;, &quot;tol_ic_2&quot;, &quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot;) for (ii_ipm in 1:6) { ii_ipm &lt;- ind[ii_ipm] paso &lt;- paste0(&quot;(_|\\\\b)&quot;, ii_ipm, &quot;(_|\\\\b)&quot;) dat_plot &lt;- inner_join(estimad_dir, temp2) %&gt;% select(ent, matches(paso)) dat_lim &lt;- dat_plot %&gt;% select(ent, matches(&quot;upp|low&quot;)) names(dat_lim) &lt;- c(&quot;ent&quot;, &quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) dat_plot %&lt;&gt;% select(-matches(&quot;upp|low&quot;)) %&gt;% gather(key = &quot;Origen&quot;, value = &quot;Prop&quot;, -ent) %&gt;% mutate(Origen = ifelse(grepl(pattern = &quot;dir&quot;, x = Origen), &quot;ENIGH&quot;, &quot;Estimación CEPAL&quot;)) %&gt;% inner_join(dat_lim) gg_plot &lt;- ggplot(data = dat_plot, aes(x = ent, y = Prop, color = Origen)) + labs( x = &quot;&quot;, y = &quot;Estimación&quot;, color = &quot;&quot;, title = paste0(&quot;Estimación de la carencia 2020 -&quot;, ii_ipm) ) + theme_bw(20) + geom_jitter(width = 0.3) + theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = 0.5)) + scale_y_continuous(labels = scales::percent_format(scale = 100)) gg_plot &lt;- gg_plot + geom_errorbar( data = dat_plot %&gt;% filter(Origen == &quot;ENIGH&quot;), aes(ymin = Lim_Inf, ymax = Lim_Sup, x = ent), width = 0.2, linewidth = 1 ) + scale_color_manual( breaks = c(&quot;ENIGH&quot;, &quot;Estimación CEPAL&quot;), values = c(&quot;red&quot;, &quot;blue3&quot;) ) + theme( legend.position = &quot;bottom&quot;, axis.title = element_text(size = 10), axis.text.y = element_text(size = 10), axis.text.x = element_text( angle = 90, size = 8, vjust = 0.3 ), legend.title = element_text(size = 15), legend.text = element_text(size = 15) ) print(gg_plot) ggsave( plot = gg_plot, width = 16, height = 9, filename = paste0(&quot;../output/Entregas/2020/plot_uni/ipm_&quot;, ii_ipm, &quot;.png&quot;) ) } "],["mapas_carencias.html", "19_Mapas_carencias.R", " 19_Mapas_carencias.R Para la ejecución del presente análisis, se debe abrir el archivo 19_Mapas_carencias.R disponible en la ruta Rcodes/2020/19_Mapas_carencias.R. El código proporciona un procedimiento para la visualización de estimaciones de carencias y errores de estimación en un formato de mapa utilizando tmap y datos geoespaciales. Primero, limpia el entorno de trabajo en R y establece un límite para el uso de memoria RAM. Luego, carga las librerías necesarias y los datasets relevantes, incluyendo los resultados de estimación de carencia y un shapefile de México que se ajusta para incluir las claves municipales y regionales. A continuación, define los cortes para la clasificación de los datos en porcentajes y genera mapas temáticos para cada indicador de carencia (como pobreza por ingresos, carencias sociales, etc.). Utiliza el objeto ShapeDAM combinado con ipm_mpios para crear mapas que visualizan estos indicadores con una paleta de colores verde. Cada mapa se guarda como una imagen PNG con alta resolución. Finalmente, el código realiza un proceso similar para visualizar los errores de estimación asociados a cada indicador. Los cortes para los errores se definen de manera diferente, y se generan mapas temáticos que muestran la magnitud de estos errores. Cada mapa se guarda en formato PNG con especificaciones similares. rm(list = ls()) gc() ################# ### Libraries ### ################# library(dplyr) library(data.table) library(haven) library(magrittr) library(stringr) library(openxlsx) library(tmap) library(sf) select &lt;- dplyr::select ###------------ Definiendo el límite de la memoria RAM a emplear ------------### memory.limit(250000000) ################################################################################ ###----------------------------- Loading datasets ---------------------------### ################################################################################ ipm_mpios &lt;- readRDS(&quot;../output/Entregas/2020/result_mpios_carencia.RDS&quot;) ShapeDAM &lt;- read_sf(&quot;../shapefile/2020/MEX_2020.shp&quot;) ShapeDAM %&lt;&gt;% mutate(cve_mun = CVEGEO , ent = substr(cve_mun, 1, 2), CVEGEO = NULL) cortes &lt;- c(0, 15, 30, 50, 80, 100 )/100 P1_mpio_norte &lt;- tm_shape(ShapeDAM %&gt;% left_join(ipm_mpios, by = &quot;cve_mun&quot;)) names(ipm_mpios) ind &lt;- c(&quot;pobrea_lp&quot;, &quot;pobrea_li&quot;,&quot;tol_ic_1&quot;,&quot;tol_ic_2&quot;,&quot;ic_segsoc&quot;, &quot;ic_ali_nc&quot;) for(yks_ind in ind){ Mapa_I &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, yks_ind, # style = &quot;quantile&quot;, title = paste0(&quot;Estimación de la carencia 2020 -&quot;, yks_ind ), palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, #legend.outside = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_I, filename = paste0(&quot;../output/Entregas/2020/mapas/carencia_&quot;, yks_ind, &quot;.png&quot;), width = 4000, height = 3000, asp = 0 ) } ################################################################################ ## Errores ################################################################################ cortes &lt;- c(0, 1,5,10, 25, 50, 100 )/100 ind_ee &lt;- paste0(&quot;ee_&quot;, ind) for(yks_ind in ind_ee){ Mapa_I_ee &lt;- P1_mpio_norte + tm_polygons( breaks = cortes, yks_ind, # style = &quot;quantile&quot;, title = paste0(&quot;Error de estimación de la carencia 2020 - &quot;, yks_ind), palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(legend.show = TRUE, #legend.outside = TRUE, legend.text.size = 1.5, legend.outside.position = &#39;left&#39;, legend.hist.width = 1, legend.hist.height = 3, legend.stack = &#39;vertical&#39;, legend.title.fontface = &#39;bold&#39;, legend.text.fontface = &#39;bold&#39;) tmap_save( Mapa_I_ee, filename = paste0(&quot;../output/Entregas/2020/mapas/carencia_&quot;, yks_ind, &quot;.png&quot;), width = 4000, height = 3000, asp = 0 ) } "],["estimacion_directa_carencias.html", "20_estimacion_directa_carencias.R", " 20_estimacion_directa_carencias.R Descripción General del Código Este código está diseñado para procesar y analizar datos de encuestas con el objetivo de generar estimaciones de carencia para diferentes entidades dentro de un país. A continuación, se detalla cada sección del código: Preparación del Entorno Al inicio, el código limpia el entorno de trabajo de R mediante el siguiente bloque de código: rm(list = ls()) Esto elimina todos los objetos existentes en el espacio de trabajo, asegurando que el análisis se realice en un entorno limpio y sin interferencias de datos anteriores. Carga de Bibliotecas Se cargan varias bibliotecas necesarias para el análisis de datos mediante el bloque de código: library(tidyverse) library(data.table) library(openxlsx) library(magrittr) library(haven) library(labelled) library(sampling) library(lme4) library(survey) library(srvyr) Estas bibliotecas proporcionan herramientas para la manipulación de datos, manejo de archivos, técnicas de muestreo y análisis de encuestas. Lectura y Preparación de Datos Se lee un archivo de datos en formato RDS que contiene la encuesta ampliada y se prepara la encuesta intercensal con el siguiente código: encuesta_ampliada &lt;- readRDS(&quot;output/2020/encuesta_ampliada.rds&quot;) encuesta_ampliada %&lt;&gt;% mutate( ic_asalud = ifelse(ic_asalud == 1, 1,0), ic_cv = ifelse(ic_cv == 1, 1,0), ic_sbv = ifelse(ic_sbv == 1, 1,0), ic_rezedu = ifelse(ic_rezedu == 1, 1,0) ) Aquí, se ajustan ciertos indicadores para que solo tomen valores binarios (0 o 1). Creación de Directorios y Definición de Variables Se crea un directorio para almacenar los resultados de estimación y se define un vector con códigos de entidades: dir.create(&quot;output/2020/estimacion_dir&quot;) c(&quot;03&quot;, &quot;06&quot;, &quot;23&quot;, &quot;04&quot;, &quot;01&quot;, &quot;22&quot;, &quot;27&quot;, &quot;25&quot;, &quot;18&quot;, &quot;05&quot;, &quot;17&quot;, &quot;28&quot;, &quot;10&quot;, &quot;26&quot;, &quot;09&quot;, &quot;32&quot;, &quot;08&quot;, &quot;19&quot;, &quot;29&quot;, &quot;24&quot;, &quot;11&quot;, &quot;31&quot;, &quot;13&quot;, &quot;16&quot;, &quot;12&quot;, &quot;14&quot;, &quot;15&quot;, &quot;07&quot;, &quot;21&quot;, &quot;30&quot;, &quot;20&quot; ,&quot;02&quot;) Análisis y Estimación Se realiza un bucle for que itera sobre cada código de entidad con el siguiente bloque de código: for(ii_ent in c(&quot;03&quot;, &quot;06&quot;, &quot;23&quot;, &quot;04&quot;, &quot;01&quot;, &quot;22&quot;, &quot;27&quot;, &quot;25&quot;, &quot;18&quot;, &quot;05&quot;, &quot;17&quot;, &quot;28&quot;, &quot;10&quot;, &quot;26&quot;, &quot;09&quot;, &quot;32&quot;, &quot;08&quot;, &quot;19&quot;, &quot;29&quot;, &quot;24&quot;, &quot;11&quot;, &quot;31&quot;, &quot;13&quot;, &quot;16&quot;, &quot;12&quot;, &quot;14&quot;, &quot;15&quot;, &quot;07&quot;, &quot;21&quot;, &quot;30&quot;, &quot;20&quot; ,&quot;02&quot; )){ cat(&quot;################################################################################################################\\n&quot;) inicio &lt;- Sys.time() print(inicio) muestra_post = encuesta_ampliada %&gt;% filter(ent == ii_ent) cat(&quot;\\n Estado = &quot;, ii_ent,&quot;\\n\\n&quot;) diseno_post &lt;- muestra_post %&gt;% mutate(fep = factor) %&gt;% as_survey_design( ids = upm, weights = fep, nest = TRUE, # strata = estrato ) estima_carencia &lt;- diseno_post %&gt;% group_by(cve_mun) %&gt;% summarise_at(.vars = yks, .funs = list( ~ survey_mean(., vartype =&quot;var&quot;, na.rm = TRUE)) ) saveRDS(estima_carencia, file = paste0( &quot;output/2020/estimacion_dir/estado_&quot;, ii_ent,&quot;.rds&quot;)) gc(TRUE) fin &lt;- Sys.time() tiempo_total &lt;- difftime(fin, inicio, units = &quot;mins&quot;) print(tiempo_total) cat(&quot;################################################################################################################\\n&quot;) } "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
