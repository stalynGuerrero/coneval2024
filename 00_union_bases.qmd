# Lectura y consolidación de bases de datos

```{r, echo=FALSE, warning=FALSE,error=FALSE, message=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE)
library(printr)
library(kableExtra)
library(tidyverse)
rstan::rstan_options(auto_write = TRUE) # speed up running time 
tba <- function(dat, cap = NA){
  kable(dat,
      format = "html", digits =  4,
      caption = cap) %>% 
     kable_styling(bootstrap_options = "striped", full_width = F)%>%
         kable_classic(full_width = F, html_font = "Arial Narrow")
}
```

En el siguiente apartado se describe el proceso de integración y ordenamiento de las bases de datos proporcionadas por la Universidad Nacional Autónoma de México (UNAM) y el Consejo Nacional de Evaluación de la Política de Desarrollo Social (CONEVAL). Este proceso tiene como propósito asegurar el adecuado desarrollo de los scripts necesarios para el análisis.

## Consolidando la base de datos de la encuesta ampliada del censo 

### Limpieza del espacio de trabajo {-}
   Esta línea elimina todos los objetos del espacio de trabajo actual. Es útil para asegurar que no haya objetos residuales que puedan interferir con el nuevo análisis.

```{r, echo=TRUE, eval=FALSE}
rm(list = ls())
```



### Carga de bibliotecas {-}
   Estas líneas cargan diversas bibliotecas en R:

```{r, warning=FALSE,error=FALSE}
library(tidyverse)
library(data.table)
library(openxlsx)
library(magrittr)
library(DataExplorer)
library(haven)
library(purrr)
library(furrr)
library(labelled)
```

### Definición de variables para validación {-}
   Se define un vector `validar_var` que contiene una lista de nombres de variables. Estos nombres podrían ser utilizados más adelante para validar o verificar datos en el análisis.
```{r, warning=FALSE,error=FALSE}
validar_var <- c("ic_rezedu",
                 "ic_asalud",
                 "ic_segsoc",
                 "ic_cv",
                 "ic_sbv",
                 "ic_ali_nc",
                 "ictpc")
```

### Obtener la lista de archivos .dta {-}
  Estas líneas generan listas de archivos con extensión `.dta` en los directorios especificados:
  
   - `file_muestra_censo_2020_estado`: Contiene la lista de archivos `.dta` en el directorio `SegSocial/SegSoc/`.
   
   - `file_muestra_censo_2020_estado_complemento`: Contiene la lista de archivos `.dta` en el directorio `Complemento_SegSoc/`.
   
   - `muestra_cuestionario_ampliado_censo_2020_estado`: Contiene la lista de archivos `.dta` en el directorio `IndicadoresCenso/`.

```{r, warning=FALSE,error=FALSE}
(file_muestra_censo_2020_estado <-
  list.files(
    "../input/2020/muestra_ampliada/SegSocial/SegSoc/",
    full.names = TRUE,
    pattern = "dta$"
  ))

(file_muestra_censo_2020_estado_complemento <-
  list.files(
    "../input/2020/muestra_ampliada/SegSocial/Complemento_SegSoc/",
    full.names = TRUE,
    pattern = "dta$"
  ))

(muestra_cuestionario_ampliado_censo_2020_estado <-
  list.files(
    "../input/2020/muestra_ampliada/IndicadoresCenso/",
    full.names = TRUE,
    pattern = "dta$"
  ))
```

### Conslidar bases de la muestra ampliada {-}

El código inicia creando un dataframe vacío y cargando las bibliotecas necesarias para el análisis de datos. Luego, itera sobre los archivos `.dta` para leer y combinar datos de diferentes fuentes, realizando uniones internas y ajustes según sea necesario. Cada conjunto de datos combinado se guarda en archivos `.rds` individuales durante el proceso. Después, los datos combinados de cada archivo se acumulan en un dataframe principal. Finalmente, el dataframe completo se guarda en un archivo `.rds`. Este procedimiento facilita la integración y el análisis de grandes volúmenes de datos provenientes de diversas fuentes.


```{r, eval=FALSE}

# Crear un dataframe vacío para almacenar los datos
df <- data.frame()

# Iterar sobre cada archivo y leerlo
for (ii in 1:32) {
  muestra_censo_2020_estado_ii <-
    read_dta(file_muestra_censo_2020_estado[ii])
 
   muestra_censo_2020_estado_complemento_ii <-
    read_dta(file_muestra_censo_2020_estado_complemento[ii]) %>%
    mutate(id_per = id_persona)
  
   muestra_cuestionario_ampliado_censo_2020_estado_ii <-
    read_dta(muestra_cuestionario_ampliado_censo_2020_estado[ii])
  
  muestra_censo <-
    inner_join(muestra_censo_2020_estado_ii,
               muestra_censo_2020_estado_complemento_ii) %>%
    select(-tamloc) %>%
    inner_join(muestra_cuestionario_ampliado_censo_2020_estado_ii)
  
  saveRDS(muestra_censo,
          paste0("output/2020/muestra_censo/depto_", ii, ".rds"))
  
  df <- bind_rows(df, muestra_censo)
  cat(file_muestra_censo_2020_estado[ii], "\n")
}

# Guardar el dataframe en un archivo .rds
saveRDS(df, file = "output/2020/muestra_cuestionario_ampliado.rds")

```

## Cambiando de formato .dta a .rds la base de la ENIGH. 

El código se enfoca en la integración y preparación de datos de la Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). Primero, lee archivos de datos relevantes para los hogares y la pobreza desde fuentes especificadas. Luego, combina estos datos mediante uniones internas, basándose en identificadores comunes para asegurar una correcta correspondencia entre registros. Se ajustan algunas variables, como la creación de una nueva columna para los datos de ictpc, y se elimina una columna innecesaria. Finalmente, el dataframe combinado se guarda en un archivo .rds para su posterior análisis. Este proceso facilita la consolidación y preparación de los datos de la ENIGH para análisis más detallados.

```{r, eval=FALSE}
enigh_hogares <- read_dta("../input/2020/enigh/base_hogares20.dta")
enigh_pobreza <- read_dta("../input/2020/enigh/pobreza_20.dta") %>% 
  select(-ent)

n_distinct(enigh_pobreza$folioviv)
n_distinct(enigh_hogares$folioviv)

enigh <- inner_join(
  enigh_pobreza,
  enigh_hogares,
  by = join_by(
    folioviv, foliohog, est_dis, upm,
    factor, rururb, ubica_geo
  ), 
  suffix = c("_pers", "_hog"),
)

enigh$ic_ali_nc
enigh$ictpc_pers
enigh$ictpc <- enigh$ictpc_pers
enigh$ic_segsoc

#saveRDS(enigh, file = "output/2020/enigh.rds")

```

